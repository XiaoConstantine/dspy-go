[{"content":"The Context Window Problem Large Language Models have revolutionized how we process text, but they face a fundamental constraint: context window limits. Even with models supporting 100K+ tokens, processing massive documents, codebases, or datasets often requires either:\nTruncation ‚Äî losing potentially critical information Chunking with summarization ‚Äî introducing lossy compression Retrieval-Augmented Generation (RAG) ‚Äî adding infrastructure complexity What if there was a way to give LLMs the ability to programmatically explore context of any size, querying exactly what they need, when they need it?\nEnter Recursive Language Models (RLM).\nWhat is RLM? RLM is a novel inference paradigm developed by researchers at MIT\u0026rsquo;s OASYS Lab (arXiv:2512.24601). Instead of stuffing everything into a single prompt, RLM provides the LLM with a REPL (Read-Eval-Print Loop) environment where:\nThe full context is stored as a variable the LLM can query The LLM writes code to explore and process the context Sub-LLM calls can be made from within the REPL Iteration continues until the LLM signals completion Think of it as giving the LLM a programming environment where it can methodically work through large contexts instead of trying to process everything at once.\nThe Core Pattern: Query() and FINAL() RLM revolves around two key primitives:\n// The LLM can query sub-sections or make sub-LLM calls result := Query(\u0026#34;Summarize the key findings in section 3\u0026#34;) // When done, the LLM signals completion FINAL(finalAnswer)\rThis simple pattern enables sophisticated exploration strategies:\nDivide and conquer ‚Äî Process sections independently, then synthesize Iterative refinement ‚Äî Build understanding progressively Targeted extraction ‚Äî Query only relevant portions Hierarchical analysis ‚Äî Drill down into specific areas RLM in DSPy-Go We\u0026rsquo;ve implemented RLM as a first-class module in DSPy-Go, fully integrated with our module system, interceptors, and tracing infrastructure.\nBasic Usage package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules/rlm\u0026#34; ) func main() { ctx := context.Background() // Initialize your LLM llm, _ := llms.NewAnthropicLLM(ctx, \u0026#34;claude-sonnet-4-20250514\u0026#34;, llms.WithAnthropicAPIKey(\u0026#34;your-api-key\u0026#34;)) // Create RLM module with configuration rlmModule := rlm.NewFromLLM(llm, rlm.WithMaxIterations(30), rlm.WithTimeout(5*time.Minute), rlm.WithVerbose(true), rlm.WithTraceDir(\u0026#34;./rlm_logs\u0026#34;), ) // Process a large document largeDocument := loadDocument(\u0026#34;research_paper.txt\u0026#34;) // 50K+ tokens result, err := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: largeDocument, \u0026#34;query\u0026#34;: \u0026#34;What are the three main contributions of this paper?\u0026#34;, }) if err != nil { panic(err) } fmt.Printf(\u0026#34;Answer: %s\\n\u0026#34;, result[\u0026#34;response\u0026#34;]) fmt.Printf(\u0026#34;Iterations used: %d\\n\u0026#34;, result[\u0026#34;iterations\u0026#34;]) fmt.Printf(\u0026#34;Total tokens: %d\\n\u0026#34;, result[\u0026#34;total_tokens\u0026#34;]) }\rHow It Works Under the Hood The RLM module orchestrates an iterative loop:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ RLM Execution Flow ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚îÇ ‚îÇ 1. Initialize ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Load context into REPL environment ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Set up Query() and FINAL() functions ‚îÇ ‚îÇ ‚îÇ ‚îÇ 2. Iteration Loop (max 30 iterations) ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ a) LLM receives: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ System prompt with RLM instructions ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Current query/task ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Iteration history ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Available variables in REPL ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ b) LLM responds with: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Reasoning (exploration strategy) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Code block (Go code to execute) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Action (explore/analyze/final) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ c) System executes code in sandboxed REPL ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Captures stdout/stderr ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Processes any Query() sub-calls ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Updates variable state ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ d) Check completion: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ FINAL() called? ‚Üí Return answer ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Otherwise ‚Üí Continue loop ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ 3. Return Result ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Final answer + metadata (tokens, iterations, time) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\rThe REPL Environment DSPy-Go\u0026rsquo;s RLM uses Yaegi, a Go interpreter, to execute code within a sandboxed environment:\n// Available in the REPL environment: // The full context as a variable context string // Your large document/data // Query function for sub-LLM calls Query(prompt string) string // Batch queries for efficiency QueryBatched(prompts []string) []string // Signal completion FINAL(answer interface{}) FINAL_VAR(variableName string) // Return a variable\u0026#39;s value\rSecurity considerations:\nRestricted imports (no os, sys, subprocess, etc.) Sandboxed execution environment Configurable timeouts Size limits on outputs Token Efficiency: The 40% Advantage RLM achieves approximately 40% token savings compared to naive approaches. Here\u0026rsquo;s how:\nTraditional Approach Prompt = Full Document (50K tokens) + Question ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Every query sends the entire document\rRLM Approach Iteration 1: Question + \u0026#34;What sections exist?\u0026#34; (~500 tokens) Iteration 2: Question + \u0026#34;Read section 3\u0026#34; (~2K tokens) Iteration 3: Question + \u0026#34;Analyze finding X\u0026#34; (~1K tokens) Iteration 4: FINAL(synthesized answer) (~500 tokens) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Total: ~4K tokens vs 50K+ traditional\rThe savings come from:\nProgrammatic chunking ‚Äî Only load what\u0026rsquo;s needed Incremental exploration ‚Äî Build understanding progressively Sub-call optimization ‚Äî Batch related queries History compression ‚Äî Summarize previous iterations Token Tracking DSPy-Go tracks token usage across the entire RLM execution:\nresult, _ := rlmModule.Process(ctx, inputs) usage := result[\u0026#34;usage\u0026#34;].(rlm.TokenUsage) fmt.Printf(\u0026#34;Root LLM tokens: %d\\n\u0026#34;, usage.RootTokens) fmt.Printf(\u0026#34;Sub-call tokens: %d\\n\u0026#34;, usage.SubCallTokens) fmt.Printf(\u0026#34;Total tokens: %d\\n\u0026#34;, usage.TotalTokens) fmt.Printf(\u0026#34;Iterations: %d\\n\u0026#34;, result[\u0026#34;iterations\u0026#34;])\rAdvanced Configuration Custom Iteration Limits // For complex analysis tasks rlm.NewFromLLM(llm, rlm.WithMaxIterations(50), // Allow more exploration rlm.WithTimeout(10*time.Minute), ) // For quick extraction tasks rlm.NewFromLLM(llm, rlm.WithMaxIterations(5), // Faster completion rlm.WithTimeout(1*time.Minute), )\rExecution Tracing Enable detailed tracing for debugging and analysis:\nrlm.NewFromLLM(llm, rlm.WithVerbose(true), rlm.WithTraceDir(\u0026#34;./rlm_traces\u0026#34;), )\rThis produces JSONL trace files containing:\nEach iteration\u0026rsquo;s LLM response Code executed and outputs Sub-LLM call details Token usage per step Timing information Integration with DSPy-Go Modules RLM works seamlessly with other DSPy-Go components:\n// Use with interceptors rlmModule := rlm.NewFromLLM(llm) rlmModule.SetInterceptors([]core.ModuleInterceptor{ interceptors.LoggingModuleInterceptor(), interceptors.TracingModuleInterceptor(), }) // Compose with other modules program := core.NewProgram( map[string]core.Module{ \u0026#34;extract\u0026#34;: rlmModule, \u0026#34;summarize\u0026#34;: modules.NewPredict(summarySignature), }, func(ctx context.Context, inputs map[string]any) (map[string]any, error) { // RLM extracts key points extracted, _ := program.Modules[\u0026#34;extract\u0026#34;].Process(ctx, inputs) // Then summarize return program.Modules[\u0026#34;summarize\u0026#34;].Process(ctx, map[string]any{ \u0026#34;content\u0026#34;: extracted[\u0026#34;response\u0026#34;], }) }, )\rUse Cases 1. Codebase Analysis result, _ := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: entireCodebase, // Thousands of files \u0026#34;query\u0026#34;: \u0026#34;Find all security vulnerabilities related to SQL injection\u0026#34;, })\rThe LLM can:\nList files and directories Read specific files Search for patterns Analyze dependencies Synthesize findings 2. Research Paper Analysis result, _ := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: researchPaper, \u0026#34;query\u0026#34;: \u0026#34;Compare the methodology with Smith et al. (2023) and identify gaps\u0026#34;, })\r3. Log Analysis result, _ := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: serverLogs, // Gigabytes of logs \u0026#34;query\u0026#34;: \u0026#34;Find the root cause of the outage on January 5th\u0026#34;, })\r4. Document Q\u0026amp;A result, _ := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: legalContract, \u0026#34;query\u0026#34;: \u0026#34;What are all the termination clauses and their conditions?\u0026#34;, })\rBest Practices 1. Clear, Specific Queries // Good: Specific and actionable \u0026#34;query\u0026#34;: \u0026#34;List all API endpoints that don\u0026#39;t have authentication middleware\u0026#34; // Less effective: Vague \u0026#34;query\u0026#34;: \u0026#34;Tell me about the API\u0026#34;\r2. Appropriate Iteration Limits Task Type Recommended Max Iterations Simple extraction 5-10 Analysis 15-25 Complex synthesis 30-50 3. Monitor Token Usage if usage.TotalTokens \u0026gt; expectedBudget { log.Warn(\u0026#34;Token usage exceeded expectations\u0026#34;, \u0026#34;expected\u0026#34;, expectedBudget, \u0026#34;actual\u0026#34;, usage.TotalTokens) }\r4. Use Tracing for Debugging When results are unexpected, enable tracing to understand the LLM\u0026rsquo;s exploration path:\nrlm.NewFromLLM(llm, rlm.WithVerbose(true), rlm.WithTraceDir(\u0026#34;./debug_traces\u0026#34;), )\rArchitecture Deep Dive Module Structure pkg/modules/rlm/ ‚îú‚îÄ‚îÄ rlm.go # Core RLM module implementation ‚îú‚îÄ‚îÄ config.go # Configuration options ‚îú‚îÄ‚îÄ repl_yaegi.go # Yaegi-based Go REPL ‚îú‚îÄ‚îÄ signature.go # DSPy signature definitions ‚îú‚îÄ‚îÄ token_tracker.go # Token usage tracking ‚îî‚îÄ‚îÄ rlm_test.go # Comprehensive tests\rKey Components RLM Module ‚Äî Orchestrates the iteration loop, manages state, handles completion detection.\nSubLLMClient ‚Äî Interface for making sub-LLM calls from within the REPL:\ntype SubLLMClient interface { Query(ctx context.Context, prompt string) (string, error) QueryBatched(ctx context.Context, prompts []string) ([]string, error) }\rTokenTracker ‚Äî Accumulates token usage across all calls:\ntype TokenTracker struct { RootInputTokens int RootOutputTokens int SubInputTokens int SubOutputTokens int }\rYaegiREPL ‚Äî Sandboxed Go interpreter for code execution with security restrictions.\nComparison with Alternatives Approach Context Limit Token Efficiency Accuracy Complexity Full Context Model limit Low High Low Chunking Unlimited Medium Medium Medium RAG Unlimited High Variable High RLM Unlimited High High Medium RLM combines the accuracy advantages of full-context processing with the efficiency of selective retrieval, without requiring external vector databases or retrieval infrastructure.\nGetting Started Install DSPy-Go:\ngo get github.com/XiaoConstantine/dspy-go\rImport the RLM module:\nimport \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules/rlm\u0026#34;\rCreate and use:\nrlmModule := rlm.NewFromLLM(yourLLM) result, _ := rlmModule.Process(ctx, map[string]any{ \u0026#34;context\u0026#34;: yourLargeDocument, \u0026#34;query\u0026#34;: \u0026#34;Your question here\u0026#34;, })\rWhat\u0026rsquo;s Next We\u0026rsquo;re actively developing RLM support in DSPy-Go with plans for:\nNested RLM calls ‚Äî Recursive sub-RLM invocations for hierarchical analysis Custom REPL environments ‚Äî Support for Python and other languages Streaming results ‚Äî Real-time iteration updates Caching layer ‚Äî Avoid redundant sub-calls across queries Visual debugger ‚Äî Web UI for exploring RLM execution traces Conclusion RLM represents a paradigm shift in how we approach large context processing. By giving LLMs the ability to programmatically explore data through iterative REPL execution, we unlock:\nNear-infinite context handling without model limits 40% token savings through intelligent exploration Higher accuracy by focusing on relevant information Transparent reasoning via execution traces The integration in DSPy-Go makes it easy to add RLM capabilities to your existing workflows while maintaining compatibility with our module, interceptor, and tracing systems.\nTry RLM today and experience a new way of working with large contexts.\nFor questions and feedback, open an issue on GitHub or join our community discussions.\nReferences Recursive Language Models: A New Inference Paradigm ‚Äî Zhang, Kraska, Khattab (MIT OASYS Lab) DSPy-Go Documentation RLM Example Code ","date":"2025-01-07","id":0,"permalink":"/dspy-go/blog/introducing-rlm-recursive-language-models-in-dspy-go/","summary":"Learn how RLM enables LLMs to programmatically explore large contexts through iterative REPL-based execution, achieving significant token efficiency gains.","tags":["rlm","modules","token-efficiency","large-context"],"title":"Introducing RLM: Recursive Language Models in DSPy-Go"},{"content":"","date":"2023-09-07","id":1,"permalink":"/dspy-go/blog/","summary":"","tags":[],"title":"Blog"},{"content":"Getting Started with dspy-go This guide will walk you through the basics of setting up dspy-go and running your first prediction. You can get started in two ways: with our CLI tool (no code required) or by writing Go code.\nChoose Your Path üöÄ Option A: CLI Tool (Recommended for Beginners) Perfect for exploring dspy-go without writing code. Try optimizers, test with sample datasets, and see results instantly.\nJump to CLI Quick Start ‚Üí\nüíª Option B: Programming with Go Build custom applications with full control. Perfect for production applications and custom workflows.\nJump to Programming Quick Start ‚Üí\nCLI Quick Start The dspy-go CLI eliminates 60+ lines of boilerplate and lets you test all optimizers instantly.\n1. Build the CLI cd cmd/dspy-cli go build -o dspy-cli\r2. Set Your API Key export GEMINI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # or export OPENAI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # or export ANTHROPIC_API_KEY=\u0026#34;your-api-key-here\u0026#34;\r3. Explore Optimizers # See all available optimizers ./dspy-cli list # Get optimizer recommendations ./dspy-cli recommend balanced # Try Bootstrap optimizer with GSM8K math problems ./dspy-cli try bootstrap --dataset gsm8k --max-examples 5 # Test advanced MIPRO optimizer ./dspy-cli try mipro --dataset gsm8k --verbose # Experiment with evolutionary GEPA ./dspy-cli try gepa --dataset gsm8k --max-examples 10\rWhat You Can Do with the CLI List Optimizers: See all available optimization algorithms Try Optimizers: Test any optimizer with built-in datasets Get Recommendations: Get suggestions based on your needs (speed/quality trade-off) Compare Results: Run multiple optimizers and compare performance No Code Required: Everything works out of the box Full CLI Documentation ‚Üí\nProgramming Quick Start Build custom LLM applications with full programmatic control.\n1. Installation Add dspy-go to your project using go get:\ngo get github.com/XiaoConstantine/dspy-go\r2. Set Up Your API Key dspy-go supports multiple LLM providers. Set the appropriate environment variable:\n# Google Gemini (multimodal support) export GEMINI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # OpenAI export OPENAI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # Anthropic Claude export ANTHROPIC_API_KEY=\u0026#34;your-api-key-here\u0026#34; # Ollama (local) export OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34;\r3. Your First Program Here\u0026rsquo;s a simple sentiment analysis application using zero-configuration setup:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // 1. Zero-Config Setup // Pass empty string - automatically uses API key from environment variable llm, err := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatalf(\u0026#34;Failed to create LLM: %v\u0026#34;, err) } core.SetDefaultLLM(llm) // 2. Define a Signature // A signature describes the task inputs, outputs, and instructions signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;sentence\u0026#34;, core.WithDescription(\u0026#34;The sentence to classify.\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;sentiment\u0026#34;, core.WithDescription(\u0026#34;The sentiment (Positive, Negative, Neutral).\u0026#34;))}, }, ).WithInstruction(\u0026#34;You are a helpful sentiment analysis expert. Classify the sentiment of the given sentence.\u0026#34;) // 3. Create a Predict Module predictor := modules.NewPredict(signature) // 4. Execute the Predictor ctx := context.Background() input := map[string]interface{}{ \u0026#34;sentence\u0026#34;: \u0026#34;dspy-go makes building AI applications easy and fun!\u0026#34;, } result, err := predictor.Process(ctx, input) if err != nil { log.Fatalf(\u0026#34;Prediction failed: %v\u0026#34;, err) } // 5. Print the Result fmt.Printf(\u0026#34;Sentence: %s\\n\u0026#34;, input[\u0026#34;sentence\u0026#34;]) fmt.Printf(\u0026#34;Sentiment: %s\\n\u0026#34;, result[\u0026#34;sentiment\u0026#34;]) }\rRunning the Code Save the code above as main.go, and run it:\ngo run main.go\rOutput:\nSentence: dspy-go makes building AI applications easy and fun!\rSentiment: Positive\rCongratulations! You\u0026rsquo;ve just built your first LLM application with dspy-go. üéâ\nWhat\u0026rsquo;s Next? Now that you have dspy-go running, explore these next steps:\nCore Concepts Learn about the building blocks of dspy-go:\nSignatures ‚Üí - Define task inputs and outputs Modules ‚Üí - Chain-of-Thought, ReAct, and more Programs ‚Üí - Compose modules into workflows Advanced Features Optimizers ‚Üí - Automatically improve prompts with GEPA, MIPRO, SIMBA Tool Management ‚Üí - Smart tool selection, chaining, and MCP integration Multimodal ‚Üí - Work with images, vision Q\u0026amp;A, and streaming Examples Check out real-world implementations:\nQuestion Answering Math Problem Solving Smart Tool Registry Tool Chaining Multimodal Processing Example Application See dspy-go in production:\nMaestro - A code review and Q\u0026amp;A agent built with dspy-go Alternative Configuration Methods While using empty string \u0026quot;\u0026quot; for zero-config is the easiest way to get started, you can also pass API keys explicitly:\nExplicit Configuration import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Anthropic Claude llm, err := llms.NewAnthropicLLM(\u0026#34;api-key\u0026#34;, core.ModelAnthropicSonnet) core.SetDefaultLLM(llm) // Google Gemini llm, err := llms.NewGeminiLLM(\u0026#34;api-key\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // OpenAI llm, err := llms.NewOpenAI(core.ModelOpenAIGPT4, \u0026#34;api-key\u0026#34;) core.SetDefaultLLM(llm) // Ollama (local) llm, err := llms.NewOllamaLLM(core.ModelOllamaLlama3_8B) core.SetDefaultLLM(llm)\rPer-Module Configuration // Use a specific LLM for a specific module llm, _ := llms.NewAnthropicLLM(\u0026#34;api-key\u0026#34;, core.ModelAnthropicSonnet) predictor := modules.NewPredict(signature) predictor.SetLLM(llm)\rTroubleshooting \u0026ldquo;Failed to configure LLM\u0026rdquo; Error Problem: LLM creation fails with \u0026ldquo;API key required\u0026rdquo; error.\nSolution: Make sure you\u0026rsquo;ve set the appropriate environment variable or pass the API key explicitly:\nGEMINI_API_KEY OPENAI_API_KEY ANTHROPIC_API_KEY OLLAMA_BASE_URL API Rate Limits Problem: Getting rate limit errors from your LLM provider.\nSolution: dspy-go includes built-in retry logic with exponential backoff. For heavy workloads, consider:\nUsing local models with Ollama Implementing request throttling Caching results with custom middleware Getting Help GitHub Issues: Report bugs or request features Discussions: Ask questions and share ideas Examples: Browse working examples ","date":"2025-10-13","id":2,"permalink":"/dspy-go/docs/guides/getting-started/","summary":"Install dspy-go and build your first LLM application","tags":[],"title":"Getting Started"},{"content":"Core Concepts dspy-go is built around three fundamental concepts that work together to create powerful LLM applications: Signatures, Modules, and Programs. Understanding these building blocks will help you build reliable, maintainable AI systems.\nSignatures Signatures define the contract between your application and the LLM. They specify what inputs the LLM needs and what outputs it should produce.\nWhy Signatures Matter Instead of crafting prompts manually, signatures let you:\nDefine clear expectations - What goes in, what comes out Type safety - Strong typing ensures correctness Reusability - Use the same signature across different modules Optimization - Optimizers can improve signatures automatically Creating a Signature Signatures are created using the core.NewSignature() function:\nsignature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;question\u0026#34;, core.WithDescription(\u0026#34;The question to answer\u0026#34;))}, {Field: core.NewField(\u0026#34;context\u0026#34;, core.WithDescription(\u0026#34;Background information\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;, core.WithDescription(\u0026#34;A concise, accurate answer\u0026#34;))}, {Field: core.NewField(\u0026#34;confidence\u0026#34;, core.WithDescription(\u0026#34;Confidence level: high/medium/low\u0026#34;))}, }, ).WithInstruction(\u0026#34;You are a helpful assistant. Answer the question accurately using the provided context.\u0026#34;)\rYou can also create simple signatures without descriptions:\nsignature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;question\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;)}, }, )\rField Descriptions Matter Important: Field descriptions aren\u0026rsquo;t just documentation‚Äîthey directly influence prompt quality and LLM behavior.\n// ‚ùå Weak description {Field: core.NewField(\u0026#34;sentiment\u0026#34;)} // ‚úÖ Strong description {Field: core.NewField(\u0026#34;sentiment\u0026#34;, core.WithDescription(\u0026#34;The emotional tone: Positive, Negative, or Neutral\u0026#34;))} // ‚úÖ Even better - guides the LLM\u0026#39;s reasoning {Field: core.NewField(\u0026#34;sentiment\u0026#34;, core.WithDescription(\u0026#34;Analyze the emotional tone. Consider word choice, context, and intensity. Return: Positive, Negative, or Neutral\u0026#34;))}\rModules Modules are the execution engines that take your signatures and turn them into working LLM applications. They encapsulate different reasoning patterns and behaviors.\nCore Modules 1. Predict - Direct Prediction The simplest module. Makes a single prediction based on your signature.\npredictor := modules.NewPredict(QuestionAnswerSignature{}) result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What is the capital of France?\u0026#34;, \u0026#34;context\u0026#34;: \u0026#34;France is a country in Western Europe.\u0026#34;, }) // result[\u0026#34;answer\u0026#34;] = \u0026#34;Paris\u0026#34; // result[\u0026#34;confidence\u0026#34;] = \u0026#34;high\u0026#34;\rWhen to use: Simple, single-step tasks where you need direct answers.\n2. ChainOfThought - Step-by-Step Reasoning Implements chain-of-thought reasoning, breaking down complex problems into steps.\ncot := modules.NewChainOfThought(signature) result, err := cot.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;If a train travels 120 miles in 2 hours, how far will it go in 5 hours?\u0026#34;, }) // result includes both: // - \u0026#34;rationale\u0026#34;: \u0026#34;Speed = 120/2 = 60 mph. Distance = 60 * 5 = 300 miles\u0026#34; // - \u0026#34;answer\u0026#34;: \u0026#34;300 miles\u0026#34;\rWhen to use: Math problems, logical reasoning, multi-step analysis.\nKey insight: ChainOfThought automatically adds a \u0026ldquo;rationale\u0026rdquo; field to guide the LLM\u0026rsquo;s thinking process.\n3. ReAct - Reasoning + Acting Combines reasoning with tool use. The LLM can call tools to gather information before answering.\n// Create tools calculator := tools.NewCalculatorTool() searchTool := tools.NewSearchTool() // Create registry registry := tools.NewInMemoryToolRegistry() registry.Register(calculator) registry.Register(searchTool) // Create ReAct module react := modules.NewReAct(signature, registry, 5) // max 5 iterations result, err := react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What is the population of Tokyo divided by 1000?\u0026#34;, }) // ReAct will: // 1. Reason: \u0026#34;I need to find Tokyo\u0026#39;s population\u0026#34; // 2. Act: Call search tool // 3. Reason: \u0026#34;Now I need to divide by 1000\u0026#34; // 4. Act: Call calculator // 5. Answer: \u0026#34;14,000 (approximately)\u0026#34;\rWhen to use: Questions requiring external data, calculations, or API calls.\n4. MultiChainComparison - Multi-Perspective Analysis Compares multiple reasoning attempts and synthesizes a comprehensive answer.\nmultiChain := modules.NewMultiChainComparison(signature, 3, 0.7) completions := []map[string]interface{}{ {\u0026#34;rationale\u0026#34;: \u0026#34;Cost-focused approach...\u0026#34;, \u0026#34;solution\u0026#34;: \u0026#34;Reduce expenses\u0026#34;}, {\u0026#34;rationale\u0026#34;: \u0026#34;Growth-focused approach...\u0026#34;, \u0026#34;solution\u0026#34;: \u0026#34;Invest in marketing\u0026#34;}, {\u0026#34;rationale\u0026#34;: \u0026#34;Balanced approach...\u0026#34;, \u0026#34;solution\u0026#34;: \u0026#34;Optimize both\u0026#34;}, } result, err := multiChain.Process(ctx, map[string]interface{}{ \u0026#34;problem\u0026#34;: \u0026#34;How should we improve business performance?\u0026#34;, \u0026#34;completions\u0026#34;: completions, }) // result contains synthesized recommendation considering all perspectives\rWhen to use: Complex decisions requiring multiple viewpoints.\n5. Refine - Quality Improvement Runs multiple attempts with different parameters and selects the best result.\nrewardFn := func(inputs, outputs map[string]interface{}) float64 { // Score based on answer length, completeness, etc. answer := outputs[\u0026#34;answer\u0026#34;].(string) return calculateQualityScore(answer) } refine := modules.NewRefine( modules.NewPredict(signature), modules.RefineConfig{ N: 5, // 5 attempts RewardFn: rewardFn, Threshold: 0.8, // Stop if quality \u0026gt; 0.8 }, ) result, err := refine.Process(ctx, inputs) // Returns the highest-quality result\rWhen to use: When quality is critical and you want the best possible answer.\n6. Parallel - Batch Processing Wraps any module for concurrent execution across multiple inputs.\nbaseModule := modules.NewPredict(signature) parallel := modules.NewParallel(baseModule, modules.WithMaxWorkers(4), // 4 concurrent workers modules.WithReturnFailures(false), // Skip failures ) batchInputs := []map[string]interface{}{ {\u0026#34;question\u0026#34;: \u0026#34;What is 2+2?\u0026#34;}, {\u0026#34;question\u0026#34;: \u0026#34;What is the capital of France?\u0026#34;}, {\u0026#34;question\u0026#34;: \u0026#34;What is the speed of light?\u0026#34;}, } result, err := parallel.Process(ctx, map[string]interface{}{ \u0026#34;batch_inputs\u0026#34;: batchInputs, }) // Process all inputs concurrently results := result[\u0026#34;results\u0026#34;].([]map[string]interface{})\rWhen to use: Batch processing, bulk operations, performance optimization.\n7. RLM - Recursive Language Model Enables LLMs to programmatically explore large contexts through a sandboxed Go REPL. The LLM iteratively writes and executes code, making sub-LLM queries until it reaches a final answer.\nimport \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules/rlm\u0026#34; // Create RLM from any LLM rlmModule := rlm.NewFromLLM(llm, rlm.WithMaxIterations(10), rlm.WithTimeout(5*time.Minute), ) // Analyze a large document result, err := rlmModule.Complete(ctx, largeDocument, \u0026#34;What are the key findings?\u0026#34;) // result.Response contains the answer // result.Iterations shows how many exploration steps were needed // result.Usage tracks total token consumption\rKey capabilities:\nQuery(prompt) - Make sub-LLM calls from within REPL code QueryBatched(prompts) - Parallel sub-LLM calls for efficiency Sandboxed execution (no os, net, syscall access) Separate token tracking for root and sub-LLM calls When to use: Documents exceeding context limits, complex multi-step analysis, programmatic data exploration.\nModule Composition Modules can be composed to create sophisticated workflows:\n// Combine ChainOfThought with Refine for high-quality reasoning cotModule := modules.NewChainOfThought(signature) refinedCot := modules.NewRefine(cotModule, refineConfig) // Wrap with Parallel for batch high-quality reasoning parallelRefinedCot := modules.NewParallel(refinedCot, parallelConfig)\rPrograms Programs orchestrate multiple modules into complete workflows. They define how data flows through your system.\nCreating a Program program := core.NewProgram( map[string]core.Module{ \u0026#34;retriever\u0026#34;: retrieverModule, \u0026#34;generator\u0026#34;: generatorModule, }, func(ctx context.Context, inputs map[string]interface{}) (map[string]interface{}, error) { // Step 1: Retrieve relevant documents retrieverResult, err := retrieverModule.Process(ctx, inputs) if err != nil { return nil, err } // Step 2: Generate answer using retrieved documents generatorInputs := map[string]interface{}{ \u0026#34;question\u0026#34;: inputs[\u0026#34;question\u0026#34;], \u0026#34;documents\u0026#34;: retrieverResult[\u0026#34;documents\u0026#34;], } return generatorModule.Process(ctx, generatorInputs) }, ) // Execute the program result, err := program.Execute(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What are the benefits of Go for LLM applications?\u0026#34;, })\rRAG (Retrieval-Augmented Generation) Example A complete RAG pipeline demonstrating program composition:\n// Define signatures retrievalSig := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;query\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;documents\u0026#34;)}, }, ) generationSig := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;question\u0026#34;)}, {Field: core.NewField(\u0026#34;documents\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;)}, }, ) // Create modules retriever := modules.NewPredict(retrievalSig) generator := modules.NewChainOfThought(generationSig) // Compose into RAG program ragProgram := core.NewProgram( map[string]core.Module{ \u0026#34;retriever\u0026#34;: retriever, \u0026#34;generator\u0026#34;: generator, }, func(ctx context.Context, inputs map[string]interface{}) (map[string]interface{}, error) { // Retrieval phase docs, err := retriever.Process(ctx, map[string]interface{}{ \u0026#34;query\u0026#34;: inputs[\u0026#34;question\u0026#34;], }) if err != nil { return nil, err } // Generation phase return generator.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: inputs[\u0026#34;question\u0026#34;], \u0026#34;documents\u0026#34;: docs[\u0026#34;documents\u0026#34;], }) }, ) // Use the program answer, err := ragProgram.Execute(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;How does dspy-go handle tool management?\u0026#34;, })\rProgram Optimization Key feature: Programs can be optimized automatically:\n// Create MIPRO optimizer optimizer := optimizers.NewMIPRO( metricFunc, optimizers.WithNumTrials(10), ) // Optimize the entire program optimizedProgram, err := optimizer.Compile(ctx, ragProgram, dataset, nil) // optimizedProgram now has improved prompts and parameters\rPutting It All Together Here\u0026rsquo;s a complete example showing Signatures, Modules, and Programs working together:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // 1. Configure LLM llm, err := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // 2. Define Signature signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;text\u0026#34;, core.WithDescription(\u0026#34;The text to analyze\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;summary\u0026#34;, core.WithDescription(\u0026#34;A concise summary\u0026#34;))}, {Field: core.NewField(\u0026#34;sentiment\u0026#34;, core.WithDescription(\u0026#34;Overall sentiment: Positive/Negative/Neutral\u0026#34;))}, {Field: core.NewField(\u0026#34;key_points\u0026#34;, core.WithDescription(\u0026#34;Main takeaways as bullet points\u0026#34;))}, }, ).WithInstruction(\u0026#34;Analyze the provided text thoroughly and extract key insights.\u0026#34;) // 3. Create Modules analyzer := modules.NewChainOfThought(signature) // 4. Create Program program := core.NewProgram( map[string]core.Module{\u0026#34;analyzer\u0026#34;: analyzer}, func(ctx context.Context, inputs map[string]interface{}) (map[string]interface{}, error) { return analyzer.Process(ctx, inputs) }, ) // 5. Execute result, err := program.Execute(context.Background(), map[string]interface{}{ \u0026#34;text\u0026#34;: \u0026#34;dspy-go provides a systematic approach to building LLM applications...\u0026#34;, }) if err != nil { log.Fatal(err) } // 6. Use Results fmt.Printf(\u0026#34;Summary: %s\\n\u0026#34;, result[\u0026#34;summary\u0026#34;]) fmt.Printf(\u0026#34;Sentiment: %s\\n\u0026#34;, result[\u0026#34;sentiment\u0026#34;]) fmt.Printf(\u0026#34;Key Points: %s\\n\u0026#34;, result[\u0026#34;key_points\u0026#34;]) }\rBest Practices Signature Design ‚úÖ DO:\nWrite detailed, specific field descriptions Include examples in descriptions when helpful Use meaningful field names Specify expected output formats ‚ùå DON\u0026rsquo;T:\nLeave descriptions empty Use vague instructions Change signatures frequently (breaks optimization) Over-complicate with too many fields Module Selection Use Case Recommended Module Simple Q\u0026amp;A Predict Math/Logic ChainOfThought Tool Use ReAct Quality-Critical Refine Batch Processing Parallel Complex Decisions MultiChainComparison Large Context Analysis RLM Program Structure ‚úÖ DO:\nKeep workflows simple and linear when possible Handle errors at each step Log intermediate results for debugging Use context for cancellation and timeouts ‚ùå DON\u0026rsquo;T:\nCreate circular dependencies Ignore error handling Make programs too deeply nested Forget to pass context through What\u0026rsquo;s Next? Now that you understand the core concepts, explore:\nOptimizers ‚Üí - Automatically improve your signatures and programs Tool Management ‚Üí - Extend ReAct with smart tool selection Examples - See these concepts in real applications Deep Dives Signatures: Field Options, Advanced Patterns Modules: Custom Modules, Module Configuration Programs: Workflows, Error Handling ","date":"2025-10-13","id":3,"permalink":"/dspy-go/docs/guides/core-concepts/","summary":"Learn about Signatures, Modules, and Programs - the foundation of every dspy-go application","tags":[],"title":"Core Concepts"},{"content":"Optimizers Optimizers are one of dspy-go\u0026rsquo;s most powerful features. Instead of manually tweaking prompts, optimizers automatically improve your signatures, instructions, and examples based on your dataset and metrics.\nWhy Optimize? Manual prompt engineering is:\nTime-consuming: Hours of trial and error Subjective: What works for you might not work universally Brittle: Small changes can break everything Un-scalable: Hard to improve systematically Optimizers solve this by:\n‚úÖ Automatically testing variations ‚úÖ Using data to find what works ‚úÖ Systematically improving performance ‚úÖ Producing reproducible results Quick Optimizer Comparison Optimizer Best For Speed Quality Complexity BootstrapFewShot Quick wins, simple tasks ‚ö°‚ö°‚ö° ‚≠ê‚≠ê‚≠ê Low COPRO Multi-module systems ‚ö°‚ö° ‚≠ê‚≠ê‚≠ê‚≠ê Medium MIPRO Systematic optimization ‚ö°‚ö° ‚≠ê‚≠ê‚≠ê‚≠ê Medium SIMBA Introspective learning ‚ö° ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê High GEPA Multi-objective evolution ‚ö° ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê High GEPA - Generative Evolutionary Prompt Adaptation The most advanced optimizer in dspy-go. Uses evolutionary algorithms with multi-objective Pareto optimization and LLM-based self-reflection.\nWhat Makes GEPA Special? üß¨ Evolutionary Optimization: Genetic algorithms for prompt evolution üéØ Multi-Objective: Optimizes across 7 dimensions simultaneously üèÜ Pareto Selection: Maintains diverse solutions for different trade-offs üß† LLM Self-Reflection: Uses LLMs to analyze and critique prompts üìä Elite Archive: Preserves high-quality solutions across generations The 7 Optimization Dimensions Success Rate: How often the program succeeds Quality: Answer accuracy and completeness Efficiency: Token usage and speed Robustness: Performance across edge cases Generalization: How well it handles new inputs Diversity: Variety in reasoning approaches Innovation: Novel problem-solving strategies When to Use GEPA ‚úÖ Perfect for:\nComplex, multi-faceted problems When you need multiple solution strategies Production systems requiring robustness Trade-offs between speed and quality ‚ùå Avoid for:\nSimple, single-objective tasks Very tight time constraints Limited compute resources GEPA Example package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/optimizers\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/datasets\u0026#34; ) func main() { // 1. Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // 2. Create your program program := createMyProgram() // Your application // 3. Load dataset dataset, _ := datasets.LoadGSM8K(\u0026#34;path/to/gsm8k\u0026#34;) // 4. Define metric metricFunc := func(example, prediction map[string]interface{}, trace *core.Trace) (float64, map[string]interface{}) { expected := example[\u0026#34;answer\u0026#34;].(string) actual := prediction[\u0026#34;answer\u0026#34;].(string) if expected == actual { return 1.0, map[string]interface{}{\u0026#34;correct\u0026#34;: true} } return 0.0, map[string]interface{}{\u0026#34;correct\u0026#34;: false} } // 5. Configure GEPA config := \u0026amp;optimizers.GEPAConfig{ PopulationSize: 20, // Size of population MaxGenerations: 10, // Number of generations SelectionStrategy: \u0026#34;adaptive_pareto\u0026#34;, // Multi-objective Pareto MutationRate: 0.3, // Mutation probability CrossoverRate: 0.7, // Crossover probability ReflectionFreq: 2, // LLM reflection every 2 generations ElitismRate: 0.1, // Preserve top 10% } gepa, err := optimizers.NewGEPA(config) if err != nil { log.Fatal(err) } // 6. Optimize ctx := context.Background() optimizedProgram, err := gepa.Compile(ctx, program, dataset, metricFunc) if err != nil { log.Fatal(err) } // 7. Access results state := gepa.GetOptimizationState() fmt.Printf(\u0026#34;Best fitness: %.3f\\n\u0026#34;, state.BestFitness) fmt.Printf(\u0026#34;Generations: %d\\n\u0026#34;, state.CurrentGeneration) // 8. Get Pareto archive (multiple solutions optimized for different trade-offs) archive := state.GetParetoArchive() fmt.Printf(\u0026#34;Elite solutions: %d\\n\u0026#34;, len(archive)) // Each solution in archive excels in different ways: // - Some optimized for speed // - Some optimized for accuracy // - Some balanced between both }\rGEPA Configuration Guide // Quick optimization (5-10 minutes) config := \u0026amp;optimizers.GEPAConfig{ PopulationSize: 10, MaxGenerations: 5, SelectionStrategy: \u0026#34;adaptive_pareto\u0026#34;, MutationRate: 0.3, CrossoverRate: 0.7, ReflectionFreq: 3, } // Balanced optimization (15-30 minutes) config := \u0026amp;optimizers.GEPAConfig{ PopulationSize: 20, MaxGenerations: 10, SelectionStrategy: \u0026#34;adaptive_pareto\u0026#34;, MutationRate: 0.3, CrossoverRate: 0.7, ReflectionFreq: 2, } // Deep optimization (1-2 hours) config := \u0026amp;optimizers.GEPAConfig{ PopulationSize: 40, MaxGenerations: 20, SelectionStrategy: \u0026#34;adaptive_pareto\u0026#34;, MutationRate: 0.2, CrossoverRate: 0.8, ReflectionFreq: 1, ElitismRate: 0.15, }\rMIPRO - Multi-step Interactive Prompt Optimization Systematic optimization using TPE (Tree-structured Parzen Estimator) search. MIPRO is ideal for methodical, data-driven prompt improvement.\nWhat Makes MIPRO Special? üéØ TPE Search: Bayesian optimization for efficient exploration üìä Systematic: Methodical testing of variations ‚ö° Multiple Modes: Light, Medium, Heavy optimization üîç Interpretable: Clear insights into what works When to Use MIPRO ‚úÖ Perfect for:\nSystematic, reproducible optimization When you have a good dataset (50+ examples) Medium-complexity tasks Need explainable improvements ‚ùå Avoid for:\nVery simple tasks (Bootstrap is faster) Extremely complex multi-objective problems (use GEPA) Limited datasets (\u0026lt; 20 examples) MIPRO Example package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/optimizers\u0026#34; ) func main() { // Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Create MIPRO optimizer mipro := optimizers.NewMIPRO( metricFunc, optimizers.WithMode(optimizers.LightMode), // Fast optimization optimizers.WithNumTrials(10), // Number of trials optimizers.WithTPEGamma(0.25), // Exploration parameter optimizers.WithMinExamplesPerModule(5), // Min examples needed ) // Optimize program optimizedProgram, err := mipro.Compile(ctx, program, dataset, nil) if err != nil { log.Fatal(err) } // Use optimized program result, _ := optimizedProgram.Execute(ctx, inputs) }\rMIPRO Modes // Light Mode - Quick optimization (5-10 minutes) // - Fewer trials // - Faster convergence // - Good for iteration optimizers.WithMode(optimizers.LightMode) // Medium Mode - Balanced (15-30 minutes) // - Moderate trials // - Better quality // - Production-ready optimizers.WithMode(optimizers.MediumMode) // Heavy Mode - Thorough (30-60 minutes) // - Many trials // - Highest quality // - Critical systems optimizers.WithMode(optimizers.HeavyMode)\rSIMBA - Stochastic Introspective Mini-Batch Ascent Introspective learning with self-analysis. SIMBA learns from its own optimization process.\nWhat Makes SIMBA Special? üß† Introspection: Analyzes its own optimization progress üì¶ Mini-Batch: Stochastic optimization for efficiency üìà Adaptive: Adjusts strategy based on progress üí° Insights: Provides detailed learning analysis When to Use SIMBA ‚úÖ Perfect for:\nComplex reasoning tasks When you want insights into the optimization process Iterative improvement workflows Research and experimentation ‚ùå Avoid for:\nSimple tasks Very limited compute Need for speed over quality SIMBA Example package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/optimizers\u0026#34; ) func main() { // Create SIMBA optimizer simba := optimizers.NewSIMBA( optimizers.WithSIMBABatchSize(8), // Mini-batch size optimizers.WithSIMBAMaxSteps(12), // Max optimization steps optimizers.WithSIMBANumCandidates(6), // Candidates per iteration optimizers.WithSamplingTemperature(0.2), // Exploration vs exploitation ) // Optimize optimizedProgram, err := simba.Compile(ctx, program, dataset, metricFunc) if err != nil { log.Fatal(err) } // Get introspective insights state := simba.GetState() fmt.Printf(\u0026#34;Steps completed: %d\\n\u0026#34;, state.CurrentStep) fmt.Printf(\u0026#34;Best score: %.3f\\n\u0026#34;, state.BestScore) // View detailed analysis for i, insight := range state.IntrospectionLog { fmt.Printf(\u0026#34;Insight %d: %s\\n\u0026#34;, i+1, insight) } // Example insights SIMBA might provide: // - \u0026#34;Longer instructions improved performance by 15%\u0026#34; // - \u0026#34;Adding context examples reduced errors in edge cases\u0026#34; // - \u0026#34;Temperature 0.7 balanced creativity and accuracy\u0026#34; }\rBootstrapFewShot - Quick Example Selection Fast and effective for most tasks. Automatically selects high-quality few-shot examples.\nWhat Makes Bootstrap Special? ‚ö° Fast: Quickest optimizer üéØ Effective: Works well for most tasks üìö Few-Shot Learning: Automatic example selection üîÑ Simple: Easy to use and understand When to Use Bootstrap ‚úÖ Perfect for:\nGetting started with optimization Simple to medium complexity tasks Quick iterations Proof of concepts ‚ùå Avoid for:\nVery complex multi-step reasoning Need for multi-objective optimization Research on optimization strategies Bootstrap Example package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/optimizers\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/metrics\u0026#34; ) func main() { // Create Bootstrap optimizer bootstrap := optimizers.NewBootstrapFewShot( dataset, metrics.NewExactMatchMetric(\u0026#34;answer\u0026#34;), optimizers.WithMaxBootstrappedDemos(5), // Max examples optimizers.WithMaxLabeledDemos(3), // Max labeled examples ) // Optimize module optimizedModule, err := bootstrap.Optimize(ctx, originalModule) if err != nil { log.Fatal(err) } // Use optimized module result, _ := optimizedModule.Process(ctx, inputs) }\rCOPRO - Collaborative Prompt Optimization Multi-module optimization for complex systems with multiple components.\nWhen to Use COPRO ‚úÖ Perfect for:\nSystems with multiple modules RAG pipelines Multi-step workflows Coordinated optimization COPRO Example package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/optimizers\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/metrics\u0026#34; ) func main() { // Create COPRO optimizer copro := optimizers.NewCopro( dataset, metrics.NewRougeMetric(\u0026#34;answer\u0026#34;), ) // Optimize module (automatically handles multi-module systems) optimizedModule, err := copro.Optimize(ctx, originalModule) if err != nil { log.Fatal(err) } }\rChoosing the Right Optimizer Decision Tree Start here\r‚Üì\rIs it a simple task? ‚Üí YES ‚Üí Use Bootstrap\r‚Üì NO\rMultiple modules? ‚Üí YES ‚Üí Use COPRO\r‚Üì NO\rNeed multi-objective? ‚Üí YES ‚Üí Use GEPA\r‚Üì NO\rWant introspection? ‚Üí YES ‚Üí Use SIMBA\r‚Üì NO\rUse MIPRO (default choice)\rBy Use Case Use Case Recommended Optimizer Getting Started BootstrapFewShot Simple Q\u0026amp;A BootstrapFewShot Math/Logic MIPRO (Medium Mode) RAG Pipelines COPRO or MIPRO Complex Reasoning SIMBA or GEPA Production Critical GEPA (Deep optimization) Research SIMBA or GEPA Multi-Module Systems COPRO Metrics - Measuring Success Every optimizer needs a metric to optimize for:\nBuilt-in Metrics import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/metrics\u0026#34; // Exact match exactMatch := metrics.NewExactMatchMetric(\u0026#34;answer\u0026#34;) // ROUGE score (for summarization) rouge := metrics.NewRougeMetric(\u0026#34;summary\u0026#34;) // F1 score f1 := metrics.NewF1Metric(\u0026#34;prediction\u0026#34;, \u0026#34;label\u0026#34;)\rCustom Metrics // Define custom metric function metricFunc := func(example, prediction map[string]interface{}, trace *core.Trace) (float64, map[string]interface{}) { expected := example[\u0026#34;answer\u0026#34;].(string) actual := prediction[\u0026#34;answer\u0026#34;].(string) // Simple exact match if expected == actual { return 1.0, map[string]interface{}{\u0026#34;correct\u0026#34;: true} } // Partial credit for close answers similarity := calculateSimilarity(expected, actual) return similarity, map[string]interface{}{ \u0026#34;correct\u0026#34;: false, \u0026#34;similarity\u0026#34;: similarity, } }\rBest Practices Dataset Preparation ‚úÖ DO:\nUse diverse, representative examples Include edge cases Aim for 50+ examples for MIPRO/SIMBA Balance positive and negative cases ‚ùå DON\u0026rsquo;T:\nUse only simple examples Ignore data quality Optimize on test set Mix different task types Optimization Strategy Start Small: Use Bootstrap first Measure: Establish baseline performance Iterate: Try MIPRO for systematic improvement Specialize: Use SIMBA/GEPA for complex needs Validate: Test on held-out data Avoiding Overfitting Use train/validation split Don\u0026rsquo;t over-optimize (stop when validation plateaus) Test on diverse examples Monitor generalization metrics CLI Tool - Zero Code Optimization Try all optimizers without writing code:\n# Build CLI cd cmd/dspy-cli \u0026amp;\u0026amp; go build # Try Bootstrap ./dspy-cli try bootstrap --dataset gsm8k --max-examples 10 # Try MIPRO ./dspy-cli try mipro --dataset gsm8k --verbose # Try GEPA ./dspy-cli try gepa --dataset hotpotqa --max-examples 20 # Compare optimizers ./dspy-cli compare --dataset gsm8k --optimizers bootstrap,mipro,gepa\rWhat\u0026rsquo;s Next? Core Concepts - Understand what optimizers improve Examples - See optimizers in action Compatibility Testing - Verify optimizer behavior Example Applications MIPRO Example SIMBA Example GEPA Example GSM8K with Bootstrap ","date":"2025-10-13","id":4,"permalink":"/dspy-go/docs/guides/optimizers/","summary":"Master GEPA, MIPRO, SIMBA, and other advanced optimizers to systematically enhance prompt performance","tags":[],"title":"Optimizers"},{"content":"Interceptors Interceptors provide composable middleware for modules, agents, and tools. They follow the gRPC interceptor pattern, allowing you to add cross-cutting concerns like logging, caching, security, and structured output parsing without modifying core logic.\nArchitecture Each interceptor can:\nInspect and modify inputs before execution Inspect and modify outputs after execution Handle errors and implement fallback logic Collect metrics and perform logging Implement security policies Input ‚Üí [Security] ‚Üí [Logging] ‚Üí [Retry] ‚Üí [Cache] ‚Üí LLM ‚Üí [Parse] ‚Üí [Metrics] ‚Üí Output\rQuick Start import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/interceptors\u0026#34; // Apply single interceptor module.SetInterceptors([]core.ModuleInterceptor{ interceptors.LoggingModuleInterceptor(), }) // Apply multiple interceptors module.SetInterceptors([]core.ModuleInterceptor{ interceptors.LoggingModuleInterceptor(), interceptors.MetricsModuleInterceptor(), interceptors.RetryModuleInterceptor(retryConfig), })\rStandard Interceptors Logging // Module logging with timing interceptors.LoggingModuleInterceptor() // Agent logging interceptors.LoggingAgentInterceptor() // Tool logging with arguments interceptors.LoggingToolInterceptor()\rTracing // Distributed tracing spans interceptors.TracingModuleInterceptor() interceptors.TracingAgentInterceptor() interceptors.TracingToolInterceptor()\rMetrics // Performance metrics collection interceptors.MetricsModuleInterceptor() interceptors.MetricsAgentInterceptor() interceptors.MetricsToolInterceptor()\rPerformance Interceptors Caching // Create memory cache with TTL cache := interceptors.NewMemoryCache() // Cache module results interceptors.CachingModuleInterceptor(cache, 10*time.Minute) // Cache tool results interceptors.CachingToolInterceptor(cache, 5*time.Minute)\rTimeouts // Enforce execution timeouts interceptors.TimeoutModuleInterceptor(30*time.Second) interceptors.TimeoutAgentInterceptor(2*time.Minute) interceptors.TimeoutToolInterceptor(10*time.Second)\rCircuit Breakers Prevent cascade failures:\n// Create circuit breaker // (failure threshold, recovery timeout, half-open requests) cb := interceptors.NewCircuitBreaker(5, 30*time.Second, 10) // Apply to module interceptors.CircuitBreakerModuleInterceptor(cb) interceptors.CircuitBreakerAgentInterceptor(cb) interceptors.CircuitBreakerToolInterceptor(cb)\rRetry Logic retryConfig := interceptors.RetryConfig{ MaxAttempts: 3, // Retry up to 3 times Delay: 100*time.Millisecond, // Initial delay MaxBackoff: 5*time.Second, // Maximum delay Backoff: 2.0, // Exponential backoff } interceptors.RetryModuleInterceptor(retryConfig) interceptors.RetryAgentInterceptor(retryConfig) interceptors.RetryToolInterceptor(retryConfig)\rSecurity Interceptors Rate Limiting // Limit to 100 requests per minute interceptors.RateLimitingAgentInterceptor(100, time.Minute) interceptors.RateLimitingToolInterceptor(50, time.Minute)\rInput Validation validationConfig := interceptors.ValidationConfig{ MaxInputSize: 10*1024*1024, // 10MB limit MaxStringLength: 100000, // 100KB per string ForbiddenPatterns: []string{ `(?i)\u0026lt;script[^\u0026gt;]*\u0026gt;.*?\u0026lt;/script\u0026gt;`, `(?i)javascript:`, `\\$\\{.*\\}`, }, RequiredFields: []string{\u0026#34;user_id\u0026#34;}, AllowHTML: false, } interceptors.ValidationModuleInterceptor(validationConfig) interceptors.ValidationAgentInterceptor(validationConfig) interceptors.ValidationToolInterceptor(validationConfig)\rAuthorization authInterceptor := interceptors.NewAuthorizationInterceptor() authInterceptor.SetPolicy(\u0026#34;sensitive_module\u0026#34;, interceptors.AuthorizationPolicy{ RequiredRoles: []string{\u0026#34;admin\u0026#34;, \u0026#34;power_user\u0026#34;}, RequireAuth: true, }) authInterceptor.ModuleAuthorizationInterceptor() authInterceptor.AgentAuthorizationInterceptor() authInterceptor.ToolAuthorizationInterceptor()\rInput Sanitization // Sanitize potentially dangerous inputs interceptors.SanitizingModuleInterceptor() interceptors.SanitizingAgentInterceptor() interceptors.SanitizingToolInterceptor()\rXML Interceptors XML interceptors provide structured output parsing as an alternative to JSON. They\u0026rsquo;re useful when you need fine-grained control over parsing behavior, security limits, or want to compose with other interceptors.\nBasic Usage // Create a module predict := modules.NewPredict(signature) // Apply XML interceptors with default config config := interceptors.DefaultXMLConfig() err := interceptors.ApplyXMLInterceptors(predict, config) // Process as normal - outputs are parsed from XML result, err := predict.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What is machine learning?\u0026#34;, }) // Structured output fields extracted reliably fmt.Println(result[\u0026#34;answer\u0026#34;]) fmt.Println(result[\u0026#34;confidence\u0026#34;])\rPreset Configurations // Balanced configuration for general use config := interceptors.DefaultXMLConfig() // Strict parsing - requires all fields, no fallback config := interceptors.StrictXMLConfig() // Flexible - allows missing fields, has fallback config := interceptors.FlexibleXMLConfig() // Performance optimized config := interceptors.PerformantXMLConfig() // Enhanced security restrictions config := interceptors.SecureXMLConfig()\rCustom Configuration config := \u0026amp;interceptors.XMLConfig{ // Parsing behavior StrictParsing: true, // Require all output fields FallbackToText: true, // Use text parsing if XML fails ValidateXML: true, // Validate XML syntax // Security limits MaxDepth: 10, // Maximum nesting depth MaxSize: 1024 * 1024, // Maximum XML size (1MB) Timeout: 30 * time.Second, // Customization CustomTags: map[string]string{ \u0026#34;answer\u0026#34;: \u0026#34;response\u0026#34;, // Use \u0026lt;response\u0026gt; instead of \u0026lt;answer\u0026gt; }, TypeHints: true, // Include type info in instructions }\rXML vs Structured Output Feature XML Interceptor .WithStructuredOutput() Output Format XML tags Native JSON Security Controls Depth/size limits Provider-dependent Fallback Configurable Built-in Interceptor Pattern Yes - composable No Provider Support All providers Providers with JSON mode Interceptor Chaining Recommended Order chain := []core.ModuleInterceptor{ // 1. Security (first line of defense) authInterceptor.ModuleAuthorizationInterceptor(), interceptors.ValidationModuleInterceptor(validationConfig), interceptors.SanitizingModuleInterceptor(), // 2. Observability interceptors.LoggingModuleInterceptor(), interceptors.TracingModuleInterceptor(), // 3. Reliability interceptors.RetryModuleInterceptor(retryConfig), interceptors.CircuitBreakerModuleInterceptor(cb), // 4. Performance interceptors.CachingModuleInterceptor(cache, 5*time.Minute), interceptors.TimeoutModuleInterceptor(30*time.Second), // 5. Content Processing interceptors.XMLFormatModuleInterceptor(xmlConfig), interceptors.XMLParseModuleInterceptor(xmlConfig), // 6. Metrics (capture final state) interceptors.MetricsModuleInterceptor(), } module.SetInterceptors(chain)\rProduction Pipeline Example // Create caching cache := interceptors.NewMemoryCache() // Create circuit breaker cb := interceptors.NewCircuitBreaker(5, 30*time.Second, 10) // Create retry config retryConfig := interceptors.RetryConfig{ MaxAttempts: 3, Delay: 100*time.Millisecond, Backoff: 2.0, } // Build production chain chain := []core.ModuleInterceptor{ interceptors.LoggingModuleInterceptor(), interceptors.ValidationModuleInterceptor(interceptors.DefaultValidationConfig()), interceptors.RetryModuleInterceptor(retryConfig), interceptors.CircuitBreakerModuleInterceptor(cb), interceptors.CachingModuleInterceptor(cache, 10*time.Minute), interceptors.TimeoutModuleInterceptor(30*time.Second), interceptors.MetricsModuleInterceptor(), } predict := modules.NewPredict(signature) predict.SetInterceptors(chain)\rCustom Interceptors Create your own interceptors:\nfunc CustomModuleInterceptor(config CustomConfig) core.ModuleInterceptor { return func(ctx context.Context, inputs map[string]any, info *core.ModuleInfo, handler core.ModuleHandler, opts ...core.Option) (map[string]any, error) { // Pre-processing modifiedInputs := preprocessInputs(inputs, config) // Execute handler outputs, err := handler(ctx, modifiedInputs, opts...) if err != nil { return nil, err } // Post-processing finalOutputs := postprocessOutputs(outputs, config) return finalOutputs, nil } }\rPerformance Based on benchmarks:\nOperation Performance Throughput XML Parsing ~3,663 ns/op 273K ops/sec Full Pipeline ~5,727 ns/op 175K ops/sec Cache Hit Near-zero - Logging Minimal overhead - Best Practices 1. Order matters - Security first, metrics last\n2. Use fallbacks in production\nxmlConfig := interceptors.FlexibleXMLConfig() // Has fallback\r3. Always include metrics\nchain := []core.ModuleInterceptor{ interceptors.MetricsModuleInterceptor(), // ... other interceptors }\r4. Defense in depth\nsecurityChain := []core.ModuleInterceptor{ authInterceptor, validationInterceptor, sanitizationInterceptor, rateLimitInterceptor, }\rNext Steps Core Concepts - Understand modules and signatures Building Agents - Use interceptors with agents Tool Management - Apply interceptors to tools ","date":"2025-01-06","id":5,"permalink":"/dspy-go/docs/guides/interceptors/","summary":"Add logging, caching, security, XML parsing, and more to your pipelines with the interceptor pattern","tags":[],"title":"Interceptors"},{"content":"Tool Management dspy-go provides a sophisticated tool management system that goes far beyond basic function calling. Build intelligent agents with Bayesian tool selection, automatic dependency resolution, and seamless MCP integration.\nSmart Tool Registry Intelligent tool selection using Bayesian inference and performance tracking.\nWhy Smart Tool Registry? Traditional tool systems pick tools randomly or use simple rules. Smart Tool Registry:\nüß† Bayesian Inference: Multi-factor scoring for optimal tool selection üìä Performance Tracking: Real-time metrics and reliability scoring üîç Capability Analysis: Automatic capability extraction and matching üîÑ Auto-Discovery: Dynamic tool registration from MCP servers üõ°Ô∏è Fallback Mechanisms: Intelligent fallback when tools fail Basic Usage package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) func main() { // Create Smart Tool Registry config := \u0026amp;tools.SmartToolRegistryConfig{ AutoDiscoveryEnabled: true, // Auto-discover from MCP PerformanceTrackingEnabled: true, // Track tool metrics FallbackEnabled: true, // Intelligent fallback } registry := tools.NewSmartToolRegistry(config) // Register tools registry.Register(mySearchTool) registry.Register(myAnalysisTool) // Intelligent tool selection based on intent ctx := context.Background() tool, err := registry.SelectBest(ctx, \u0026#34;find user information\u0026#34;) if err != nil { log.Fatal(err) } // Execute with performance tracking result, err := registry.ExecuteWithTracking(ctx, tool.Name(), params) }\rFull Smart Tool Registry Example ‚Üí\nTool Chaining Sequential pipelines with data transformation and conditional execution.\nPipeline Builder Create sophisticated workflows with a fluent API:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) func main() { // Create a tool pipeline pipeline, err := tools.NewPipelineBuilder(\u0026#34;data_processing\u0026#34;, registry). Step(\u0026#34;data_extractor\u0026#34;). // Extract data StepWithTransformer(\u0026#34;data_validator\u0026#34;, // Validate tools.TransformExtractField(\u0026#34;result\u0026#34;)). ConditionalStep(\u0026#34;data_enricher\u0026#34;, // Conditional enrichment tools.ConditionExists(\u0026#34;validation_result\u0026#34;), tools.ConditionEquals(\u0026#34;status\u0026#34;, \u0026#34;validated\u0026#34;)). StepWithRetries(\u0026#34;data_transformer\u0026#34;, 3). // Transform with retries FailFast(). // Stop on first error EnableCaching(). // Cache results Build() if err != nil { log.Fatal(err) } // Execute the pipeline ctx := context.Background() result, err := pipeline.Execute(ctx, map[string]interface{}{ \u0026#34;raw_data\u0026#34;: \u0026#34;input data to process\u0026#34;, }) }\rData Transformations Transform data between pipeline steps:\n// Extract specific fields transformer := tools.TransformExtractField(\u0026#34;important_field\u0026#34;) // Rename fields transformer := tools.TransformRename(map[string]string{ \u0026#34;old_name\u0026#34;: \u0026#34;new_name\u0026#34;, }) // Chain multiple transformations transformer := tools.TransformChain( tools.TransformRename(map[string]string{\u0026#34;status\u0026#34;: \u0026#34;processing_status\u0026#34;}), tools.TransformAddConstant(map[string]interface{}{\u0026#34;pipeline_id\u0026#34;: \u0026#34;001\u0026#34;}), tools.TransformFilter([]string{\u0026#34;result\u0026#34;, \u0026#34;pipeline_id\u0026#34;, \u0026#34;processing_status\u0026#34;}), )\rFull Tool Chaining Example ‚Üí\nDependency Resolution Automatic execution planning with parallel optimization.\nDependency Graph Define tool dependencies and let dspy-go optimize execution:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) func main() { // Create dependency graph graph := tools.NewDependencyGraph() // Define tool dependencies graph.AddNode(\u0026amp;tools.DependencyNode{ ToolName: \u0026#34;data_extractor\u0026#34;, Dependencies: []string{}, // No dependencies Outputs: []string{\u0026#34;raw_data\u0026#34;}, Priority: 1, }) graph.AddNode(\u0026amp;tools.DependencyNode{ ToolName: \u0026#34;data_validator\u0026#34;, Dependencies: []string{\u0026#34;data_extractor\u0026#34;}, // Depends on extractor Inputs: []string{\u0026#34;raw_data\u0026#34;}, Outputs: []string{\u0026#34;validated_data\u0026#34;}, Priority: 2, }) graph.AddNode(\u0026amp;tools.DependencyNode{ ToolName: \u0026#34;data_analyzer\u0026#34;, Dependencies: []string{\u0026#34;data_validator\u0026#34;}, // Depends on validator Inputs: []string{\u0026#34;validated_data\u0026#34;}, Outputs: []string{\u0026#34;analysis\u0026#34;}, Priority: 3, }) // Create dependency-aware pipeline options := \u0026amp;tools.DependencyPipelineOptions{ MaxParallelism: 4, // Run up to 4 tools in parallel EnableCaching: true, } depPipeline, err := tools.NewDependencyPipeline( \u0026#34;smart_pipeline\u0026#34;, registry, graph, options, ) // Execute with automatic parallelization ctx := context.Background() result, err := depPipeline.ExecuteWithDependencies(ctx, input) }\rBenefits:\n‚ö° Automatic parallel execution of independent tools üéØ Topological sorting ensures correct execution order üíæ Result caching prevents redundant work üîÑ Retry logic for failed dependencies Parallel Execution High-performance parallel tool execution with intelligent scheduling.\nParallel Executor package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) func main() { // Create parallel executor executor := tools.NewParallelExecutor(registry, 4) // 4 workers // Define parallel tasks tasks := []*tools.ParallelTask{ { ID: \u0026#34;task1\u0026#34;, ToolName: \u0026#34;analyzer\u0026#34;, Input: data1, Priority: 1, }, { ID: \u0026#34;task2\u0026#34;, ToolName: \u0026#34;processor\u0026#34;, Input: data2, Priority: 2, }, { ID: \u0026#34;task3\u0026#34;, ToolName: \u0026#34;enricher\u0026#34;, Input: data3, Priority: 1, }, } // Execute with priority scheduling ctx := context.Background() results, err := executor.ExecuteParallel(ctx, tasks, \u0026amp;tools.PriorityScheduler{}) // Or use fair share scheduling results, err := executor.ExecuteParallel(ctx, tasks, tools.NewFairShareScheduler()) }\rScheduling Strategies:\nPriority Scheduler: High-priority tasks first Fair Share Scheduler: Equal CPU time for all tasks Custom Schedulers: Implement your own scheduling logic Tool Composition Create reusable composite tools by combining multiple tools into single units.\nBuilding Composite Tools package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) // Helper function to create composite tools func NewCompositeTool(name string, registry core.ToolRegistry, builder func(*tools.PipelineBuilder) *tools.PipelineBuilder) (*CompositeTool, error) { pipeline, err := builder(tools.NewPipelineBuilder(name+\u0026#34;_pipeline\u0026#34;, registry)).Build() if err != nil { return nil, err } return \u0026amp;CompositeTool{ name: name, pipeline: pipeline, }, nil } func main() { // Create a composite tool for text processing textProcessor, err := NewCompositeTool(\u0026#34;text_processor\u0026#34;, registry, func(builder *tools.PipelineBuilder) *tools.PipelineBuilder { return builder. Step(\u0026#34;text_uppercase\u0026#34;). Step(\u0026#34;text_reverse\u0026#34;). Step(\u0026#34;text_length\u0026#34;) }) // Register and use like any other tool registry.Register(textProcessor) result, err := textProcessor.Execute(ctx, input) // Use in other pipelines or compositions complexPipeline, err := tools.NewPipelineBuilder(\u0026#34;complex\u0026#34;, registry). Step(\u0026#34;text_processor\u0026#34;). // Using our composite tool Step(\u0026#34;final_formatter\u0026#34;). Build() }\rFull Tool Composition Example ‚Üí\nMCP Integration Model Context Protocol integration for accessing external tools and services.\nConnecting to MCP Servers package main import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; \u0026#34;github.com/XiaoConstantine/mcp-go/pkg/client\u0026#34; ) func main() { // Connect to an MCP server mcpClient, err := client.NewStdioClient(\u0026#34;path/to/mcp-server\u0026#34;) if err != nil { log.Fatal(err) } // Use with ReAct (requires InMemoryToolRegistry) registry := tools.NewInMemoryToolRegistry() err = tools.RegisterMCPTools(registry, mcpClient) if err != nil { log.Fatal(err) } // Create ReAct module with MCP tools react := modules.NewReAct(signature, registry, 5) }\rWith Smart Tool Registry // Use Smart Tool Registry for intelligent tool selection smartRegistry := tools.NewSmartToolRegistry(\u0026amp;tools.SmartToolRegistryConfig{ PerformanceTrackingEnabled: true, AutoDiscoveryEnabled: true, }) // Register MCP tools err = tools.RegisterMCPTools(smartRegistry, mcpClient) if err != nil { log.Fatal(err) } // Intelligent selection of MCP tools ctx := context.Background() selectedTool, err := smartRegistry.SelectBest(ctx, \u0026#34;analyze financial data\u0026#34;)\rKey Features Summary Feature Description Use Case Smart Registry Bayesian tool selection Optimal tool choice for any task Tool Chaining Sequential pipelines Multi-step data processing Dependency Resolution Automatic parallelization Complex workflow optimization Parallel Execution High-performance scheduling Batch operations Tool Composition Reusable composite tools Modular tool building MCP Integration External tool access Extend with any MCP server Examples Complete Examples Smart Tool Registry - Intelligent tool selection showcase Tool Chaining - Pipeline building and transformations Tool Composition - Creating composite tools Running the Examples # Smart Tool Registry cd examples/smart_tool_registry \u0026amp;\u0026amp; go run main.go # Tool Chaining cd examples/tool_chaining \u0026amp;\u0026amp; go run main.go # Tool Composition cd examples/tool_composition \u0026amp;\u0026amp; go run main.go\rNext Steps Agents Guide ‚Üí - Build agents using ReAct and tool management Core Concepts ‚Üí - Understand modules and programs Examples ‚Üí - More working examples ","date":"2025-10-13","id":6,"permalink":"/dspy-go/docs/guides/tool-management/","summary":"Build sophisticated agent workflows with intelligent tool management","tags":[],"title":"Tool Management"},{"content":"Building Agents dspy-go\u0026rsquo;s agent package provides powerful abstractions for building intelligent agents that can reason, use tools, maintain conversation history, and orchestrate complex workflows.\nAgent Architecture An agent in dspy-go combines:\nReAct Module: Reasoning + Acting pattern Tool Registry: Available tools the agent can use Memory: Conversation history and context Orchestrator: Task decomposition and coordination ReAct Pattern Reasoning and Acting - The foundation of intelligent agents.\nWhat is ReAct? ReAct combines:\nThought: The agent reasons about what to do Action: The agent uses a tool Observation: The agent sees the tool\u0026rsquo;s result Repeat: Until the task is complete Basic ReAct Agent package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; ) func main() { // Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Create tools calculator := tools.NewCalculatorTool() searchTool := tools.NewSearchTool() weatherTool := tools.NewWeatherTool() // Create tool registry registry := tools.NewInMemoryToolRegistry() registry.Register(calculator) registry.Register(searchTool) registry.Register(weatherTool) // Define signature for the agent\u0026#39;s task signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;question\u0026#34;, core.WithDescription(\u0026#34;The question to answer\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;, core.WithDescription(\u0026#34;The final answer\u0026#34;))}, }, ) // Create ReAct module react := modules.NewReAct( signature, registry, 5, // max iterations ) // Execute ctx := context.Background() result, err := react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What is the population of Tokyo divided by 1000?\u0026#34;, }) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;Answer: %s\\n\u0026#34;, result[\u0026#34;answer\u0026#34;]) }\rHow ReAct Works Given the question \u0026ldquo;What is the population of Tokyo divided by 1000?\u0026rdquo;:\nIteration 1:\nThought: \u0026ldquo;I need to find the population of Tokyo\u0026rdquo; Action: search(\u0026quot;Tokyo population\u0026quot;) Observation: \u0026ldquo;Tokyo has a population of approximately 14 million\u0026rdquo; Iteration 2:\nThought: \u0026ldquo;Now I need to divide 14,000,000 by 1000\u0026rdquo; Action: calculator(\u0026quot;14000000 / 1000\u0026quot;) Observation: \u0026ldquo;14000\u0026rdquo; Iteration 3:\nThought: \u0026ldquo;I have the answer\u0026rdquo; Action: finish(\u0026quot;14,000\u0026quot;) Custom Tools Extend agents with domain-specific tools.\nCreating a Custom Tool package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) // Custom Weather Tool type WeatherTool struct{} func (t *WeatherTool) GetName() string { return \u0026#34;weather\u0026#34; } func (t *WeatherTool) GetDescription() string { return \u0026#34;Get the current weather for a location. Usage: weather(location)\u0026#34; } func (t *WeatherTool) CanHandle(action string) bool { return strings.HasPrefix(action, \u0026#34;weather(\u0026#34;) } func (t *WeatherTool) Execute(ctx context.Context, action string) (string, error) { // Parse location from action string location := parseLocation(action) // Fetch weather data (your implementation) weather, err := fetchWeather(location) if err != nil { return \u0026#34;\u0026#34;, err } return fmt.Sprintf(\u0026#34;Weather in %s: %s, %d¬∞C\u0026#34;, location, weather.Condition, weather.Temperature), nil } func parseLocation(action string) string { // Extract \u0026#34;Paris\u0026#34; from \u0026#34;weather(Paris)\u0026#34; start := strings.Index(action, \u0026#34;(\u0026#34;) + 1 end := strings.Index(action, \u0026#34;)\u0026#34;) return action[start:end] }\rUsing Custom Tools // Register custom tool registry := tools.NewInMemoryToolRegistry() registry.Register(\u0026amp;WeatherTool{}) registry.Register(\u0026amp;DatabaseTool{}) registry.Register(\u0026amp;EmailTool{}) // Create ReAct agent with custom tools react := modules.NewReAct(signature, registry, 10)\rFull Agents Example ‚Üí\nMemory Management Conversation history and context tracking.\nBuffer Memory Store recent conversation turns:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/memory\u0026#34; ) func main() { // Create buffer memory (keeps last 10 exchanges) mem := memory.NewBufferMemory(10) ctx := context.Background() // Add messages mem.Add(ctx, \u0026#34;user\u0026#34;, \u0026#34;Hello, how can you help me?\u0026#34;) mem.Add(ctx, \u0026#34;assistant\u0026#34;, \u0026#34;I can answer questions and help with tasks.\u0026#34;) mem.Add(ctx, \u0026#34;user\u0026#34;, \u0026#34;What\u0026#39;s the weather in Paris?\u0026#34;) mem.Add(ctx, \u0026#34;assistant\u0026#34;, \u0026#34;The weather in Paris is sunny, 22¬∞C.\u0026#34;) // Retrieve conversation history history, err := mem.Get(ctx) if err != nil { log.Fatal(err) } // Use history in prompts for _, msg := range history { fmt.Printf(\u0026#34;%s: %s\\n\u0026#34;, msg.Role, msg.Content) } }\rSummary Memory Compress long conversations into summaries:\n// Create summary memory summaryMem := memory.NewSummaryMemory( core.GetDefaultLLM(), 100, // summarize after 100 messages ) // Add messages (automatically summarizes when threshold is reached) summaryMem.Add(ctx, \u0026#34;user\u0026#34;, \u0026#34;Long conversation...\u0026#34;) summaryMem.Add(ctx, \u0026#34;assistant\u0026#34;, \u0026#34;Response...\u0026#34;) // Get summarized history history, err := summaryMem.Get(ctx)\rUsing Memory with Agents // Create agent with memory mem := memory.NewBufferMemory(20) // In your agent loop for { userInput := getUserInput() // Add user message to memory mem.Add(ctx, \u0026#34;user\u0026#34;, userInput) // Get conversation history history, _ := mem.Get(ctx) // Include history in agent context result, err := react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: userInput, \u0026#34;history\u0026#34;: history, }) // Add assistant response to memory mem.Add(ctx, \u0026#34;assistant\u0026#34;, result[\u0026#34;answer\u0026#34;].(string)) fmt.Println(result[\u0026#34;answer\u0026#34;]) }\rOrchestrator Task decomposition and multi-agent coordination.\nBasic Orchestrator package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // Create orchestrator orchestrator := agents.NewOrchestrator() // Define subtasks with different modules researchTask := agents.NewTask(\u0026#34;research\u0026#34;, researchModule) analyzeTask := agents.NewTask(\u0026#34;analyze\u0026#34;, analyzeModule) summarizeTask := agents.NewTask(\u0026#34;summarize\u0026#34;, summarizeModule) // Add tasks to orchestrator orchestrator.AddTask(researchTask) orchestrator.AddTask(analyzeTask) orchestrator.AddTask(summarizeTask) // Execute orchestration ctx := context.Background() result, err := orchestrator.Execute(ctx, map[string]interface{}{ \u0026#34;topic\u0026#34;: \u0026#34;Impact of AI on healthcare\u0026#34;, }) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;Final Result: %v\\n\u0026#34;, result) }\rTask Dependencies // Create tasks with dependencies task1 := agents.NewTask(\u0026#34;fetch_data\u0026#34;, fetchModule) task2 := agents.NewTask(\u0026#34;process_data\u0026#34;, processModule) task3 := agents.NewTask(\u0026#34;analyze\u0026#34;, analyzeModule) // Set dependencies (task2 depends on task1, task3 depends on task2) task2.DependsOn(task1) task3.DependsOn(task2) // Orchestrator automatically handles execution order orchestrator.AddTask(task1) orchestrator.AddTask(task2) orchestrator.AddTask(task3)\rAdvanced Agent Patterns Multi-Agent System Specialized agents working together:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents\u0026#34; ) type MultiAgentSystem struct { researcher *Agent analyst *Agent writer *Agent coordinator *agents.Orchestrator } func NewMultiAgentSystem() *MultiAgentSystem { return \u0026amp;MultiAgentSystem{ researcher: NewResearchAgent(), analyst: NewAnalystAgent(), writer: NewWriterAgent(), coordinator: agents.NewOrchestrator(), } } func (m *MultiAgentSystem) Execute(ctx context.Context, task string) (string, error) { // 1. Research phase researchTask := agents.NewTask(\u0026#34;research\u0026#34;, m.researcher.module) research, err := researchTask.Execute(ctx, map[string]interface{}{ \u0026#34;task\u0026#34;: task, }) // 2. Analysis phase analysisTask := agents.NewTask(\u0026#34;analyze\u0026#34;, m.analyst.module) analysis, err := analysisTask.Execute(ctx, map[string]interface{}{ \u0026#34;research\u0026#34;: research, }) // 3. Writing phase writingTask := agents.NewTask(\u0026#34;write\u0026#34;, m.writer.module) result, err := writingTask.Execute(ctx, map[string]interface{}{ \u0026#34;analysis\u0026#34;: analysis, }) return result[\u0026#34;output\u0026#34;].(string), nil }\rAgent with Reflection Self-improvement through reflection:\ntype ReflectiveAgent struct { react *modules.ReAct memory memory.Memory reflection *modules.ChainOfThought } func (a *ReflectiveAgent) ExecuteWithReflection(ctx context.Context, task string) (string, error) { // 1. Execute task result, err := a.react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: task, }) // 2. Reflect on performance reflection, err := a.reflection.Process(ctx, map[string]interface{}{ \u0026#34;action\u0026#34;: \u0026#34;reflect on previous attempt\u0026#34;, \u0026#34;result\u0026#34;: result, \u0026#34;task\u0026#34;: task, }) // 3. Store reflection for future improvement a.memory.Add(ctx, \u0026#34;reflection\u0026#34;, reflection[\u0026#34;rationale\u0026#34;].(string)) // 4. Retry if needed based on reflection if shouldRetry(reflection) { return a.react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: task, \u0026#34;reflection\u0026#34;: reflection, }) } return result[\u0026#34;answer\u0026#34;].(string), nil }\rACE Framework (Agentic Context Engineering) Self-improving agents that learn from execution trajectories.\nBased on the ACE paper (arXiv:2510.04618), ACE enables agents to:\nRecord execution trajectories (steps, tool calls, reasoning) Extract patterns from successes and failures Persist learnings across sessions Inject learnings into future prompts Quick Start with ACE Enable ACE on a ReAct agent:\nimport ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/ace\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/react\u0026#34; ) // Configure ACE aceConfig := ace.Config{ Enabled: true, LearningsPath: \u0026#34;./learnings/agent.md\u0026#34;, // Persistent storage AsyncReflection: true, // Process in background CurationFrequency: 10, // Curate every 10 trajectories MinConfidence: 0.7, // Threshold for new learnings MaxTokens: 80000, // Token budget for learnings } // Create agent with ACE agent := react.NewReActAgent( \u0026#34;my-agent\u0026#34;, \u0026#34;Research Assistant\u0026#34;, react.WithACE(aceConfig), // Enable ACE! react.WithReflection(true, 3), // Also enable reflection react.WithMaxIterations(10), )\rHow ACE Works ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Agent Execution ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ 1. StartTrajectory() - Begin recording ‚îÇ\r‚îÇ 2. RecordStep() - Capture each action/observation ‚îÇ\r‚îÇ 3. EndTrajectory() - Finalize with outcome ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Reflection ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ ‚Ä¢ UnifiedReflector combines multiple insight sources ‚îÇ\r‚îÇ ‚Ä¢ SimpleReflector extracts basic patterns (no LLM) ‚îÇ\r‚îÇ ‚Ä¢ Adapters bridge existing systems (SelfReflector, etc.) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Curation ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ ‚Ä¢ Add new learnings (strategies, mistakes) ‚îÇ\r‚îÇ ‚Ä¢ Update existing learnings (helpful/harmful counts) ‚îÇ\r‚îÇ ‚Ä¢ Prune ineffective learnings ‚îÇ\r‚îÇ ‚Ä¢ Merge similar learnings ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Storage (learnings.md) ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ ## STRATEGIES ‚îÇ\r‚îÇ [strategies-00001] helpful=5 harmful=0 :: Use calculator ‚îÇ\r‚îÇ [strategies-00002] helpful=3 harmful=1 :: Search once ‚îÇ\r‚îÇ ‚îÇ\r‚îÇ ## MISTAKES ‚îÇ\r‚îÇ [mistakes-00001] helpful=0 harmful=4 :: Avoid broken_db ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\rStandalone ACE Usage Use ACE components directly without a ReAct agent:\nimport ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/ace\u0026#34; ) func main() { // Configure config := ace.Config{ Enabled: true, LearningsPath: \u0026#34;./learnings.md\u0026#34;, AsyncReflection: false, CurationFrequency: 5, MinConfidence: 0.6, MaxTokens: 80000, } // Create reflector and manager reflector := ace.NewUnifiedReflector(nil, ace.NewSimpleReflector()) manager, _ := ace.NewManager(config, reflector) defer manager.Close() ctx := context.Background() // Record a trajectory recorder := manager.StartTrajectory(\u0026#34;agent-1\u0026#34;, \u0026#34;research\u0026#34;, \u0026#34;Find weather in NYC\u0026#34;) recorder.RecordStep( \u0026#34;search\u0026#34;, // action \u0026#34;web_search\u0026#34;, // tool \u0026#34;Searching for NYC weather\u0026#34;, // reasoning map[string]any{\u0026#34;query\u0026#34;: \u0026#34;NYC weather\u0026#34;}, // input map[string]any{\u0026#34;result\u0026#34;: \u0026#34;Sunny, 72F\u0026#34;}, // output nil, // error (nil = success) ) manager.EndTrajectory(ctx, recorder, ace.OutcomeSuccess) // Get learnings for context injection contextStr := manager.LearningsContext() fmt.Println(contextStr) // Check metrics metrics := manager.Metrics() fmt.Printf(\u0026#34;Trajectories: %d, Learnings: %d\\n\u0026#34;, metrics[\u0026#34;trajectories_processed\u0026#34;], metrics[\u0026#34;learnings_added\u0026#34;]) }\rCitation Tracking ACE tracks when the agent cites learnings in its reasoning:\n// Agent reasoning that cites a learning recorder.RecordStep( \u0026#34;search\u0026#34;, \u0026#34;web_search\u0026#34;, \u0026#34;Using [L001] efficient search strategy, I\u0026#39;ll search once\u0026#34;, // Cites L001! input, output, nil, ) // After successful execution, L001 gets a \u0026#34;helpful\u0026#34; vote // After failure, L001 gets a \u0026#34;harmful\u0026#34; vote // Learnings with low success rates get pruned\rLearnings File Format ACE stores learnings in a human-readable markdown format:\n## STRATEGIES [strategies-00001] helpful=5 harmful=0 :: Use calculator for arithmetic [strategies-00002] helpful=3 harmful=1 :: Search once, then respond ## MISTAKES [mistakes-00001] helpful=0 harmful=4 :: Avoid broken_database tool\rContext Injection Learnings are formatted for injection into agent prompts:\ncontextStr := manager.LearningsContext() // Returns: // ## Learned Strategies (cite by ID if using) // [L001] Use calculator for arithmetic (100% success) // [L002] Search once, then respond (75% success) // // ## Mistakes to Avoid (cite by ID if avoiding) // [M001] Avoid broken_database tool\rACE Examples Two complete examples are available:\n# Basic ACE usage (no LLM required) go run ./examples/ace_basic/... # ACE integrated with ReAct agent GEMINI_API_KEY=your-key go run ./examples/ace_react/... # Persist learnings across runs go run ./examples/ace_react/... --learnings-dir=./my_learnings\rACE Examples ‚Üí\nProduction Agent Example Complete production-ready agent with all features:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/tools\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/memory\u0026#34; ) type ProductionAgent struct { react *modules.ReAct memory memory.Memory registry core.ToolRegistry } func NewProductionAgent() *ProductionAgent { // Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Create Smart Tool Registry registry := tools.NewSmartToolRegistry(\u0026amp;tools.SmartToolRegistryConfig{ AutoDiscoveryEnabled: true, PerformanceTrackingEnabled: true, FallbackEnabled: true, }) // Register tools registry.Register(tools.NewCalculatorTool()) registry.Register(tools.NewSearchTool()) registry.Register(tools.NewDatabaseTool()) registry.Register(tools.NewAPITool()) // Create signature signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;question\u0026#34;)}, {Field: core.NewField(\u0026#34;history\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;)}, }, ) // Create ReAct module react := modules.NewReAct(signature, registry, 10) // Create memory memory := memory.NewBufferMemory(50) return \u0026amp;ProductionAgent{ react: react, memory: memory, registry: registry, } } func (a *ProductionAgent) Chat(ctx context.Context, userInput string) (string, error) { // Add user message to memory a.memory.Add(ctx, \u0026#34;user\u0026#34;, userInput) // Get conversation history history, err := a.memory.Get(ctx) if err != nil { return \u0026#34;\u0026#34;, err } // Execute with history result, err := a.react.Process(ctx, map[string]interface{}{ \u0026#34;question\u0026#34;: userInput, \u0026#34;history\u0026#34;: formatHistory(history), }) if err != nil { return \u0026#34;\u0026#34;, err } answer := result[\u0026#34;answer\u0026#34;].(string) // Add assistant response to memory a.memory.Add(ctx, \u0026#34;assistant\u0026#34;, answer) return answer, nil } func formatHistory(history []memory.Message) string { var formatted string for _, msg := range history { formatted += fmt.Sprintf(\u0026#34;%s: %s\\n\u0026#34;, msg.Role, msg.Content) } return formatted } func main() { agent := NewProductionAgent() ctx := context.Background() // Interactive loop for { fmt.Print(\u0026#34;You: \u0026#34;) var input string fmt.Scanln(\u0026amp;input) if input == \u0026#34;exit\u0026#34; { break } response, err := agent.Chat(ctx, input) if err != nil { log.Printf(\u0026#34;Error: %v\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;Agent: %s\\n\\n\u0026#34;, response) } }\rKey Agent Features Feature Description Example ReAct Pattern Reasoning + tool use Research agents, Q\u0026amp;A bots Custom Tools Domain-specific actions Database queries, API calls Memory Conversation history Multi-turn chat Orchestration Task decomposition Complex workflows Multi-Agent Specialized agents Research + analysis + writing Reflection Self-improvement Iterative refinement ACE Framework Self-improving agents Learn from trajectories Examples Complete Agent Examples Agents Package Examples - ReAct, orchestrator, memory ACE Basic Example - Standalone ACE usage (no LLM) ACE + ReAct Example - Self-improving ReAct agent Maestro - Production code review agent Smart Tool Registry - Advanced tool management Running the Examples # Basic agent examples cd examples/agents \u0026amp;\u0026amp; go run main.go # ACE examples go run ./examples/ace_basic/... GEMINI_API_KEY=your-key go run ./examples/ace_react/... # Production agent (Maestro) git clone https://github.com/XiaoConstantine/maestro cd maestro \u0026amp;\u0026amp; go run main.go\rNext Steps Tool Management ‚Üí - Build sophisticated tool systems Core Concepts ‚Üí - Understand modules and signatures Optimizers ‚Üí - Improve agent performance automatically Examples ‚Üí - More agent patterns ","date":"2025-10-13","id":7,"permalink":"/dspy-go/docs/guides/building-agents/","summary":"Create intelligent agents with reasoning, tool use, and conversation memory","tags":[],"title":"Building Agents"},{"content":"A2A Protocol (Agent-to-Agent Communication) The A2A Protocol enables multi-agent orchestration with hierarchical composition, allowing agents to delegate work to specialized sub-agents. Build sophisticated workflows where multiple agents collaborate to solve complex tasks.\nWhy A2A? Traditional single-agent approaches struggle with:\nComplex tasks requiring diverse expertise Long-running workflows that benefit from decomposition Specialized knowledge needed for different parts of a task A2A solves this by enabling:\nHierarchical Composition: Orchestrators manage specialized sub-agents Standardized Messages: Structured message format with metadata In-Process Communication: No HTTP overhead for local coordination Capability Discovery: Agents can advertise and discover capabilities Architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Orchestrator Agent ‚îÇ ‚îÇ (ResearchOrchestrator) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 1. Receives task: \u0026#34;Research AI in healthcare\u0026#34; ‚îÇ ‚îÇ 2. Delegates to sub-agents via CallSubAgent() ‚îÇ ‚îÇ 3. Aggregates results into final output ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚ñº ‚ñº ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ SearchAgent ‚îÇ ‚îÇAnalysisAgent‚îÇ ‚îÇSynthesisAgent‚îÇ ‚îÇ (search) ‚îÇ ‚îÇ (analysis) ‚îÇ ‚îÇ (synthesis) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Generates ‚îÇ ‚îÇ Extracts ‚îÇ ‚îÇ Creates ‚îÇ ‚îÇ search ‚îÇ ‚îÇ key findings‚îÇ ‚îÇ final ‚îÇ ‚îÇ queries ‚îÇ ‚îÇ and patterns‚îÇ ‚îÇ report ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\rQuick Start 1. Create Specialized Agents Each agent implements the agents.Agent interface:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) // SearchAgent performs web searches type SearchAgent struct { module core.Module } func NewSearchAgent() (*SearchAgent, error) { signature := core.NewSignature( []core.InputField{ {Field: core.Field{Name: \u0026#34;topic\u0026#34;, Description: \u0026#34;Research topic\u0026#34;}}, }, []core.OutputField{ {Field: core.Field{Name: \u0026#34;search_queries\u0026#34;, Description: \u0026#34;3-5 search queries\u0026#34;}}, {Field: core.Field{Name: \u0026#34;search_results\u0026#34;, Description: \u0026#34;Search results\u0026#34;}}, }, ).WithInstruction(`Generate targeted search queries and find relevant information.`) return \u0026amp;SearchAgent{ module: modules.NewPredict(signature), }, nil } func (s *SearchAgent) Execute(ctx context.Context, input map[string]interface{}) (map[string]interface{}, error) { return s.module.Process(ctx, input) } func (s *SearchAgent) GetCapabilities() []core.Tool { return nil } func (s *SearchAgent) GetMemory() agents.Memory { return nil }\r2. Wrap Agents with A2A Executors import a2a \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/communication\u0026#34; // Create specialized agents searchAgent, _ := NewSearchAgent() analysisAgent, _ := NewAnalysisAgent() synthesisAgent, _ := NewSynthesisAgent() // Wrap with A2A executors searchExec := a2a.NewExecutorWithConfig(searchAgent, a2a.ExecutorConfig{ Name: \u0026#34;SearchAgent\u0026#34;, }) analysisExec := a2a.NewExecutorWithConfig(analysisAgent, a2a.ExecutorConfig{ Name: \u0026#34;AnalysisAgent\u0026#34;, }) synthesisExec := a2a.NewExecutorWithConfig(synthesisAgent, a2a.ExecutorConfig{ Name: \u0026#34;SynthesisAgent\u0026#34;, })\r3. Create Orchestrator type ResearchOrchestrator struct { executor *a2a.A2AExecutor } func NewResearchOrchestrator() (*ResearchOrchestrator, *a2a.A2AExecutor) { agent := \u0026amp;ResearchOrchestrator{} executor := a2a.NewExecutorWithConfig(agent, a2a.ExecutorConfig{ Name: \u0026#34;ResearchOrchestrator\u0026#34;, }) agent.executor = executor return agent, executor } func (r *ResearchOrchestrator) Execute(ctx context.Context, input map[string]interface{}) (map[string]interface{}, error) { topic := input[\u0026#34;topic\u0026#34;].(string) // Step 1: Search for information searchResult, err := r.executor.CallSubAgent(ctx, \u0026#34;search\u0026#34;, a2a.NewUserMessage(topic)) if err != nil { return nil, fmt.Errorf(\u0026#34;search failed: %w\u0026#34;, err) } // Extract search results from artifact searchResults := extractField(searchResult, \u0026#34;search_results\u0026#34;) // Step 2: Analyze the results analysisInput := a2a.NewMessage(a2a.RoleUser, a2a.NewTextPartWithMetadata(topic, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;topic\u0026#34;}), a2a.NewTextPartWithMetadata(searchResults, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;search_results\u0026#34;}), ) analysisResult, err := r.executor.CallSubAgent(ctx, \u0026#34;analysis\u0026#34;, analysisInput) if err != nil { return nil, fmt.Errorf(\u0026#34;analysis failed: %w\u0026#34;, err) } // Step 3: Synthesize final report // ... continue with synthesis agent return output, nil } func (r *ResearchOrchestrator) GetCapabilities() []core.Tool { return nil } func (r *ResearchOrchestrator) GetMemory() agents.Memory { return nil }\r4. Register Sub-Agents and Execute func main() { // Create and configure _, orchestratorExec := NewResearchOrchestrator() orchestratorExec. WithSubAgent(\u0026#34;search\u0026#34;, searchExec). WithSubAgent(\u0026#34;analysis\u0026#34;, analysisExec). WithSubAgent(\u0026#34;synthesis\u0026#34;, synthesisExec) // Execute workflow msg := a2a.NewMessage(a2a.RoleUser, a2a.NewTextPartWithMetadata(\u0026#34;AI in healthcare\u0026#34;, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;topic\u0026#34;}), ) artifact, err := orchestratorExec.Execute(ctx, msg) if err != nil { log.Fatal(err) } // Process results from artifact for _, part := range artifact.Parts { if field, ok := part.Metadata[\u0026#34;field\u0026#34;].(string); ok { fmt.Printf(\u0026#34;%s: %s\\n\u0026#34;, field, part.Text) } } }\rMessage Protocol Creating Messages // Simple user message msg := a2a.NewUserMessage(\u0026#34;What is the weather?\u0026#34;) // Message with multiple parts and metadata msg := a2a.NewMessage(a2a.RoleUser, a2a.NewTextPartWithMetadata(\u0026#34;Paris\u0026#34;, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;}), a2a.NewTextPartWithMetadata(\u0026#34;today\u0026#34;, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;}), ) // Message roles a2a.RoleUser // User input a2a.RoleAssistant // Agent response a2a.RoleSystem // System instructions\rWorking with Artifacts Artifacts are the result of agent execution:\nartifact, err := executor.Execute(ctx, msg) // Iterate over parts for _, part := range artifact.Parts { // Access text content fmt.Println(part.Text) // Access metadata if field, ok := part.Metadata[\u0026#34;field\u0026#34;].(string); ok { fmt.Printf(\u0026#34;Field: %s = %s\\n\u0026#34;, field, part.Text) } } // Convert to map result := make(map[string]interface{}) for _, part := range artifact.Parts { if field, ok := part.Metadata[\u0026#34;field\u0026#34;].(string); ok { result[field] = part.Text } }\rAdvanced Patterns Parallel Sub-Agent Execution Execute multiple sub-agents concurrently:\nfunc (o *Orchestrator) Execute(ctx context.Context, input map[string]interface{}) (map[string]interface{}, error) { topic := input[\u0026#34;topic\u0026#34;].(string) // Create channels for results type result struct { name string data *a2a.Artifact err error } results := make(chan result, 3) // Launch parallel searches searchTopics := []string{\u0026#34;technical\u0026#34;, \u0026#34;business\u0026#34;, \u0026#34;regulatory\u0026#34;} for _, searchType := range searchTopics { go func(st string) { msg := a2a.NewMessage(a2a.RoleUser, a2a.NewTextPartWithMetadata(topic, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;topic\u0026#34;}), a2a.NewTextPartWithMetadata(st, map[string]interface{}{\u0026#34;field\u0026#34;: \u0026#34;search_type\u0026#34;}), ) artifact, err := o.executor.CallSubAgent(ctx, \u0026#34;search\u0026#34;, msg) results \u0026lt;- result{st, artifact, err} }(searchType) } // Collect results combined := make(map[string]*a2a.Artifact) for i := 0; i \u0026lt; len(searchTopics); i++ { r := \u0026lt;-results if r.err != nil { return nil, r.err } combined[r.name] = r.data } // Continue with aggregation... }\rConditional Routing Route to different agents based on task type:\nfunc (o *Orchestrator) Execute(ctx context.Context, input map[string]interface{}) (map[string]interface{}, error) { task := input[\u0026#34;task\u0026#34;].(string) taskType := classifyTask(task) var agentName string switch taskType { case \u0026#34;research\u0026#34;: agentName = \u0026#34;research_agent\u0026#34; case \u0026#34;analysis\u0026#34;: agentName = \u0026#34;analysis_agent\u0026#34; case \u0026#34;writing\u0026#34;: agentName = \u0026#34;writing_agent\u0026#34; default: agentName = \u0026#34;general_agent\u0026#34; } result, err := o.executor.CallSubAgent(ctx, agentName, a2a.NewUserMessage(task)) // ... }\rError Handling and Fallbacks func (o *Orchestrator) Execute(ctx context.Context, input map[string]interface{}) (map[string]interface{}, error) { // Try primary agent result, err := o.executor.CallSubAgent(ctx, \u0026#34;primary_search\u0026#34;, msg) if err != nil { // Fallback to secondary agent log.Printf(\u0026#34;Primary search failed: %v, trying fallback\u0026#34;, err) result, err = o.executor.CallSubAgent(ctx, \u0026#34;fallback_search\u0026#34;, msg) if err != nil { return nil, fmt.Errorf(\u0026#34;all search agents failed: %w\u0026#34;, err) } } return processResult(result), nil }\rKey Features Feature Description Hierarchical Composition Orchestrators manage multiple sub-agents Standardized Messages Consistent message format with metadata In-Process Communication Zero network overhead for local agents Capability Discovery Agents advertise their capabilities Field Metadata Tag data fields for structured passing Flexible Routing Route tasks to appropriate agents Complete Example See the full deep research agent example:\ncd examples/a2a_composition go run main.go --api-key YOUR_API_KEY\rThis example demonstrates:\nMulti-agent hierarchical composition Search -\u0026gt; Analysis -\u0026gt; Synthesis workflow Standardized message/artifact protocol Real LLM integration with Gemini A2A Composition Example\nNext Steps Building Agents - ReAct patterns and agent architecture Tool Management - Smart tool selection for agents ACE Framework - Self-improving agents ","date":"2025-01-06","id":8,"permalink":"/dspy-go/docs/guides/a2a-protocol/","summary":"Build sophisticated multi-agent systems with hierarchical composition and standardized messaging","tags":[],"title":"A2A Protocol"},{"content":"Multimodal Processing dspy-go has native multimodal support from day one. Process images, build vision Q\u0026amp;A systems, and create multimodal chat applications with seamless integration.\nWhy Multimodal? Modern LLM applications need to work with more than just text:\nüì∑ Image Analysis: Describe, analyze, and understand images üëÅÔ∏è Vision Q\u0026amp;A: Answer questions about visual content üí¨ Multimodal Chat: Conversations mixing text and images üé¨ Streaming: Real-time multimodal content processing Supported Providers Provider Image Support Streaming Models Google Gemini ‚úÖ Yes ‚úÖ Yes gemini-pro-vision, gemini-1.5-pro Anthropic Claude ‚úÖ Yes ‚úÖ Yes claude-3-opus, claude-3-sonnet OpenAI ‚úÖ Yes ‚úÖ Yes gpt-4-vision-preview, gpt-4o Image Analysis Analyze images with natural language questions.\nBasic Image Analysis package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // Configure Gemini (has multimodal support) llm, err := llms.NewGeminiLLM(\u0026#34;your-api-key\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // Define signature for image analysis signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;image\u0026#34;, core.WithDescription(\u0026#34;The image to analyze\u0026#34;))}, {Field: core.NewField(\u0026#34;question\u0026#34;, core.WithDescription(\u0026#34;Question about the image\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;answer\u0026#34;, core.WithDescription(\u0026#34;Answer based on the image\u0026#34;))}, }, ) // Create Predict module predictor := modules.NewPredict(signature) // Load image imageData, err := os.ReadFile(\u0026#34;path/to/image.jpg\u0026#34;) if err != nil { log.Fatal(err) } // Analyze image ctx := context.Background() result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), \u0026#34;question\u0026#34;: \u0026#34;What objects are in this image?\u0026#34;, }) fmt.Printf(\u0026#34;Answer: %s\\n\u0026#34;, result[\u0026#34;answer\u0026#34;]) }\rVision Question Answering Structured analysis of visual content.\nVision Q\u0026amp;A Example package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // Configure LLM with vision support llm, err := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) // Will use GEMINI_API_KEY if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // Define a comprehensive vision analysis signature signature := core.NewSignature( []core.InputField{ {Field: core.NewImageField(\u0026#34;image\u0026#34;, core.WithDescription(\u0026#34;The image to analyze in detail\u0026#34;))}, {Field: core.NewField(\u0026#34;focus\u0026#34;, core.WithDescription(\u0026#34;Specific aspect to focus on\u0026#34;))}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;description\u0026#34;, core.WithDescription(\u0026#34;Detailed description of the image\u0026#34;))}, {Field: core.NewField(\u0026#34;objects\u0026#34;, core.WithDescription(\u0026#34;List of objects identified\u0026#34;))}, {Field: core.NewField(\u0026#34;colors\u0026#34;, core.WithDescription(\u0026#34;Dominant colors in the image\u0026#34;))}, {Field: core.NewField(\u0026#34;mood\u0026#34;, core.WithDescription(\u0026#34;Overall mood or atmosphere\u0026#34;))}, }, ).WithInstruction(\u0026#34;Analyze the image thoroughly and provide detailed observations.\u0026#34;) // Create ChainOfThought for detailed analysis analyzer := modules.NewChainOfThought(signature) // Load image imageData, _ := os.ReadFile(\u0026#34;photo.jpg\u0026#34;) // Analyze ctx := context.Background() result, err := analyzer.Process(ctx, map[string]interface{}{ \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), \u0026#34;focus\u0026#34;: \u0026#34;architectural details and lighting\u0026#34;, }) // Print detailed analysis fmt.Printf(\u0026#34;Description: %s\\n\u0026#34;, result[\u0026#34;description\u0026#34;]) fmt.Printf(\u0026#34;Objects: %s\\n\u0026#34;, result[\u0026#34;objects\u0026#34;]) fmt.Printf(\u0026#34;Colors: %s\\n\u0026#34;, result[\u0026#34;colors\u0026#34;]) fmt.Printf(\u0026#34;Mood: %s\\n\u0026#34;, result[\u0026#34;mood\u0026#34;]) fmt.Printf(\u0026#34;Reasoning: %s\\n\u0026#34;, result[\u0026#34;rationale\u0026#34;]) // From ChainOfThought }\rMultimodal Chat Interactive conversations with images.\nChat with Images package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/agents/memory\u0026#34; ) type MultimodalChat struct { predictor modules.Module memory memory.Memory } func NewMultimodalChat() *MultimodalChat { signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;history\u0026#34;)}, {Field: core.NewField(\u0026#34;user_message\u0026#34;)}, {Field: core.NewField(\u0026#34;image\u0026#34;)}, // Optional }, []core.OutputField{ {Field: core.NewField(\u0026#34;response\u0026#34;)}, }, ) return \u0026amp;MultimodalChat{ predictor: modules.NewPredict(signature), memory: memory.NewBufferMemory(20), } } func (c *MultimodalChat) SendText(ctx context.Context, message string) (string, error) { history, _ := c.memory.Get(ctx) result, err := c.predictor.Process(ctx, map[string]interface{}{ \u0026#34;history\u0026#34;: formatHistory(history), \u0026#34;user_message\u0026#34;: message, }) if err != nil { return \u0026#34;\u0026#34;, err } response := result[\u0026#34;response\u0026#34;].(string) c.memory.Add(ctx, \u0026#34;user\u0026#34;, message) c.memory.Add(ctx, \u0026#34;assistant\u0026#34;, response) return response, nil } func (c *MultimodalChat) SendImage(ctx context.Context, message string, imageData []byte) (string, error) { history, _ := c.memory.Get(ctx) result, err := c.predictor.Process(ctx, map[string]interface{}{ \u0026#34;history\u0026#34;: formatHistory(history), \u0026#34;user_message\u0026#34;: message, \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), }) if err != nil { return \u0026#34;\u0026#34;, err } response := result[\u0026#34;response\u0026#34;].(string) c.memory.Add(ctx, \u0026#34;user\u0026#34;, fmt.Sprintf(\u0026#34;%s [image]\u0026#34;, message)) c.memory.Add(ctx, \u0026#34;assistant\u0026#34;, response) return response, nil } func main() { chat := NewMultimodalChat() ctx := context.Background() // Text conversation response, _ := chat.SendText(ctx, \u0026#34;Hello! I\u0026#39;m going to show you a photo.\u0026#34;) fmt.Println(\u0026#34;Assistant:\u0026#34;, response) // Send image imageData, _ := os.ReadFile(\u0026#34;vacation.jpg\u0026#34;) response, _ = chat.SendImage(ctx, \u0026#34;Where was this photo taken?\u0026#34;, imageData) fmt.Println(\u0026#34;Assistant:\u0026#34;, response) // Follow-up question (using conversation memory) response, _ = chat.SendText(ctx, \u0026#34;What\u0026#39;s the weather like there?\u0026#34;) fmt.Println(\u0026#34;Assistant:\u0026#34;, response) }\rStreaming Multimodal Content Real-time processing of multimodal content.\nStreaming Example package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Create signature signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;image\u0026#34;)}, {Field: core.NewField(\u0026#34;prompt\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;description\u0026#34;)}, }, ) // Create module predictor := modules.NewPredict(signature) // Set streaming handler predictor.SetStreamingHandler(func(chunk string) { fmt.Print(chunk) // Print each chunk as it arrives }) // Load image imageData, _ := os.ReadFile(\u0026#34;scene.jpg\u0026#34;) // Process with streaming ctx := context.Background() result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), \u0026#34;prompt\u0026#34;: \u0026#34;Describe this scene in vivid detail\u0026#34;, }) fmt.Printf(\u0026#34;\\n\\nFinal: %s\\n\u0026#34;, result[\u0026#34;description\u0026#34;]) }\rMultiple Images Compare and analyze multiple images simultaneously.\nMulti-Image Analysis package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/modules\u0026#34; ) func main() { // Configure LLM llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Define signature for comparing images signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;image1\u0026#34;)}, {Field: core.NewField(\u0026#34;image2\u0026#34;)}, {Field: core.NewField(\u0026#34;question\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;comparison\u0026#34;)}, {Field: core.NewField(\u0026#34;differences\u0026#34;)}, {Field: core.NewField(\u0026#34;similarities\u0026#34;)}, }, ) // Create module comparator := modules.NewChainOfThought(signature) // Load images image1, _ := os.ReadFile(\u0026#34;before.jpg\u0026#34;) image2, _ := os.ReadFile(\u0026#34;after.jpg\u0026#34;) // Compare ctx := context.Background() result, err := comparator.Process(ctx, map[string]interface{}{ \u0026#34;image1\u0026#34;: core.NewImageContent(image1, \u0026#34;image/jpeg\u0026#34;), \u0026#34;image2\u0026#34;: core.NewImageContent(image2, \u0026#34;image/jpeg\u0026#34;), \u0026#34;question\u0026#34;: \u0026#34;What changed between these two images?\u0026#34;, }) fmt.Printf(\u0026#34;Comparison: %s\\n\u0026#34;, result[\u0026#34;comparison\u0026#34;]) fmt.Printf(\u0026#34;Differences: %s\\n\u0026#34;, result[\u0026#34;differences\u0026#34;]) fmt.Printf(\u0026#34;Similarities: %s\\n\u0026#34;, result[\u0026#34;similarities\u0026#34;]) }\rContent Block System Flexible handling of mixed content types.\nContent Blocks package main import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; ) func main() { // Create mixed content content := []core.ContentBlock{ core.NewTextContent(\u0026#34;Please analyze this image:\u0026#34;), core.NewImageContent(imageData1, \u0026#34;image/jpeg\u0026#34;), core.NewTextContent(\u0026#34;And compare it to this one:\u0026#34;), core.NewImageContent(imageData2, \u0026#34;image/jpeg\u0026#34;), core.NewTextContent(\u0026#34;What are the key differences?\u0026#34;), } // Use in module result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;content\u0026#34;: content, }) }\rUse Cases Document Analysis // Analyze scanned documents signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;document_image\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;text_content\u0026#34;)}, {Field: core.NewField(\u0026#34;document_type\u0026#34;)}, {Field: core.NewField(\u0026#34;key_information\u0026#34;)}, }, ) extractor := modules.NewChainOfThought(signature) result, _ := extractor.Process(ctx, map[string]interface{}{ \u0026#34;document_image\u0026#34;: core.NewImageContent(scanData, \u0026#34;image/jpeg\u0026#34;), })\rChart Analysis // Extract data from charts and graphs signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;chart_image\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;chart_type\u0026#34;)}, {Field: core.NewField(\u0026#34;data_points\u0026#34;)}, {Field: core.NewField(\u0026#34;trends\u0026#34;)}, {Field: core.NewField(\u0026#34;insights\u0026#34;)}, }, ) analyzer := modules.NewPredict(signature)\rVisual Search // Find similar products signature := core.NewSignature( []core.InputField{ {Field: core.NewField(\u0026#34;query_image\u0026#34;)}, {Field: core.NewField(\u0026#34;description\u0026#34;)}, }, []core.OutputField{ {Field: core.NewField(\u0026#34;product_name\u0026#34;)}, {Field: core.NewField(\u0026#34;category\u0026#34;)}, {Field: core.NewField(\u0026#34;attributes\u0026#34;)}, }, ) searcher := modules.NewPredict(signature)\rBest Practices Image Preparation ‚úÖ DO:\nUse appropriate image formats (JPEG, PNG) Resize large images to reduce costs Ensure good image quality Provide clear, specific questions ‚ùå DON\u0026rsquo;T:\nSend extremely high-resolution images unnecessarily Use corrupted or unclear images Ask vague questions Expect pixel-perfect OCR (use specialized tools) Performance Optimization // Resize images before sending func resizeImage(data []byte, maxWidth, maxHeight int) []byte { // Your resize implementation return resizedData } // Use appropriate compression imageData := resizeImage(originalData, 1024, 1024)\rCost Management Cache results for repeated queries Use lower-resolution images when possible Batch similar queries together Monitor API usage and costs Examples Complete Multimodal Examples Multimodal Processing - All multimodal capabilities Basic image analysis Vision Q\u0026amp;A Multimodal chat Streaming Multiple images Running the Examples # Set API key export GEMINI_API_KEY=\u0026#34;your-api-key\u0026#34; # Run multimodal example cd examples/multimodal \u0026amp;\u0026amp; go run main.go\rNext Steps Core Concepts ‚Üí - Understand signatures and modules Agents ‚Üí - Build agents with vision capabilities Examples ‚Üí - More multimodal examples ","date":"2025-10-13","id":9,"permalink":"/dspy-go/docs/guides/multimodal-processing/","summary":"Process images and build multimodal applications with native vision support","tags":[],"title":"Multimodal Processing"},{"content":"","date":"2023-09-07","id":10,"permalink":"/dspy-go/docs/guides/","summary":"","tags":[],"title":"Guides"},{"content":"Complete technical reference for all dspy-go packages, modules, and configuration options.\nQuick Links Core Packages core - Signatures, fields, and base abstractions modules - Predict, ChainOfThought, ReAct, and more llms - LLM provider integrations optimizers - GEPA, MIPRO, SIMBA, Bootstrap, COPRO Advanced Packages tools - Smart tool registry and management agents - Agent orchestration and memory agents/memory - Conversation memory systems Package Documentation GoDoc Reference For complete API documentation with all types, interfaces, and functions, visit:\npkg.go.dev/github.com/XiaoConstantine/dspy-go\nThe GoDoc provides:\n‚úÖ Full type definitions ‚úÖ Function signatures ‚úÖ Method documentation ‚úÖ Code examples ‚úÖ Source code navigation Reference Guides By Topic Guide Description Configuration Reference ‚Üí Environment variables, LLM setup, provider options CLI Reference ‚Üí Command-line tool usage and flags LLM Providers ‚Üí Supported providers and model configurations Common Patterns Initialization Patterns // Zero-config (recommended) llm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Explicit configuration llm, _ := llms.NewGeminiLLM(\u0026#34;api-key\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Per-module override module := modules.NewPredict(signature) module.SetLLM(customLLM)\rError Handling import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; // Check for specific error types if err != nil { switch { case errors.Is(err, core.ErrNoLLMConfigured): // Handle missing LLM configuration case errors.Is(err, core.ErrInvalidSignature): // Handle invalid signature default: // Handle other errors } }\rContext Management import \u0026#34;context\u0026#34; // Always pass context ctx := context.Background() // With timeout ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() // With cancellation ctx, cancel := context.WithCancel(context.Background()) defer cancel()\rType System Core Types Signature type Signature interface { Inputs() []InputField Outputs() []OutputField Instruction() string }\rModule type Module interface { Process(ctx context.Context, inputs map[string]interface{}) (map[string]interface{}, error) }\rLLM type LLM interface { Generate(ctx context.Context, input string) (string, error) GenerateWithOptions(ctx context.Context, input string, opts GenerateOptions) (string, error) }\rConstants Model Identifiers // OpenAI Models core.ModelOpenAIGPT4 = \u0026#34;gpt-4\u0026#34; core.ModelOpenAIGPT4Turbo = \u0026#34;gpt-4-turbo-preview\u0026#34; core.ModelOpenAIGPT35Turbo = \u0026#34;gpt-3.5-turbo\u0026#34; // Anthropic Models core.ModelAnthropicOpus = \u0026#34;claude-3-opus-20240229\u0026#34; core.ModelAnthropicSonnet = \u0026#34;claude-3-sonnet-20240229\u0026#34; core.ModelAnthropicHaiku = \u0026#34;claude-3-haiku-20240307\u0026#34;\rError Constants var ( ErrNoLLMConfigured = errors.New(\u0026#34;no LLM configured\u0026#34;) ErrInvalidSignature = errors.New(\u0026#34;invalid signature\u0026#34;) ErrEmptyInput = errors.New(\u0026#34;empty input\u0026#34;) ErrMaxRetriesExceeded = errors.New(\u0026#34;max retries exceeded\u0026#34;) )\rEnvironment Variables LLM Provider Keys # Google Gemini GEMINI_API_KEY=\u0026#34;your-api-key\u0026#34; # OpenAI OPENAI_API_KEY=\u0026#34;your-api-key\u0026#34; OPENAI_BASE_URL=\u0026#34;https://api.openai.com/v1\u0026#34; # optional # Anthropic Claude ANTHROPIC_API_KEY=\u0026#34;your-api-key\u0026#34; # Ollama (local) OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34;\rOptimization Settings # Enable debug logging DSPY_DEBUG=true # Set default timeout (seconds) DSPY_TIMEOUT=30 # Enable caching DSPY_CACHE_ENABLED=true\rBest Practices Memory Management // Use context with timeout for long-running operations ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute) defer cancel() // Clean up resources defer module.Close()\rConcurrent Usage // Modules are NOT goroutine-safe by default // Create separate instances for concurrent use module1 := modules.NewPredict(signature) module2 := modules.NewPredict(signature) go func() { module1.Process(ctx, input1) }() go func() { module2.Process(ctx, input2) }()\rRate Limiting // Use built-in retry logic llm, _ := llms.NewGeminiLLM(apiKey, model) llm.SetMaxRetries(3) llm.SetRetryDelay(time.Second) // Or implement custom rate limiting limiter := rate.NewLimiter(rate.Every(time.Second), 10) limiter.Wait(ctx)\rNext Steps Browse Full API on pkg.go.dev ‚Üí Configuration Reference ‚Üí CLI Reference ‚Üí LLM Providers ‚Üí ","date":"2025-10-13","id":11,"permalink":"/dspy-go/docs/reference/","summary":"Comprehensive technical reference for all dspy-go components","tags":[],"title":"API Reference"},{"content":"Complete guide to configuring dspy-go for development and production environments.\nEnvironment Variables LLM Provider Configuration Google Gemini # Required GEMINI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # Optional - specify model (defaults to gemini-pro) GEMINI_MODEL=\u0026#34;gemini-1.5-pro\u0026#34;\rGet your API key: Google AI Studio\nOpenAI # Required OPENAI_API_KEY=\u0026#34;your-api-key-here\u0026#34; # Optional - custom base URL (for proxies or Azure) OPENAI_BASE_URL=\u0026#34;https://api.openai.com/v1\u0026#34; # Optional - specify organization OPENAI_ORG_ID=\u0026#34;your-org-id\u0026#34;\rGet your API key: OpenAI Platform\nAnthropic Claude # Required ANTHROPIC_API_KEY=\u0026#34;your-api-key-here\u0026#34; # Optional - API version (defaults to latest) ANTHROPIC_API_VERSION=\u0026#34;2023-06-01\u0026#34;\rGet your API key: Anthropic Console\nOllama (Local) # Required - Ollama server URL OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34; # Optional - specify model OLLAMA_MODEL=\u0026#34;llama2\u0026#34;\rInstall Ollama: ollama.com\nApplication Settings General Configuration # Enable debug logging DSPY_DEBUG=true # Set log level (debug, info, warn, error) DSPY_LOG_LEVEL=\u0026#34;info\u0026#34; # Default timeout for LLM calls (seconds) DSPY_TIMEOUT=30 # Maximum retries for failed requests DSPY_MAX_RETRIES=3 # Retry delay (milliseconds) DSPY_RETRY_DELAY=1000\rCaching # Enable response caching DSPY_CACHE_ENABLED=true # Cache directory DSPY_CACHE_DIR=\u0026#34;~/.cache/dspy-go\u0026#34; # Cache TTL (time-to-live in seconds) DSPY_CACHE_TTL=3600 # Maximum cache size (MB) DSPY_CACHE_MAX_SIZE=1000\rPerformance # Enable parallel execution DSPY_PARALLEL_ENABLED=true # Maximum parallel workers DSPY_MAX_WORKERS=4 # Request rate limit (requests per second) DSPY_RATE_LIMIT=10 # Enable request batching DSPY_BATCH_ENABLED=true # Batch size DSPY_BATCH_SIZE=5\rProgrammatic Configuration Zero-Config Setup Automatically configures based on environment variables:\nimport \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; func main() { // Detects and configures LLM from environment llm, err := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // Ready to use modules predictor := modules.NewPredict(signature) }\rPriority order:\nGEMINI_API_KEY ‚Üí Gemini OPENAI_API_KEY ‚Üí OpenAI ANTHROPIC_API_KEY ‚Üí Anthropic OLLAMA_BASE_URL ‚Üí Ollama Explicit LLM Configuration Gemini import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Basic setup llm, err := llms.NewGeminiLLM(\u0026#34;api-key\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // With options llm, err := llms.NewGeminiLLM(\u0026#34;api-key\u0026#34;, core.ModelGoogleGeminiPro, llms.WithTemperature(0.7), llms.WithMaxTokens(2048), llms.WithTopP(0.9), )\rOpenAI // Basic setup llm, err := llms.NewOpenAI(core.ModelOpenAIGPT4, \u0026#34;api-key\u0026#34;) core.SetDefaultLLM(llm) // With custom base URL (e.g., Azure) llm, err := llms.NewOpenAI( \u0026#34;gpt-4\u0026#34;, \u0026#34;api-key\u0026#34;, llms.WithBaseURL(\u0026#34;https://your-azure-endpoint.openai.azure.com\u0026#34;), llms.WithAPIVersion(\u0026#34;2024-02-15-preview\u0026#34;), ) // With options llm, err := llms.NewOpenAI(core.ModelOpenAIGPT4Turbo, \u0026#34;api-key\u0026#34;, llms.WithTemperature(0.8), llms.WithMaxTokens(4096), llms.WithPresencePenalty(0.1), llms.WithFrequencyPenalty(0.1), )\rAnthropic Claude // Basic setup llm, err := llms.NewAnthropicLLM(\u0026#34;api-key\u0026#34;, core.ModelAnthropicSonnet) core.SetDefaultLLM(llm) // With options llm, err := llms.NewAnthropicLLM(\u0026#34;api-key\u0026#34;, core.ModelAnthropicOpus, llms.WithTemperature(0.7), llms.WithMaxTokens(4096), llms.WithTopP(0.9), llms.WithTopK(40), )\rOllama (Local) // Basic setup llm, err := llms.NewOllamaLLM(\u0026#34;llama2\u0026#34;) core.SetDefaultLLM(llm) // With custom server llm, err := llms.NewOllamaLLM(\u0026#34;llama2\u0026#34;, llms.WithBaseURL(\u0026#34;http://192.168.1.100:11434\u0026#34;), ) // With options llm, err := llms.NewOllamaLLM(\u0026#34;mistral\u0026#34;, llms.WithTemperature(0.8), llms.WithNumCtx(4096), // Context window llms.WithNumPredict(2048), // Max tokens to predict )\rPer-Module Configuration Override LLM for specific modules:\n// Create module with default LLM predictor := modules.NewPredict(signature) // Override with specific LLM customLLM, _ := llms.NewAnthropicLLM(\u0026#34;key\u0026#34;, core.ModelAnthropicOpus) predictor.SetLLM(customLLM) // Now this module uses Claude Opus result, _ := predictor.Process(ctx, inputs)\rGeneration Options Common Options Available for all LLM providers:\ntype GenerateOptions struct { Temperature float64 // Randomness (0.0 - 1.0) MaxTokens int // Maximum tokens to generate TopP float64 // Nucleus sampling (0.0 - 1.0) StopSequences []string // Stop generation at these strings PresencePenalty float64 // Penalize new topics (-2.0 - 2.0) FrequencyPenalty float64 // Penalize repetition (-2.0 - 2.0) Stream bool // Enable streaming }\rTemperature Guidelines Temperature Use Case Example 0.0 - 0.3 Factual, deterministic Classification, extraction 0.4 - 0.6 Balanced Question answering, analysis 0.7 - 0.9 Creative Writing, brainstorming 0.9 - 1.0 Highly creative Fiction, poetry Example Usage opts := core.GenerateOptions{ Temperature: 0.7, MaxTokens: 2048, TopP: 0.9, StopSequences: []string{\u0026#34;\\n\\n\u0026#34;, \u0026#34;END\u0026#34;}, } result, err := llm.GenerateWithOptions(ctx, prompt, opts)\rAdvanced Configuration Retry Configuration import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; llm.SetMaxRetries(5) llm.SetRetryDelay(2 * time.Second) llm.SetRetryBackoff(true) // Exponential backoff\rTimeout Configuration // Global timeout core.SetDefaultTimeout(30 * time.Second) // Per-request timeout ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() result, err := predictor.Process(ctx, inputs)\rRate Limiting import \u0026#34;golang.org/x/time/rate\u0026#34; // Create rate limiter (10 requests per second) limiter := rate.NewLimiter(rate.Every(time.Second), 10) // Wait before making request if err := limiter.Wait(ctx); err != nil { log.Fatal(err) } result, err := predictor.Process(ctx, inputs)\rConfiguration Files .env File Support # .env GEMINI_API_KEY=your-api-key DSPY_DEBUG=true DSPY_CACHE_ENABLED=true\rLoad with:\nimport \u0026#34;github.com/joho/godotenv\u0026#34; func init() { if err := godotenv.Load(); err != nil { log.Println(\u0026#34;No .env file found\u0026#34;) } }\rConfig File (YAML) # config.yaml llm: provider: gemini model: gemini-1.5-pro api_key: ${GEMINI_API_KEY} temperature: 0.7 max_tokens: 2048 cache: enabled: true directory: ~/.cache/dspy-go ttl: 3600 performance: max_workers: 4 rate_limit: 10 batch_size: 5\rProduction Best Practices Security // ‚úÖ DO: Use environment variables apiKey := os.Getenv(\u0026#34;GEMINI_API_KEY\u0026#34;) // ‚ùå DON\u0026#39;T: Hardcode API keys apiKey := \u0026#34;hardcoded-key\u0026#34; // Never do this! // ‚úÖ DO: Validate configuration if apiKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;GEMINI_API_KEY not set\u0026#34;) }\rError Handling // Configure with error handling llm, err := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(\u0026#34;Failed to create LLM:\u0026#34;, err) } core.SetDefaultLLM(llm)\rMonitoring // Add logging middleware type LoggingLLM struct { wrapped core.LLM } func (l *LoggingLLM) Generate(ctx context.Context, prompt string) (string, error) { start := time.Now() result, err := l.wrapped.Generate(ctx, prompt) duration := time.Since(start) log.Printf(\u0026#34;LLM call completed in %v (tokens: %d, error: %v)\u0026#34;, duration, len(result), err) return result, err }\rTroubleshooting Common Issues \u0026ldquo;No LLM configured\u0026rdquo; Error Cause: No API key found in environment\nSolution:\nexport GEMINI_API_KEY=\u0026#34;your-key\u0026#34; # or export OPENAI_API_KEY=\u0026#34;your-key\u0026#34;\rRate Limit Errors Cause: Too many requests to API\nSolution:\nllm.SetMaxRetries(5) llm.SetRetryDelay(2 * time.Second) llm.SetRetryBackoff(true)\rTimeout Errors Cause: Requests taking too long\nSolution:\ncore.SetDefaultTimeout(60 * time.Second) // or ctx, cancel := context.WithTimeout(ctx, 60*time.Second)\rNext Steps CLI Reference ‚Üí - Command-line tool configuration LLM Providers ‚Üí - Provider-specific details API Reference ‚Üí - Full API documentation ","date":"2025-10-13","id":12,"permalink":"/dspy-go/docs/reference/configuration-reference/","summary":"Environment variables, LLM setup, and all configuration options","tags":[],"title":"Configuration Reference"},{"content":"Complete reference for the dspy-cli command-line tool. Test optimizers, run experiments, and explore dspy-go without writing code.\nInstallation Build from Source cd cmd/dspy-cli go build -o dspy-cli\rInstall Globally cd cmd/dspy-cli go install\rAfter installing, dspy-cli will be available in your $GOPATH/bin directory.\nConfiguration API Keys Set your LLM provider API key:\n# Google Gemini (recommended for multimodal) export GEMINI_API_KEY=\u0026#34;your-api-key\u0026#34; # OpenAI export OPENAI_API_KEY=\u0026#34;your-api-key\u0026#34; # Anthropic Claude export ANTHROPIC_API_KEY=\u0026#34;your-api-key\u0026#34; # Ollama (local) export OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34;\rCommands list - List Available Optimizers Display all available optimization algorithms with descriptions.\nUsage:\ndspy-cli list [flags]\rFlags:\nFlag Short Description Default --detailed -d Show detailed information false --json Output as JSON false Examples:\n# List all optimizers dspy-cli list # Show detailed information dspy-cli list --detailed # Output as JSON dspy-cli list --json\rOutput:\nAvailable Optimizers:\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\r1. bootstrap\rAutomated few-shot learning with example selection\rBest for: Quick optimization with limited data\r2. gepa\rMulti-objective evolutionary optimization with Pareto selection\rBest for: Highest quality results, complex tasks\r3. mipro\rSystematic TPE-based prompt and demonstration optimization\rBest for: Balanced performance and quality\r4. simba\rIntrospective mini-batch learning\rBest for: Fast iteration with moderate quality\r5. copro\rCollaborative multi-module prompt optimization\rBest for: Complex programs with multiple modules\rtry - Test an Optimizer Run an optimizer with a built-in dataset.\nUsage:\ndspy-cli try \u0026lt;optimizer\u0026gt; [flags]\rArguments:\nArgument Description Required \u0026lt;optimizer\u0026gt; Optimizer name (bootstrap, gepa, mipro, simba, copro) Yes Flags:\nFlag Short Description Default --dataset -d Dataset to use (gsm8k, hotpotqa) gsm8k --max-examples -n Number of examples to use 10 --verbose -v Verbose output false --timeout -t Timeout in seconds 300 --output -o Output file for results stdout --json Output as JSON false Examples:\n# Try Bootstrap with default settings dspy-cli try bootstrap # Try GEPA with GSM8K dataset dspy-cli try gepa --dataset gsm8k --max-examples 20 # Try MIPRO with verbose output dspy-cli try mipro --verbose # Try SIMBA and save results dspy-cli try simba --output results.json --json # Try with custom timeout dspy-cli try copro --timeout 600\rSample Output:\nüöÄ Starting Bootstrap Optimizer\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\rDataset: GSM8K (Math Problems)\rExamples: 10\rLLM: gemini-pro\r‚è≥ Optimizing prompts...\r[1/10] Processing example: \u0026#34;Janet\u0026#39;s ducks lay 16 eggs...\u0026#34;\r‚úì Generated demonstration\r‚úì Score: 1.0\r[2/10] Processing example: \u0026#34;A robe takes 2 bolts...\u0026#34;\r‚úì Generated demonstration\r‚úì Score: 1.0\r...\r‚ú® Optimization Complete!\rResults:\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\rAccuracy: 90.0%\rExamples: 10\rDuration: 45.2s\rAvg Time: 4.5s per example\rBest Examples Selected: 5\rPrompt Optimized: Yes\rrecommend - Get Optimizer Recommendations Get optimizer recommendations based on your requirements.\nUsage:\ndspy-cli recommend \u0026lt;profile\u0026gt; [flags]\rArguments:\nArgument Description Values \u0026lt;profile\u0026gt; Use case profile speed, balanced, quality, custom Flags (for custom profile):\nFlag Description Range --time-budget Time budget in seconds 1-3600 --quality-target Target quality score 0.0-1.0 --complexity Task complexity low, medium, high --data-size Dataset size small, medium, large Examples:\n# Quick recommendation for speed dspy-cli recommend speed # Balanced recommendation dspy-cli recommend balanced # High-quality results dspy-cli recommend quality # Custom recommendation dspy-cli recommend custom \\ --time-budget 300 \\ --quality-target 0.9 \\ --complexity high \\ --data-size medium\rSample Output:\nüéØ Optimizer Recommendation\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\rProfile: Balanced\rTime Budget: ~5 minutes\rQuality Target: High\rRecommended: MIPRO\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\r‚úÖ Systematic prompt optimization\r‚úÖ Good balance of speed and quality\r‚úÖ Works well with moderate datasets\r‚úÖ TPE-based search is efficient\rCommand to try:\rdspy-cli try mipro --dataset gsm8k --max-examples 20\rAlternative: Bootstrap (faster, slightly lower quality)\rAlternative: GEPA (slower, higher quality)\rcompare - Compare Optimizers Compare multiple optimizers on the same dataset.\nUsage:\ndspy-cli compare [optimizers...] [flags]\rFlags:\nFlag Short Description Default --dataset -d Dataset to use gsm8k --max-examples -n Examples per optimizer 10 --parallel -p Run in parallel false --output -o Output file stdout --format Output format (table, json, csv) table Examples:\n# Compare all optimizers dspy-cli compare bootstrap mipro gepa # Compare with specific dataset dspy-cli compare bootstrap mipro --dataset hotpotqa # Run comparisons in parallel dspy-cli compare bootstrap mipro simba --parallel # Output as CSV dspy-cli compare bootstrap gepa --format csv --output comparison.csv\rSample Output:\n‚öñÔ∏è Optimizer Comparison\r‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\rDataset: GSM8K\rExamples: 10\r‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\r‚îÉ Optimizer ‚îÉ Accuracy ‚îÉ Time (s) ‚îÉ Quality ‚îÉ\r‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\r‚îÉ Bootstrap ‚îÉ 85.0% ‚îÉ 32.1 ‚îÉ ‚≠ê‚≠ê‚≠ê ‚îÉ\r‚îÉ MIPRO ‚îÉ 90.0% ‚îÉ 58.4 ‚îÉ ‚≠ê‚≠ê‚≠ê‚≠ê ‚îÉ\r‚îÉ GEPA ‚îÉ 95.0% ‚îÉ 124.7 ‚îÉ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚îÉ\r‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\rüèÜ Winner: GEPA (95.0% accuracy)\r‚ö° Fastest: Bootstrap (32.1s)\rüíé Best Value: MIPRO (good balance)\rbenchmark - Run Comprehensive Benchmarks Run comprehensive benchmarks across datasets and optimizers.\nUsage:\ndspy-cli benchmark [flags]\rFlags:\nFlag Short Description Default --optimizers Comma-separated optimizer list all --datasets Comma-separated dataset list all --samples -n Samples per benchmark 20 --iterations -i Benchmark iterations 3 --parallel -p Parallel execution false --output -o Output directory ./benchmarks Examples:\n# Run full benchmark suite dspy-cli benchmark # Benchmark specific optimizers dspy-cli benchmark --optimizers bootstrap,mipro # Benchmark with more samples dspy-cli benchmark --samples 50 --iterations 5 # Parallel benchmarking dspy-cli benchmark --parallel --output ./results\rconfig - Manage Configuration View and manage CLI configuration.\nUsage:\ndspy-cli config \u0026lt;subcommand\u0026gt; [flags]\rSubcommands:\nSubcommand Description show Show current configuration set Set configuration value get Get configuration value reset Reset to defaults Examples:\n# Show configuration dspy-cli config show # Set default LLM dspy-cli config set default_llm gemini-1.5-pro # Get default dataset dspy-cli config get default_dataset # Reset configuration dspy-cli config reset\rGlobal Flags These flags work with all commands:\nFlag Short Description Default --help -h Show help --version Show version --quiet -q Quiet mode (minimal output) false --debug Enable debug logging false --no-color Disable colored output false Exit Codes Code Meaning 0 Success 1 General error 2 Invalid arguments 3 Configuration error 4 LLM error 5 Timeout Environment Variables The CLI respects these environment variables:\n# LLM Configuration GEMINI_API_KEY=\u0026#34;...\u0026#34; OPENAI_API_KEY=\u0026#34;...\u0026#34; ANTHROPIC_API_KEY=\u0026#34;...\u0026#34; OLLAMA_BASE_URL=\u0026#34;...\u0026#34; # CLI Defaults DSPY_CLI_DEFAULT_DATASET=\u0026#34;gsm8k\u0026#34; DSPY_CLI_DEFAULT_OPTIMIZER=\u0026#34;mipro\u0026#34; DSPY_CLI_MAX_EXAMPLES=\u0026#34;10\u0026#34; # Output Options DSPY_CLI_NO_COLOR=\u0026#34;false\u0026#34; DSPY_CLI_QUIET=\u0026#34;false\u0026#34; DSPY_CLI_OUTPUT_FORMAT=\u0026#34;table\u0026#34; # Performance DSPY_CLI_TIMEOUT=\u0026#34;300\u0026#34; DSPY_CLI_PARALLEL=\u0026#34;false\u0026#34;\rConfiguration File Create ~/.dspy-cli.yaml for persistent settings:\n# Default LLM provider default_llm: gemini-1.5-pro # Default dataset default_dataset: gsm8k # Default optimizer default_optimizer: mipro # Example limits max_examples: 20 # Output preferences output_format: table colored_output: true verbose: false # Performance timeout: 300 parallel: false\rAdvanced Usage Scripting with JSON Output # Get optimizer recommendations as JSON recommendations=$(dspy-cli recommend balanced --json) echo \u0026#34;$recommendations\u0026#34; | jq \u0026#39;.recommended_optimizer\u0026#39; # Run optimizer and parse results results=$(dspy-cli try mipro --json --output -) accuracy=$(echo \u0026#34;$results\u0026#34; | jq \u0026#39;.accuracy\u0026#39;) echo \u0026#34;Accuracy: $accuracy\u0026#34;\rBatch Processing #!/bin/bash # test_all_optimizers.sh optimizers=(\u0026#34;bootstrap\u0026#34; \u0026#34;mipro\u0026#34; \u0026#34;gepa\u0026#34; \u0026#34;simba\u0026#34;) dataset=\u0026#34;gsm8k\u0026#34; for opt in \u0026#34;${optimizers[@]}\u0026#34;; do echo \u0026#34;Testing $opt...\u0026#34; dspy-cli try \u0026#34;$opt\u0026#34; \\ --dataset \u0026#34;$dataset\u0026#34; \\ --max-examples 20 \\ --output \u0026#34;${opt}_results.json\u0026#34; \\ --json done\rCI/CD Integration # .github/workflows/test.yml name: Test Optimizers on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Build CLI run: | cd cmd/dspy-cli go build -o dspy-cli - name: Run optimizer tests env: GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }} run: | ./cmd/dspy-cli/dspy-cli try bootstrap --max-examples 5 ./cmd/dspy-cli/dspy-cli try mipro --max-examples 5\rTroubleshooting Common Issues \u0026ldquo;No API key found\u0026rdquo; # Make sure you\u0026#39;ve set an API key export GEMINI_API_KEY=\u0026#34;your-key\u0026#34; # Verify it\u0026#39;s set echo $GEMINI_API_KEY\r\u0026ldquo;Command not found: dspy-cli\u0026rdquo; # Add $GOPATH/bin to your PATH export PATH=$PATH:$(go env GOPATH)/bin # Or run from the build directory cd cmd/dspy-cli ./dspy-cli list\rTimeout Errors # Increase timeout for long-running optimizations dspy-cli try gepa --timeout 600 # 10 minutes\rExamples Quick Start # 1. List available optimizers dspy-cli list # 2. Get a recommendation dspy-cli recommend balanced # 3. Try the recommended optimizer dspy-cli try mipro --dataset gsm8k --max-examples 10 # 4. Compare with others dspy-cli compare bootstrap mipro --max-examples 10\rProduction Testing # Full evaluation with 50 examples dspy-cli try gepa \\ --dataset gsm8k \\ --max-examples 50 \\ --verbose \\ --output production_results.json \\ --json # Parallel comparison of top optimizers dspy-cli compare mipro gepa simba \\ --dataset hotpotqa \\ --max-examples 30 \\ --parallel \\ --output comparison.csv \\ --format csv\rNext Steps Configuration Reference ‚Üí - Configure the CLI Getting Started ‚Üí - Learn the basics Optimizers Guide ‚Üí - Understand each optimizer ","date":"2025-10-13","id":13,"permalink":"/dspy-go/docs/reference/cli-reference/","summary":"All commands, flags, and usage examples for the dspy-go CLI tool","tags":[],"title":"CLI Reference"},{"content":"dspy-go supports multiple LLM providers with native integrations. Each provider has unique capabilities and configuration options.\nSupported Providers Provider Streaming Multimodal Function Calling Local Best For Google Gemini ‚úÖ ‚úÖ ‚úÖ ‚ùå Multimodal, long context (2M tokens!) OpenAI ‚úÖ ‚úÖ ‚úÖ ‚ùå Latest GPT-5 models, reliability Anthropic Claude ‚úÖ ‚úÖ ‚úÖ ‚ùå Long context, reasoning Ollama ‚úÖ ‚ùå ‚ùå ‚úÖ Local Llama 3.2, Qwen 2.5, privacy LlamaCpp ‚úÖ ‚ùå ‚ùå ‚úÖ Local GGUF models, quantization LiteLLM ‚úÖ ‚úÖ ‚úÖ ‚ùå Unified API for 100+ models Google Gemini Best for: Multimodal applications, 2M token context, cost-effective\nSetup import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Basic setup llm, err := llms.NewGeminiLLM(\u0026#34;your-api-key\u0026#34;, core.ModelGoogleGeminiPro) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm)\rAvailable Models Model Context Window Features Best For gemini-2.5-pro 2M tokens Multimodal, function calling, best reasoning Complex tasks, entire codebases gemini-2.5-flash 1M tokens Fast, cost-effective, multimodal Quick responses, high volume gemini-2.5-flash-lite 1M tokens Ultra-fast, efficient Lightweight tasks, batch processing Configuration llm, err := llms.NewGeminiLLM(\u0026#34;api-key\u0026#34;, core.ModelGoogleGeminiPro, llms.WithTemperature(0.7), // Creativity (0.0-1.0) llms.WithMaxTokens(2048), // Max output tokens llms.WithTopP(0.9), // Nucleus sampling llms.WithTopK(40), // Top-K sampling llms.WithStopSequences([]string{\u0026#34;END\u0026#34;, \u0026#34;\\n\\n\u0026#34;}), )\rMultimodal Support // Analyze images imageData, _ := os.ReadFile(\u0026#34;image.jpg\u0026#34;) result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), \u0026#34;question\u0026#34;: \u0026#34;What\u0026#39;s in this image?\u0026#34;, }) // Multiple images result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;image1\u0026#34;: core.NewImageContent(data1, \u0026#34;image/jpeg\u0026#34;), \u0026#34;image2\u0026#34;: core.NewImageContent(data2, \u0026#34;image/jpeg\u0026#34;), \u0026#34;question\u0026#34;: \u0026#34;What changed between these images?\u0026#34;, })\rStreaming llm.SetStreaming(true) // Handle streaming chunks llm.SetStreamHandler(func(chunk string) { fmt.Print(chunk) }) result, err := llm.Generate(ctx, prompt)\rRate Limits \u0026amp; Pricing Model RPM (Free) RPM (Paid) Cost (Input/Output) gemini-2.5-pro 2 360 $0.00125 / $0.005 per 1K tokens gemini-2.5-flash 15 1000 $0.00004 / $0.00015 per 1K tokens gemini-2.5-flash-lite 30 2000 $0.00002 / $0.00006 per 1K tokens Best Practices // ‚úÖ Use 2.5-flash for speed and cost llm, _ := llms.NewGeminiLLM(key, core.ModelGoogleGeminiFlash) // ‚úÖ Leverage 2M token context for RAG // No need to chunk! Can handle entire codebases // ‚úÖ Use for multimodal tasks llm, _ := llms.NewGeminiLLM(key, core.ModelGoogleGeminiPro)\rGet API Key: Google AI Studio\nOpenAI Best for: GPT-5 models, reliability, ecosystem\nSetup import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Basic setup llm, err := llms.NewOpenAI(core.ModelOpenAIGPT5, \u0026#34;your-api-key\u0026#34;) core.SetDefaultLLM(llm)\rAvailable Models Model Context Window Features Best For gpt-5 256K Flagship model, multimodal, superior reasoning Most complex tasks gpt-5-mini 256K Efficient, fast, multimodal Balanced tasks gpt-5-nano 128K Ultra-efficient, fast High-volume, quick tasks gpt-4o 128K Optimized, fast, multimodal General purpose gpt-4o-mini 128K Affordable, fast High-volume tasks gpt-4-turbo 128K Latest GPT-4, multimodal Complex reasoning gpt-4 8K Proven, reliable Production apps gpt-3.5-turbo 16K Fast, cheap Quick tasks, chat Configuration llm, err := llms.NewOpenAI(core.ModelOpenAIGPT5, \u0026#34;api-key\u0026#34;, llms.WithTemperature(0.7), // Creativity llms.WithMaxTokens(4096), // Max output llms.WithTopP(0.9), // Nucleus sampling llms.WithPresencePenalty(0.1), // Discourage repetition llms.WithFrequencyPenalty(0.1), // Penalize frequent words llms.WithStopSequences([]string{\u0026#34;\\n\\n\u0026#34;}), )\rFunction Calling // Define functions functions := []core.Function{ { Name: \u0026#34;get_weather\u0026#34;, Description: \u0026#34;Get current weather for a location\u0026#34;, Parameters: map[string]interface{}{ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: map[string]interface{}{ \u0026#34;location\u0026#34;: map[string]interface{}{ \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;City name\u0026#34;, }, }, \u0026#34;required\u0026#34;: []string{\u0026#34;location\u0026#34;}, }, }, } llm.SetFunctions(functions)\rAzure OpenAI llm, err := llms.NewOpenAI(\u0026#34;gpt-5\u0026#34;, \u0026#34;api-key\u0026#34;, llms.WithBaseURL(\u0026#34;https://your-resource.openai.azure.com\u0026#34;), llms.WithAPIVersion(\u0026#34;2024-02-15-preview\u0026#34;), llms.WithAPIType(\u0026#34;azure\u0026#34;), )\rStreaming llm.SetStreaming(true) llm.SetStreamHandler(func(chunk string) { fmt.Print(chunk) }) result, err := llm.Generate(ctx, prompt)\rRate Limits \u0026amp; Pricing Model TPM (Tier 1) Cost (Input/Output) gpt-5 500K $0.005 / $0.015 per 1K tokens (estimated) gpt-5-mini 1M $0.0015 / $0.004 per 1K tokens (estimated) gpt-5-nano 2M $0.0005 / $0.001 per 1K tokens (estimated) gpt-4o 500K $0.0025 / $0.01 per 1K tokens gpt-4o-mini 2M $0.00015 / $0.0006 per 1K tokens gpt-4-turbo 300K $0.01 / $0.03 per 1K tokens gpt-4 40K $0.03 / $0.06 per 1K tokens gpt-3.5-turbo 200K $0.0005 / $0.0015 per 1K tokens Best Practices // ‚úÖ Use GPT-5 for most complex reasoning llm, _ := llms.NewOpenAI(core.ModelOpenAIGPT5, key) // ‚úÖ Use gpt-5-nano for high-volume tasks llm, _ := llms.NewOpenAI(core.ModelOpenAIGPT5Nano, key) // ‚úÖ Use gpt-4o for production balance llm, _ := llms.NewOpenAI(core.ModelOpenAIGPT4o, key) // ‚úÖ Implement retry logic llm.SetMaxRetries(3) llm.SetRetryDelay(time.Second)\rGet API Key: OpenAI Platform\nAnthropic Claude Best for: Long context, detailed reasoning, safety\nSetup import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/core\u0026#34; \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Basic setup llm, err := llms.NewAnthropicLLM(\u0026#34;your-api-key\u0026#34;, core.ModelAnthropicSonnet) core.SetDefaultLLM(llm)\rAvailable Models Model Context Window Features Best For claude-3.5-sonnet 200K Latest, balanced, multimodal General purpose, production claude-3-opus 200K Most capable, best reasoning Complex analysis, research claude-3-haiku 200K Fast, efficient Quick tasks, high volume Configuration llm, err := llms.NewAnthropicLLM(\u0026#34;api-key\u0026#34;, core.ModelAnthropicSonnet, llms.WithTemperature(0.7), llms.WithMaxTokens(4096), llms.WithTopP(0.9), llms.WithTopK(40), )\rMultimodal Support // Analyze images with Claude imageData, _ := os.ReadFile(\u0026#34;document.jpg\u0026#34;) result, err := predictor.Process(ctx, map[string]interface{}{ \u0026#34;image\u0026#34;: core.NewImageContent(imageData, \u0026#34;image/jpeg\u0026#34;), \u0026#34;question\u0026#34;: \u0026#34;Extract all text from this document\u0026#34;, })\rStreaming llm.SetStreaming(true) llm.SetStreamHandler(func(chunk string) { fmt.Print(chunk) }) result, err := llm.Generate(ctx, prompt)\rRate Limits \u0026amp; Pricing Model TPM Cost (Input/Output) claude-3.5-sonnet 400K $0.003 / $0.015 per 1K tokens claude-3-opus 400K $0.015 / $0.075 per 1K tokens claude-3-haiku 400K $0.00025 / $0.00125 per 1K tokens Best Practices // ‚úÖ Use 3.5 Sonnet for production llm, _ := llms.NewAnthropicLLM(key, core.ModelAnthropicSonnet) // ‚úÖ Use Haiku for fast, cheap tasks llm, _ := llms.NewAnthropicLLM(key, core.ModelAnthropicHaiku) // ‚úÖ Leverage 200K context for documents // Can analyze entire books!\rGet API Key: Anthropic Console\nOllama (Local) Best for: Privacy, offline use, no API costs, Llama 3.2 \u0026amp; Qwen 2.5\nSetup import ( \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/llms\u0026#34; ) // Basic setup (assumes Ollama running on localhost:11434) llm, err := llms.NewOllamaLLM(\u0026#34;llama3:8b\u0026#34;) // Custom server llm, err := llms.NewOllamaLLM(\u0026#34;qwen2.5:7b\u0026#34;, llms.WithBaseURL(\u0026#34;http://192.168.1.100:11434\u0026#34;), )\rAvailable Models Latest models supported by dspy-go:\nModel Size Context Best For llama3.2:3b 3B 8K Efficient, fast, latest Llama llama3.1:8b 8B 128K Latest Llama 3.1, long context llama3.1:70b 70B 128K Most capable Llama qwen2.5:7b 7B 32K Latest Qwen, excellent reasoning qwen2.5:14b 14B 32K Best Qwen, superior performance codellama:13b 13B 16K Code generation codellama:34b 34B 16K Advanced code tasks mistral:7b 7B 32K Fast, efficient gemma:2b 2B 8K Ultra-efficient gemma:7b 7B 8K Balanced efficiency Installation # Install Ollama curl https://ollama.ai/install.sh | sh # Pull latest models ollama pull llama3.2:3b ollama pull qwen2.5:7b # Run Ollama server ollama serve\rConfiguration llm, err := llms.NewOllamaLLM(\u0026#34;llama3.1:8b\u0026#34;, llms.WithTemperature(0.8), llms.WithNumCtx(8192), // Context window llms.WithNumPredict(2048), // Max tokens llms.WithNumGPU(1), // GPU layers llms.WithRepeatPenalty(1.1), // Repetition penalty )\rStreaming llm.SetStreaming(true) llm.SetStreamHandler(func(chunk string) { fmt.Print(chunk) }) result, err := llm.Generate(ctx, prompt)\rEmbedding Models // Use Ollama for embeddings llm, err := llms.NewOllamaLLM(\u0026#34;nomic-embed-text\u0026#34;) embeddings, err := llm.CreateEmbedding(ctx, \u0026#34;text to embed\u0026#34;)\rAvailable embedding models:\nnomic-embed-text - 768 dimensions, best quality mxbai-embed-large - 1024 dimensions, large all-minilm - 384 dimensions, fast Performance Tips # Use quantized models for speed ollama pull llama3.2:3b-q4_K_M # Enable GPU acceleration export OLLAMA_NUM_GPU=1 # Increase context for long documents ollama run llama3.1:8b --ctx-size 16384\rBest Practices // ‚úÖ Use Llama 3.2 for latest capabilities llm, _ := llms.NewOllamaLLM(\u0026#34;llama3.2:3b\u0026#34;) // ‚úÖ Use Qwen 2.5 for best reasoning llm, _ := llms.NewOllamaLLM(\u0026#34;qwen2.5:7b\u0026#34;) // ‚úÖ Use CodeLlama for code tasks llm, _ := llms.NewOllamaLLM(\u0026#34;codellama:13b\u0026#34;) // ‚úÖ Batch requests for efficiency\rGet Started: ollama.com\nLlamaCpp (Local GGUF) Best for: Running quantized models locally, maximum control, GGUF format\nSetup // Basic setup (assumes llama.cpp server on localhost:8080) llm, err := llms.NewLlamacppLLM(\u0026#34;http://localhost:8080\u0026#34;) if err != nil { log.Fatal(err) } core.SetDefaultLLM(llm) // Custom configuration llm, err := llms.NewLlamacppLLM(\u0026#34;http://localhost:8080\u0026#34;, llms.WithTemperature(0.7), llms.WithMaxTokens(2048), )\rInstallation # Clone llama.cpp git clone https://github.com/ggerganov/llama.cpp cd llama.cpp # Build with GPU support (optional) make LLAMA_CUBLAS=1 # Download a GGUF model from Hugging Face # Example: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF wget https://huggingface.co/.../llama-2-7b-chat.Q4_K_M.gguf # Start server ./server -m llama-2-7b-chat.Q4_K_M.gguf --port 8080\rAvailable Models Any GGUF quantized model from Hugging Face:\nQuantization Size Quality Use Case Q2_K Smallest Lower Testing, memory-constrained Q4_K_M Medium Good Balanced performance Q5_K_M Larger Better Recommended for most Q8_0 Largest Best Maximum quality F16 Full Native Best quality, large memory Configuration llm, err := llms.NewLlamacppLLM(\u0026#34;http://localhost:8080\u0026#34;, llms.WithTemperature(0.8), llms.WithTopK(40), llms.WithTopP(0.9), llms.WithRepeatPenalty(1.1), )\rPopular GGUF Models Llama 3.1 8B - Latest Meta Llama Qwen 2.5 7B - Excellent reasoning Mistral 7B - Fast, efficient CodeLlama 13B - Code generation Yi 34B - Strong general purpose Find more: Hugging Face GGUF Models\nStreaming llm.SetStreaming(true) llm.SetStreamHandler(func(chunk string) { fmt.Print(chunk) }) result, err := llm.Generate(ctx, prompt)\rBest Practices // ‚úÖ Use Q4_K_M for balance // Good quality, reasonable size // ‚úÖ Use Q5_K_M for better quality // Slightly larger, better output // ‚úÖ Monitor GPU memory usage // Adjust context size if needed // ‚úÖ Use --ctx-size for long contexts ./server -m model.gguf --ctx-size 8192\rLiteLLM (Unified API) Best for: Supporting 100+ models through one API, multi-provider flexibility\nSetup // Basic setup (assumes LiteLLM proxy running) config := core.ProviderConfig{ Name: \u0026#34;litellm\u0026#34;, BaseURL: \u0026#34;http://localhost:4000\u0026#34;, } llm, err := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;gpt-4\u0026#34;) core.SetDefaultLLM(llm) // With API key llm, err := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;claude-3-sonnet\u0026#34;, llms.WithAPIKey(\u0026#34;your-litellm-key\u0026#34;), )\rSupported Providers LiteLLM provides unified access to 100+ models:\nCategory Providers Major APIs OpenAI, Anthropic, Google, Cohere Cloud AWS Bedrock, Azure OpenAI, Vertex AI Open Source Hugging Face, Replicate, Together AI Local Ollama, LlamaCpp, LocalAI Installation # Install LiteLLM pip install litellm[proxy] # Create config file cat \u0026gt; litellm_config.yaml \u0026lt;\u0026lt;EOF model_list: - model_name: gpt-4 litellm_params: model: openai/gpt-4 api_key: os.environ/OPENAI_API_KEY - model_name: claude-3-sonnet litellm_params: model: anthropic/claude-3-sonnet-20240229 api_key: os.environ/ANTHROPIC_API_KEY - model_name: llama-3-70b litellm_params: model: together_ai/meta-llama/Llama-3-70b-chat-hf api_key: os.environ/TOGETHER_API_KEY EOF # Start proxy server litellm --config litellm_config.yaml --port 4000\rConfiguration // Use any provider through LiteLLM config := core.ProviderConfig{ Name: \u0026#34;litellm\u0026#34;, BaseURL: \u0026#34;http://localhost:4000\u0026#34;, } // OpenAI GPT-4 llmGPT4, _ := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;gpt-4\u0026#34;) // Anthropic Claude llmClaude, _ := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;claude-3-sonnet\u0026#34;) // Together AI Llama llmLlama, _ := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;llama-3-70b\u0026#34;)\rModel Routing # litellm_config.yaml - Advanced routing router_settings: routing_strategy: least-busy model_list: - model_name: gpt-4 litellm_params: model: openai/gpt-4 api_key: os.environ/OPENAI_API_KEY - model_name: gpt-4 litellm_params: model: azure/gpt-4 api_key: os.environ/AZURE_API_KEY api_base: os.environ/AZURE_ENDPOINT\rLoad Balancing // LiteLLM automatically load balances // Just configure multiple instances in litellm_config.yaml llm, err := llms.LiteLLMProviderFactory(ctx, config, \u0026#34;gpt-4\u0026#34;) // Requests automatically distributed across providers\rCost Tracking LiteLLM provides built-in cost tracking:\n# View costs curl http://localhost:4000/spend/logs # Set budget limits in config general_settings: master_key: sk-1234 budget_duration: 30d max_budget: 100\rBest Practices // ‚úÖ Use for multi-provider applications // Switch providers without code changes // ‚úÖ Implement fallback logic // LiteLLM can auto-fallback to backup models // ‚úÖ Monitor costs centrally // Single dashboard for all providers // ‚úÖ Use for A/B testing // Easy to compare different models\rGet Started: LiteLLM Docs\nProvider Comparison Performance Benchmarks Provider Latency (P50) Throughput Cost Efficiency Gemini 2.5 Flash 200ms ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê GPT-5 Nano 300ms ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Claude Haiku 250ms ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Ollama (local) 50ms ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Feature Matrix Feature Gemini OpenAI Claude Ollama LlamaCpp LiteLLM Streaming ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚úÖ Multimodal ‚úÖ ‚úÖ ‚úÖ ‚ùå ‚ùå ‚úÖ Function Calling ‚úÖ ‚úÖ ‚úÖ ‚ùå ‚ùå ‚úÖ Embeddings ‚úÖ ‚úÖ ‚ùå ‚úÖ ‚úÖ ‚úÖ JSON Mode ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚úÖ Long Context 2M tokens 256K tokens 200K tokens 128K Varies Varies Context Window Comparison Gemini 2.5 Pro ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 2M tokens\rGPT-5 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 256K tokens\rClaude 3.5 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 200K tokens\rLlama 3.1 70B ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 128K tokens\rGPT-4o ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 128K tokens\rMistral 7B ‚ñà‚ñà‚ñà‚ñà 32K tokens\rGPT-4 ‚ñà 8K tokens\rEnvironment Variables Quick reference for all providers:\n# Google Gemini export GEMINI_API_KEY=\u0026#34;your-api-key\u0026#34; # OpenAI export OPENAI_API_KEY=\u0026#34;your-api-key\u0026#34; export OPENAI_BASE_URL=\u0026#34;https://api.openai.com/v1\u0026#34; # optional # Anthropic Claude export ANTHROPIC_API_KEY=\u0026#34;your-api-key\u0026#34; # Ollama (local) export OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34; # LiteLLM export LITELLM_BASE_URL=\u0026#34;http://localhost:4000\u0026#34; export LITELLM_API_KEY=\u0026#34;optional-key\u0026#34;\rTroubleshooting Rate Limit Errors // Implement exponential backoff llm.SetMaxRetries(5) llm.SetRetryDelay(2 * time.Second) llm.SetRetryBackoff(true)\rContext Length Errors // Check model\u0026#39;s context window maxContext := llm.GetContextWindow() // Truncate if needed if len(prompt) \u0026gt; maxContext { prompt = prompt[:maxContext] }\rAPI Key Issues // Verify API key is set apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) if apiKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;API key not found\u0026#34;) } // Test with simple request result, err := llm.Generate(ctx, \u0026#34;Hello, world!\u0026#34;)\rLocal Model Issues # Ollama - Check if running curl http://localhost:11434/api/tags # LlamaCpp - Check server curl http://localhost:8080/health\rNext Steps Configuration Reference ‚Üí - Detailed configuration options Getting Started ‚Üí - Quick start guide Multimodal Guide ‚Üí - Work with images and vision ","date":"2025-10-13","id":14,"permalink":"/dspy-go/docs/reference/llm-providers/","summary":"Provider-specific configurations, capabilities, and best practices","tags":[],"title":"LLM Providers"},{"content":"Resources Comprehensive collection of tools, examples, and learning materials for dspy-go.\nOfficial Resources Documentation GoDoc API Reference - Complete API documentation GitHub Repository - Source code and latest updates Getting Started Guide - Quick start tutorial Core Concepts - Understanding Signatures, Modules, Programs Optimizers Guide - Master GEPA, MIPRO, SIMBA Tools dspy-cli - Command-line tool for testing optimizers Compatibility Testing Framework - Validate against Python DSPy Examples Core Examples Quick Start Sentiment Analysis - Simple prediction example Question Answering - HotPotQA implementation Math Problems - GSM8K math reasoning Advanced Features Smart Tool Registry - Bayesian tool selection Tool Chaining - Sequential pipeline execution Tool Composition - Reusable composite tools Parallel Processing - Concurrent batch operations Refine Module - Quality improvement MultiChainComparison - Multi-perspective reasoning Multimodal Multimodal Processing - Image analysis and vision Q\u0026amp;A Image analysis with questions Vision question answering Multimodal chat Streaming multimodal content Multiple image comparison Optimizers MIPRO Example - TPE-based optimization SIMBA Example - Introspective learning GEPA Example - Evolutionary optimization Agent Patterns Agent Examples - Various agent implementations ReAct pattern Orchestrator pattern Memory management Production Applications Maestro - Code Review Agent GitHub: XiaoConstantine/maestro\nA production code review and question-answering agent built with dspy-go. Demonstrates:\nRAG pipeline implementation Tool integration (MCP) Smart tool registry usage Production deployment patterns Key Features üîç Automated code review üí¨ Natural language Q\u0026amp;A about codebases üîß MCP tool integration üìä Performance optimization with MIPRO Learning Materials Video Tutorials Coming soon! Check the GitHub repository for announcements.\nBlog Posts \u0026amp; Articles Introduction to DSPy - Original DSPy paper Building LLM Applications with Go (coming soon) Prompt Optimization Strategies (coming soon) Community Examples Check GitHub Discussions for community-contributed examples and patterns.\nDatasets Built-in Dataset Support dspy-go includes automatic downloading and management for popular datasets:\nGSM8K - Grade School Math import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/datasets\u0026#34; // Automatically downloads if not present gsm8kPath, err := datasets.EnsureDataset(\u0026#34;gsm8k\u0026#34;) dataset, err := datasets.LoadGSM8K(gsm8kPath)\rUse for: Math reasoning, chain-of-thought, optimization\nHotPotQA - Multi-hop Question Answering hotpotPath, err := datasets.EnsureDataset(\u0026#34;hotpotqa\u0026#34;) dataset, err := datasets.LoadHotPotQA(hotpotPath)\rUse for: Multi-step reasoning, RAG pipelines, complex Q\u0026amp;A\nCustom Datasets // Create in-memory dataset dataset := datasets.NewInMemoryDataset() dataset.AddExample(map[string]interface{}{ \u0026#34;question\u0026#34;: \u0026#34;What is the capital of France?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Paris\u0026#34;, })\rLLM Provider Setup Google Gemini export GEMINI_API_KEY=\u0026#34;your-api-key\u0026#34;\r‚úÖ Multimodal support (images) ‚úÖ Fast responses ‚úÖ Good for prototyping Get API Key ‚Üí\nOpenAI export OPENAI_API_KEY=\u0026#34;your-api-key\u0026#34;\r‚úÖ GPT-4, GPT-3.5 ‚úÖ Reliable performance ‚úÖ Well-documented Get API Key ‚Üí\nAnthropic Claude export ANTHROPIC_API_KEY=\u0026#34;your-api-key\u0026#34;\r‚úÖ Long context windows ‚úÖ Strong reasoning ‚úÖ Safety features Get API Key ‚Üí\nOllama (Local) # Install Ollama curl -fsSL https://ollama.com/install.sh | sh # Pull a model ollama pull llama2 # Set base URL export OLLAMA_BASE_URL=\u0026#34;http://localhost:11434\u0026#34;\r‚úÖ Free, local execution ‚úÖ Privacy (no data leaves your machine) ‚úÖ No API costs Install Ollama ‚Üí\nDevelopment Tools IDE Extensions Go extension for VS Code - Essential for Go development GitHub Copilot - AI pair programmer Testing \u0026amp; Debugging // Enable detailed tracing ctx = core.WithExecutionState(context.Background()) // Execute your program result, err := program.Execute(ctx, inputs) // Inspect trace executionState := core.GetExecutionState(ctx) steps := executionState.GetSteps(\u0026#34;moduleId\u0026#34;) for _, step := range steps { fmt.Printf(\u0026#34;Duration: %s\\n\u0026#34;, step.Duration) fmt.Printf(\u0026#34;Prompt: %s\\n\u0026#34;, step.Prompt) fmt.Printf(\u0026#34;Response: %s\\n\u0026#34;, step.Response) }\rMonitoring import \u0026#34;github.com/XiaoConstantine/dspy-go/pkg/logging\u0026#34; // Configure logging logger := logging.NewLogger(logging.Config{ Severity: logging.DEBUG, Outputs: []logging.Output{logging.NewConsoleOutput(true)}, }) logging.SetLogger(logger)\rCommunity \u0026amp; Support Get Help GitHub Issues - Bug reports and feature requests GitHub Discussions - Questions and community support Stack Overflow - Q\u0026amp;A with the community Contributing Contributing Guide - How to contribute Development Setup - Set up your dev environment Code of Conduct - Community guidelines Stay Updated ‚≠ê Star on GitHub - Get notifications üëÄ Watch Releases - Stay informed of new versions üí¨ Join Discussions - Participate in the community Related Projects MCP (Model Context Protocol) mcp-go - Go implementation of MCP MCP Servers - Official MCP servers DSPy Ecosystem DSPy (Python) - Original Python implementation DSPy Documentation - Python DSPy docs Benchmarks \u0026amp; Performance Compatibility Results dspy-go maintains compatibility with Python DSPy implementations. See:\nCompatibility Test Results - Validation against Python DSPy Performance Metrics Parallel processing can improve throughput by 3-4x Smart Tool Registry adds \u0026lt; 50ms overhead Optimization times vary by optimizer (see Optimizers Guide) Tips \u0026amp; Best Practices Getting Started ‚úÖ Start with BootstrapFewShot optimizer ‚úÖ Use clear, detailed field descriptions ‚úÖ Test with small datasets first ‚úÖ Enable logging during development Production Readiness ‚úÖ Use train/validation splits ‚úÖ Monitor performance metrics ‚úÖ Implement error handling ‚úÖ Cache results where possible ‚úÖ Use Parallel module for batches Optimization ‚úÖ Aim for 50+ training examples ‚úÖ Balance dataset (don\u0026rsquo;t skew toward one class) ‚úÖ Start simple, then optimize ‚úÖ Validate on held-out data Next Steps Getting Started ‚Üí - Build your first application Core Concepts ‚Üí - Master the fundamentals Optimizers ‚Üí - Improve your prompts automatically Examples - Learn from working code ","date":"2024-02-27","id":15,"permalink":"/dspy-go/docs/resources/","summary":"Examples, tools, and community resources for building with dspy-go","tags":[],"title":"Resources"},{"content":"Welcome to dspy-go dspy-go is a native Go implementation of the DSPy framework that brings systematic prompt engineering and automated reasoning capabilities to Go applications. Build reliable and effective Language Model (LLM) applications through composable modules, powerful optimizers, and intelligent tool management.\nWhy dspy-go? üéØ Go-Native Architecture Built from the ground up for Go‚Äînot just a port. Idiomatic Go patterns, strong typing, and seamless integration with your existing Go applications.\nüöÄ Zero to Production in Minutes Get started with zero configuration. One-line setup automatically detects your environment and configures the appropriate LLM provider.\nllm, _ := llms.NewGeminiLLM(\u0026#34;\u0026#34;, core.ModelGoogleGeminiPro) core.SetDefaultLLM(llm) // Ready to go!\rüß† Advanced Optimizers State-of-the-art prompt optimization algorithms that automatically improve your prompts:\nGEPA: Multi-objective evolutionary optimization with Pareto selection MIPRO: TPE-based systematic prompt optimization SIMBA: Introspective mini-batch learning BootstrapFewShot: Automated few-shot example selection COPRO: Collaborative multi-module optimization üîß Intelligent Tool Management Smart Tool Registry uses Bayesian inference for intelligent tool selection. Automatic performance tracking, capability analysis, and MCP (Model Context Protocol) integration.\n‚ö° Built for Performance Parallel module execution out of the box Concurrent batch processing Efficient streaming support Optimized for production workloads üé® Multimodal from Day One Native support for:\nImage analysis and vision Q\u0026amp;A Multimodal chat conversations Streaming multimodal content Content block system for flexible media handling ü§ñ Intelligent Agents Build production-ready agents with reasoning, tool use, and memory:\nReAct Pattern: Reasoning + Acting with iterative tool execution Memory Management: Buffer and summary memory for conversation context Orchestration: Task decomposition and multi-agent coordination Custom Tools: Easy integration with APIs, databases, and external systems Reflection: Self-improving agents that learn from their actions ACE Framework: Agentic Context Engineering for self-improving agents // Create a self-improving agent with ACE aceConfig := ace.Config{ Enabled: true, LearningsPath: \u0026#34;./learnings/agent.md\u0026#34;, } agent := react.NewReActAgent(\u0026#34;my-agent\u0026#34;, \u0026#34;Assistant\u0026#34;, react.WithACE(aceConfig), // Enable self-improvement! ) // Agent automatically: // 1. Records execution trajectories // 2. Learns from successes and failures // 3. Applies learnings to future tasks\rWhat Can You Build? Question Answering Systems: RAG pipelines with retrieval and generation Code Analysis Tools: Automated code review and refactoring suggestions Data Processing Pipelines: Extract, transform, and analyze data with LLMs Intelligent Agents: ReAct patterns with tool use and reasoning Multi-Step Workflows: Chain modules for complex task decomposition Optimization Systems: Automatically improve prompt performance Key Features at a Glance Feature Description Modular Design Compose simple, reusable components into complex applications Multiple LLMs Anthropic, OpenAI, Google Gemini, Ollama, LlamaCPP, and more Intelligent Agents ReAct patterns, ACE framework, memory management, and custom tools A2A Protocol Multi-agent orchestration with hierarchical composition RLM Module Large context exploration through sandboxed Go REPL XML Adapters Structured output parsing with security controls Tool Chaining Sequential pipelines with data transformation and conditional logic Dependency Resolution Automatic execution planning with parallel optimization CLI Tool Explore optimizers and view RLM session logs Compatibility Testing Framework ensures Python DSPy parity Dataset Management Auto-download GSM8K, HotPotQA, and more Quick Links Getting Started ‚Üí - Install and run your first program Core Concepts ‚Üí - Understand Signatures, Modules, and Programs Building Agents ‚Üí - ReAct patterns, ACE framework, and memory A2A Protocol ‚Üí - Multi-agent orchestration and composition RLM Module ‚Üí - Large context exploration with recursive LLM XML Adapters ‚Üí - Structured output with XML parsing Optimizers ‚Üí - Master GEPA, MIPRO, SIMBA and more Tool Management ‚Üí - Smart Registry, MCP, and tool chaining Examples ‚Üí - Real-world implementations Community \u0026amp; Support GitHub: XiaoConstantine/dspy-go Go Docs: pkg.go.dev Example App: Maestro - Code review agent built with dspy-go Ready to get started? Head to Getting Started ‚Üí\n","date":"2023-09-07","id":16,"permalink":"/dspy-go/docs/","summary":"Learn how to build reliable, production-ready LLM applications using systematic prompt engineering","tags":[],"title":"Documentation"},{"content":"","date":"2025-10-13","id":17,"permalink":"/dspy-go/","summary":"","tags":[],"title":"dspy-go"},{"content":"","date":"2025-01-07","id":18,"permalink":"/dspy-go/categories/advanced/","summary":"","tags":[],"title":"Advanced"},{"content":"","date":"2025-01-07","id":19,"permalink":"/dspy-go/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"2025-01-07","id":20,"permalink":"/dspy-go/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"2025-01-07","id":21,"permalink":"/dspy-go/categories/features/","summary":"","tags":[],"title":"Features"},{"content":"","date":"2025-01-07","id":22,"permalink":"/dspy-go/tags/large-context/","summary":"","tags":[],"title":"Large-Context"},{"content":"","date":"2025-01-07","id":23,"permalink":"/dspy-go/tags/modules/","summary":"","tags":[],"title":"Modules"},{"content":"","date":"2025-01-07","id":24,"permalink":"/dspy-go/tags/rlm/","summary":"","tags":[],"title":"Rlm"},{"content":"","date":"2025-01-07","id":25,"permalink":"/dspy-go/tags/","summary":"","tags":[],"title":"Tags"},{"content":"","date":"2025-01-07","id":26,"permalink":"/dspy-go/tags/token-efficiency/","summary":"","tags":[],"title":"Token-Efficiency"},{"content":"","date":"2025-01-07","id":27,"permalink":"/dspy-go/contributors/xiao-constantine/","summary":"","tags":[],"title":"Xiao Constantine"},{"content":"","date":"2023-09-07","id":28,"permalink":"/dspy-go/privacy/","summary":"","tags":[],"title":"Privacy Policy"}]