# Multimodal Gemini Configuration
llm:
  provider: "google"
  default:
    model: "gemini-2.5-flash"
    api_key: "${GEMINI_API_KEY}"
    endpoint:
      base_url: "https://generativelanguage.googleapis.com/v1beta"
      timeout: 60
    generation:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.9

# Logging configuration
logging:
  level: "info"
  sample_rate: 1

# Execution settings
execution:
  default_timeout: "60s"
  max_concurrency: 5
  tracing:
    enabled: true
    sampling_rate: 1.0

# Example-specific settings
examples:
  multimodal:
    # Image processing settings
    image:
      max_size_mb: 20
      supported_formats: ["jpeg", "jpg", "png", "webp", "gif"]
      resize_threshold: 2048

    # Default prompts
    prompts:
      image_analysis: "Analyze the provided image and answer the given question about it."
      vision_qa: "Examine the image carefully and provide detailed observations, then answer the specific task."
      chat: "You are a helpful assistant that can see and analyze images. Respond naturally to the user's message while considering the provided image."

    # Streaming settings
    streaming:
      enabled: true
      chunk_size: 1024
      timeout: "30s"

    # Sample images directory
    sample_images_dir: "./examples/multimodal/images"

    # Output settings
    output:
      format: "json"
      include_metadata: true
      include_token_usage: true
