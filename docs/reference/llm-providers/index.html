<!doctype html><html lang=en data-bs-theme=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=preload href=https://xiaoconstantine.github.io/dspy-go/fonts/vendor/jost/jost-v4-latin-regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://xiaoconstantine.github.io/dspy-go/fonts/vendor/jost/jost-v4-latin-500.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://xiaoconstantine.github.io/dspy-go/fonts/vendor/jost/jost-v4-latin-700.woff2 as=font type=font/woff2 crossorigin><script src=/dspy-go/js/color-mode.86a91f050a481d0a3f0c72ac26543cb6228c770875981c58dcbc008fd3f875c8.js integrity="sha256-hqkfBQpIHQo/DHKsJlQ8tiKMdwh1mBxY3LwAj9P4dcg="></script><link rel=stylesheet href="/dspy-go/main.ae8f61f265c5133811ec6b15586fbdab0adedfa204457d8fd61b681b73c74caedcfc5e511865466c89e79b14041964ca4192b5ff9fb9124488cd0b13d400cb94.css" integrity="sha512-ro9h8mXFEzgR7GsVWG+9qwre36IERX2P1htoG3PHTK7c/F5RGGVGbInnmxQEGWTKQZK1/5+5EkSIzQsT1ADLlA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><base href=https://xiaoconstantine.github.io/dspy-go/docs/reference/llm-providers/><link rel=canonical href=https://xiaoconstantine.github.io/dspy-go/docs/reference/llm-providers/><title>LLM Providers - dspy-go</title><meta name=description content="Complete guide to LLM providers in dspy-go"><link rel=icon href=/dspy-go/favicon.ico sizes=32x32><link rel=icon href=/dspy-go/favicon.svg type=image/svg+xml><link rel=apple-touch-icon href=/dspy-go/dspy-go/apple-touch-icon.png sizes=180x180 type=image/png><link rel=icon href=/dspy-go/favicon-192x192.png sizes=192x192 type=image/png><link rel=icon href=/dspy-go/favicon-512x512.png sizes=512x512 type=image/png><link rel=manifest href=/dspy-go/manifest.webmanifest><meta property="og:url" content="https://xiaoconstantine.github.io/dspy-go/docs/reference/llm-providers/"><meta property="og:site_name" content="dspy-go"><meta property="og:title" content="LLM Providers"><meta property="og:description" content="Complete guide to LLM providers supported by dspy-go"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-10-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-13T00:00:00+00:00"><meta property="og:image" content="https://xiaoconstantine.github.io/dspy-go/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xiaoconstantine.github.io/dspy-go/cover.png"><meta name=twitter:title content="LLM Providers"><meta name=twitter:description content="Complete guide to LLM providers supported by dspy-go"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://xiaoconstantine.github.io/dspy-go/","name":"Dspy Go","position":1},{"@type":"ListItem","item":"https://xiaoconstantine.github.io/dspy-go/docs/","name":"Documentation","position":2},{"@type":"ListItem","item":"https://xiaoconstantine.github.io/dspy-go/docs/reference/","name":"API Reference","position":3},{"@type":"ListItem","name":"Llm Providers","position":4}]}</script></head><body class="single section docs" data-bs-spy=scroll data-bs-target=#toc data-bs-root-margin="0px 0px -60%" data-bs-smooth-scroll=true tabindex=0><div class=sticky-top><header class="navbar navbar-expand-lg"><div class=container-lg><a class="navbar-brand me-auto me-lg-3" href=/dspy-go/>dspy-go</a>
<button type=button id=searchToggleMobile class="btn btn-link nav-link mx-2 d-lg-none" aria-label="Search website">
<svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
</button>
<button class="btn btn-link d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasNavSection aria-controls=offcanvasNavSection aria-label="Open section navigation menu">
<svg class="icon icon-tabler icon-tabler-dots-vertical" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M12 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M12 5m-1 0a1 1 0 102 0 1 1 0 10-2 0"/></svg></button><div class="offcanvas offcanvas-start d-lg-none" tabindex=-1 id=offcanvasNavSection aria-labelledby=offcanvasNavSectionLabel><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasNavSectionLabel>Docs</h5><button type=button class="btn btn-link nav-link p-0 ms-auto" data-bs-dismiss=offcanvas aria-label=Close>
<svg class="icon icon-tabler icon-tabler-x" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 6 6 18"/><path d="M6 6l12 12"/></svg></button></div><div class=offcanvas-body><aside class="doks-sidebar mt-n3"><nav id=doks-docs-nav aria-label="Tertiary navigation"><nav class="section-nav docs-links"><ul class=list-unstyled><li><details open><summary>Guides</summary><ul class="list-unstyled list-nested"><li><a href=/dspy-go/docs/guides/getting-started/>Getting Started</a></li><li><a href=/dspy-go/docs/guides/core-concepts/>Core Concepts</a></li><li><a href=/dspy-go/docs/guides/optimizers/>Optimizers</a></li><li><a href=/dspy-go/docs/guides/tool-management/>Tool Management</a></li><li><a href=/dspy-go/docs/guides/building-agents/>Building Agents</a></li><li><a href=/dspy-go/docs/guides/multimodal-processing/>Multimodal Processing</a></li></ul></details></li><li><details open open><summary>API Reference</summary><ul class="list-unstyled list-nested"><li><a href=/dspy-go/docs/reference/configuration-reference/>Configuration Reference</a></li><li><a href=/dspy-go/docs/reference/cli-reference/>CLI Reference</a></li><li class=active><a aria-current=page href=/dspy-go/docs/reference/llm-providers/>LLM Providers</a></li></ul></details></li></ul></nav></nav></aside></div></div><button class="btn btn-link nav-link mx-2 order-3 d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasNavMain aria-controls=offcanvasNavMain aria-label="Open main navigation menu">
<svg class="icon icon-tabler icon-tabler-menu" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><div class="offcanvas offcanvas-end h-auto" tabindex=-1 id=offcanvasNavMain aria-labelledby=offcanvasNavMainLabel><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasNavMainLabel>dspy-go</h5><button type=button class="btn btn-link nav-link p-0 ms-auto" data-bs-dismiss=offcanvas aria-label=Close>
<svg class="icon icon-tabler icon-tabler-x" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 6 6 18"/><path d="M6 6l12 12"/></svg></button></div><div class="offcanvas-body d-flex flex-column flex-lg-row justify-content-between"><ul class="navbar-nav flex-grow-1"><li class=nav-item><a class="nav-link active" href=https://xiaoconstantine.github.io/dspy-go/docs/guides/getting-started/ aria-current=true>Docs</a></li><li class=nav-item><a class=nav-link href=https://xiaoconstantine.github.io/dspy-go/blog/>Blog</a></li></ul><button type=button id=searchToggleDesktop class="btn btn-link nav-link p-2 d-none d-lg-block" aria-label="Search website">
<svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
</button>
<button id=buttonColorMode class="btn btn-link mx-auto nav-link p-0 ms-lg-2 me-lg-1" type=button aria-label="Toggle theme">
<svg data-bs-theme-value="dark" class="icon icon-tabler icon-tabler-moon" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/></svg>
<svg data-bs-theme-value="light" class="icon icon-tabler icon-tabler-sun" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-4 0a4 4 0 108 0 4 4 0 10-8 0m-5 0h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7-.7.7m0 11.4.7.7m-12.1-.7-.7.7"/></svg></button><ul id=socialMenu class="nav mx-auto flex-row order-lg-4"><li class=nav-item><a class="nav-link social-link" href=https://github.com/XiaoConstantine/dspy-go><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg><small class="ms-2 visually-hidden">GitHub</small></a></li></ul></div></div></div></header></div><div class=modal id=searchModal tabindex=-1 aria-labelledby=searchModalLabel aria-hidden=true><div class="modal-dialog modal-dialog-scrollable modal-fullscreen-md-down"><div class=modal-content><div class=modal-header><h1 class="modal-title fs-5 visually-hidden" id=searchModalLabel>Search</h1><button type=button class="btn-close visually-hidden" data-bs-dismiss=modal aria-label=Close></button><div class="search-input flex-grow-1 d-none"><form id=search-form class=search-form action=# method=post accept-charset=UTF-8 role=search><label for=query class=visually-hidden>Search</label><div class=d-flex><input type=search id=query name=query class="search-text form-control form-control-lg" placeholder=Search aria-label=Search maxlength=128 autocomplete=off>
<button type=button class="btn btn-link text-decoration-none px-0 ms-3 d-md-none" data-bs-dismiss=modal aria-label=Close>Cancel</button></div></form></div></div><div class=modal-body><p class="search-loading status message d-none mt-3 text-center">Loading search index…</p><p class="search-no-recent message d-none mt-3 text-center">No recent searches</p><p class="search-no-results message d-none mt-3 text-center">No results for "<strong><span class=query-no-results>Query here</span></strong>"</p><div id=searchResults class=search-results></div><template><article class="search-result list-view"><div class="card my-3"><div class=card-body><header><h2 class="h5 title title-submitted mb-0"><a class="stretched-link text-decoration-none text-reset" href=#>Title here</a></h2><div class="submitted d-none"><time class=created-date>Date here</time></div></header><div class=content>Summary here</div></div></div></article></template></div><div class=modal-footer><ul class="list-inline me-auto d-none d-md-block"><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"/></g></svg></kbd><span class=DocSearch-Label>to select</span></li><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"/></g></svg></kbd><kbd class=me-2><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"/></g></svg></kbd><span class=DocSearch-Label>to navigate</span></li><li class=list-inline-item><kbd class=me-2><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"/></g></svg></kbd><span class=DocSearch-Label>to close</span></li></ul><p class=d-md-none>Search by <a class=text-decoration-none href=https://github.com/nextapps-de/flexsearch>FlexSearch</a></p></div></div></div></div><div class="wrap container-lg" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class="section-nav docs-links"><ul class=list-unstyled><li><details open><summary>Guides</summary><ul class="list-unstyled list-nested"><li><a href=/dspy-go/docs/guides/getting-started/>Getting Started</a></li><li><a href=/dspy-go/docs/guides/core-concepts/>Core Concepts</a></li><li><a href=/dspy-go/docs/guides/optimizers/>Optimizers</a></li><li><a href=/dspy-go/docs/guides/tool-management/>Tool Management</a></li><li><a href=/dspy-go/docs/guides/building-agents/>Building Agents</a></li><li><a href=/dspy-go/docs/guides/multimodal-processing/>Multimodal Processing</a></li></ul></details></li><li><details open open><summary>API Reference</summary><ul class="list-unstyled list-nested"><li><a href=/dspy-go/docs/reference/configuration-reference/>Configuration Reference</a></li><li><a href=/dspy-go/docs/reference/cli-reference/>CLI Reference</a></li><li class=active><a aria-current=page href=/dspy-go/docs/reference/llm-providers/>LLM Providers</a></li></ul></details></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=toc><ul><li><a href=#supported-providers>Supported Providers</a></li><li><a href=#google-gemini>Google Gemini</a><ul><li><a href=#setup>Setup</a></li><li><a href=#available-models>Available Models</a></li><li><a href=#configuration>Configuration</a></li><li><a href=#multimodal-support>Multimodal Support</a></li><li><a href=#streaming>Streaming</a></li><li><a href=#rate-limits--pricing>Rate Limits & Pricing</a></li><li><a href=#best-practices>Best Practices</a></li></ul></li><li><a href=#openai>OpenAI</a><ul><li><a href=#setup-1>Setup</a></li><li><a href=#available-models-1>Available Models</a></li><li><a href=#configuration-1>Configuration</a></li><li><a href=#function-calling>Function Calling</a></li><li><a href=#azure-openai>Azure OpenAI</a></li><li><a href=#streaming-1>Streaming</a></li><li><a href=#rate-limits--pricing-1>Rate Limits & Pricing</a></li><li><a href=#best-practices-1>Best Practices</a></li></ul></li><li><a href=#anthropic-claude>Anthropic Claude</a><ul><li><a href=#setup-2>Setup</a></li><li><a href=#available-models-2>Available Models</a></li><li><a href=#configuration-2>Configuration</a></li><li><a href=#multimodal-support-1>Multimodal Support</a></li><li><a href=#streaming-2>Streaming</a></li><li><a href=#rate-limits--pricing-2>Rate Limits & Pricing</a></li><li><a href=#best-practices-2>Best Practices</a></li></ul></li><li><a href=#ollama-local>Ollama (Local)</a><ul><li><a href=#setup-3>Setup</a></li><li><a href=#available-models-3>Available Models</a></li><li><a href=#installation>Installation</a></li><li><a href=#configuration-3>Configuration</a></li><li><a href=#streaming-3>Streaming</a></li><li><a href=#embedding-models>Embedding Models</a></li><li><a href=#performance-tips>Performance Tips</a></li><li><a href=#best-practices-3>Best Practices</a></li></ul></li><li><a href=#llamacpp-local-gguf>LlamaCpp (Local GGUF)</a><ul><li><a href=#setup-4>Setup</a></li><li><a href=#installation-1>Installation</a></li><li><a href=#available-models-4>Available Models</a></li><li><a href=#configuration-4>Configuration</a></li><li><a href=#popular-gguf-models>Popular GGUF Models</a></li><li><a href=#streaming-4>Streaming</a></li><li><a href=#best-practices-4>Best Practices</a></li></ul></li><li><a href=#litellm-unified-api>LiteLLM (Unified API)</a><ul><li><a href=#setup-5>Setup</a></li><li><a href=#supported-providers-1>Supported Providers</a></li><li><a href=#installation-2>Installation</a></li><li><a href=#configuration-5>Configuration</a></li><li><a href=#model-routing>Model Routing</a></li><li><a href=#load-balancing>Load Balancing</a></li><li><a href=#cost-tracking>Cost Tracking</a></li><li><a href=#best-practices-5>Best Practices</a></li></ul></li><li><a href=#provider-comparison>Provider Comparison</a><ul><li><a href=#performance-benchmarks>Performance Benchmarks</a></li><li><a href=#feature-matrix>Feature Matrix</a></li><li><a href=#context-window-comparison>Context Window Comparison</a></li></ul></li><li><a href=#environment-variables>Environment Variables</a></li><li><a href=#troubleshooting>Troubleshooting</a><ul><li><a href=#rate-limit-errors>Rate Limit Errors</a></li><li><a href=#context-length-errors>Context Length Errors</a></li><li><a href=#api-key-issues>API Key Issues</a></li><li><a href=#local-model-issues>Local Model Issues</a></li></ul></li><li><a href=#next-steps>Next Steps</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>LLM Providers</h1><nav class="toc-mobile d-xl-none" aria-label="Quaternary navigation"><details><summary>On this page</summary><div class=page-links><nav id=TableOfContents><ul><li><a href=#supported-providers>Supported Providers</a></li><li><a href=#google-gemini>Google Gemini</a><ul><li><a href=#setup>Setup</a></li><li><a href=#available-models>Available Models</a></li><li><a href=#configuration>Configuration</a></li><li><a href=#multimodal-support>Multimodal Support</a></li><li><a href=#streaming>Streaming</a></li><li><a href=#rate-limits--pricing>Rate Limits & Pricing</a></li><li><a href=#best-practices>Best Practices</a></li></ul></li><li><a href=#openai>OpenAI</a><ul><li><a href=#setup-1>Setup</a></li><li><a href=#available-models-1>Available Models</a></li><li><a href=#configuration-1>Configuration</a></li><li><a href=#function-calling>Function Calling</a></li><li><a href=#azure-openai>Azure OpenAI</a></li><li><a href=#streaming-1>Streaming</a></li><li><a href=#rate-limits--pricing-1>Rate Limits & Pricing</a></li><li><a href=#best-practices-1>Best Practices</a></li></ul></li><li><a href=#anthropic-claude>Anthropic Claude</a><ul><li><a href=#setup-2>Setup</a></li><li><a href=#available-models-2>Available Models</a></li><li><a href=#configuration-2>Configuration</a></li><li><a href=#multimodal-support-1>Multimodal Support</a></li><li><a href=#streaming-2>Streaming</a></li><li><a href=#rate-limits--pricing-2>Rate Limits & Pricing</a></li><li><a href=#best-practices-2>Best Practices</a></li></ul></li><li><a href=#ollama-local>Ollama (Local)</a><ul><li><a href=#setup-3>Setup</a></li><li><a href=#available-models-3>Available Models</a></li><li><a href=#installation>Installation</a></li><li><a href=#configuration-3>Configuration</a></li><li><a href=#streaming-3>Streaming</a></li><li><a href=#embedding-models>Embedding Models</a></li><li><a href=#performance-tips>Performance Tips</a></li><li><a href=#best-practices-3>Best Practices</a></li></ul></li><li><a href=#llamacpp-local-gguf>LlamaCpp (Local GGUF)</a><ul><li><a href=#setup-4>Setup</a></li><li><a href=#installation-1>Installation</a></li><li><a href=#available-models-4>Available Models</a></li><li><a href=#configuration-4>Configuration</a></li><li><a href=#popular-gguf-models>Popular GGUF Models</a></li><li><a href=#streaming-4>Streaming</a></li><li><a href=#best-practices-4>Best Practices</a></li></ul></li><li><a href=#litellm-unified-api>LiteLLM (Unified API)</a><ul><li><a href=#setup-5>Setup</a></li><li><a href=#supported-providers-1>Supported Providers</a></li><li><a href=#installation-2>Installation</a></li><li><a href=#configuration-5>Configuration</a></li><li><a href=#model-routing>Model Routing</a></li><li><a href=#load-balancing>Load Balancing</a></li><li><a href=#cost-tracking>Cost Tracking</a></li><li><a href=#best-practices-5>Best Practices</a></li></ul></li><li><a href=#provider-comparison>Provider Comparison</a><ul><li><a href=#performance-benchmarks>Performance Benchmarks</a></li><li><a href=#feature-matrix>Feature Matrix</a></li><li><a href=#context-window-comparison>Context Window Comparison</a></li></ul></li><li><a href=#environment-variables>Environment Variables</a></li><li><a href=#troubleshooting>Troubleshooting</a><ul><li><a href=#rate-limit-errors>Rate Limit Errors</a></li><li><a href=#context-length-errors>Context Length Errors</a></li><li><a href=#api-key-issues>API Key Issues</a></li><li><a href=#local-model-issues>Local Model Issues</a></li></ul></li><li><a href=#next-steps>Next Steps</a></li></ul></nav></div></details></nav><p>dspy-go supports multiple LLM providers with native integrations. Each provider has unique capabilities and configuration options.</p><hr><h2 id=supported-providers>Supported Providers<a href=#supported-providers class=anchor aria-hidden=true>#</a></h2><table><thead><tr><th>Provider</th><th>Streaming</th><th>Multimodal</th><th>Function Calling</th><th>Local</th><th>Best For</th></tr></thead><tbody><tr><td><strong>Google Gemini</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>Multimodal, long context (2M tokens!)</td></tr><tr><td><strong>OpenAI</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>Latest GPT-5 models, reliability</td></tr><tr><td><strong>Anthropic Claude</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>Long context, reasoning</td></tr><tr><td><strong>Ollama</strong></td><td>✅</td><td>❌</td><td>❌</td><td>✅</td><td>Local Llama 3.2, Qwen 2.5, privacy</td></tr><tr><td><strong>LlamaCpp</strong></td><td>✅</td><td>❌</td><td>❌</td><td>✅</td><td>Local GGUF models, quantization</td></tr><tr><td><strong>LiteLLM</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>Unified API for 100+ models</td></tr></tbody></table><hr><h2 id=google-gemini>Google Gemini<a href=#google-gemini class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> Multimodal applications, 2M token context, cost-effective</p><h3 id=setup>Setup<a href=#setup class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kn>import</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/core&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/llms&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Basic setup</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewGeminiLLM</span><span class=p>(</span><span class=s>&#34;your-api-key&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelGoogleGeminiPro</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>log</span><span class=p>.</span><span class=nf>Fatal</span><span class=p>(</span><span class=nx>err</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>core</span><span class=p>.</span><span class=nf>SetDefaultLLM</span><span class=p>(</span><span class=nx>llm</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=available-models>Available Models<a href=#available-models class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>Context Window</th><th>Features</th><th>Best For</th></tr></thead><tbody><tr><td><strong>gemini-2.5-pro</strong></td><td>2M tokens</td><td>Multimodal, function calling, best reasoning</td><td>Complex tasks, entire codebases</td></tr><tr><td><strong>gemini-2.5-flash</strong></td><td>1M tokens</td><td>Fast, cost-effective, multimodal</td><td>Quick responses, high volume</td></tr><tr><td><strong>gemini-2.5-flash-lite</strong></td><td>1M tokens</td><td>Ultra-fast, efficient</td><td>Lightweight tasks, batch processing</td></tr></tbody></table><h3 id=configuration>Configuration<a href=#configuration class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewGeminiLLM</span><span class=p>(</span><span class=s>&#34;api-key&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelGoogleGeminiPro</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.7</span><span class=p>),</span><span class=w>      </span><span class=c1>// Creativity (0.0-1.0)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithMaxTokens</span><span class=p>(</span><span class=mi>2048</span><span class=p>),</span><span class=w>        </span><span class=c1>// Max output tokens</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopP</span><span class=p>(</span><span class=mf>0.9</span><span class=p>),</span><span class=w>              </span><span class=c1>// Nucleus sampling</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopK</span><span class=p>(</span><span class=mi>40</span><span class=p>),</span><span class=w>               </span><span class=c1>// Top-K sampling</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithStopSequences</span><span class=p>([]</span><span class=kt>string</span><span class=p>{</span><span class=s>&#34;END&#34;</span><span class=p>,</span><span class=w> </span><span class=s>&#34;\n\n&#34;</span><span class=p>}),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=multimodal-support>Multimodal Support<a href=#multimodal-support class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Analyze images</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>imageData</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>ReadFile</span><span class=p>(</span><span class=s>&#34;image.jpg&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>predictor</span><span class=p>.</span><span class=nf>Process</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;image&#34;</span><span class=p>:</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nf>NewImageContent</span><span class=p>(</span><span class=nx>imageData</span><span class=p>,</span><span class=w> </span><span class=s>&#34;image/jpeg&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;question&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;What&#39;s in this image?&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Multiple images</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>predictor</span><span class=p>.</span><span class=nf>Process</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;image1&#34;</span><span class=p>:</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nf>NewImageContent</span><span class=p>(</span><span class=nx>data1</span><span class=p>,</span><span class=w> </span><span class=s>&#34;image/jpeg&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;image2&#34;</span><span class=p>:</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nf>NewImageContent</span><span class=p>(</span><span class=nx>data2</span><span class=p>,</span><span class=w> </span><span class=s>&#34;image/jpeg&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;question&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;What changed between these images?&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span></span></span></code></pre></div></figure></div><h3 id=streaming>Streaming<a href=#streaming class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreaming</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Handle streaming chunks</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreamHandler</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>chunk</span><span class=w> </span><span class=kt>string</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>chunk</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>prompt</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=rate-limits--pricing>Rate Limits & Pricing<a href=#rate-limits--pricing class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>RPM (Free)</th><th>RPM (Paid)</th><th>Cost (Input/Output)</th></tr></thead><tbody><tr><td>gemini-2.5-pro</td><td>2</td><td>360</td><td>$0.00125 / $0.005 per 1K tokens</td></tr><tr><td>gemini-2.5-flash</td><td>15</td><td>1000</td><td>$0.00004 / $0.00015 per 1K tokens</td></tr><tr><td>gemini-2.5-flash-lite</td><td>30</td><td>2000</td><td>$0.00002 / $0.00006 per 1K tokens</td></tr></tbody></table><h3 id=best-practices>Best Practices<a href=#best-practices class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use 2.5-flash for speed and cost</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewGeminiLLM</span><span class=p>(</span><span class=nx>key</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelGoogleGeminiFlash</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Leverage 2M token context for RAG</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// No need to chunk! Can handle entire codebases</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use for multimodal tasks</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewGeminiLLM</span><span class=p>(</span><span class=nx>key</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelGoogleGeminiPro</span><span class=p>)</span></span></span></code></pre></div></figure></div><p><strong>Get API Key:</strong> <a href=https://makersuite.google.com/app/apikey>Google AI Studio</a></p><hr><h2 id=openai>OpenAI<a href=#openai class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> GPT-5 models, reliability, ecosystem</p><h3 id=setup-1>Setup<a href=#setup-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kn>import</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/core&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/llms&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Basic setup</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelOpenAIGPT5</span><span class=p>,</span><span class=w> </span><span class=s>&#34;your-api-key&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>core</span><span class=p>.</span><span class=nf>SetDefaultLLM</span><span class=p>(</span><span class=nx>llm</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=available-models-1>Available Models<a href=#available-models-1 class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>Context Window</th><th>Features</th><th>Best For</th></tr></thead><tbody><tr><td><strong>gpt-5</strong></td><td>256K</td><td>Flagship model, multimodal, superior reasoning</td><td>Most complex tasks</td></tr><tr><td><strong>gpt-5-mini</strong></td><td>256K</td><td>Efficient, fast, multimodal</td><td>Balanced tasks</td></tr><tr><td><strong>gpt-5-nano</strong></td><td>128K</td><td>Ultra-efficient, fast</td><td>High-volume, quick tasks</td></tr><tr><td><strong>gpt-4o</strong></td><td>128K</td><td>Optimized, fast, multimodal</td><td>General purpose</td></tr><tr><td><strong>gpt-4o-mini</strong></td><td>128K</td><td>Affordable, fast</td><td>High-volume tasks</td></tr><tr><td><strong>gpt-4-turbo</strong></td><td>128K</td><td>Latest GPT-4, multimodal</td><td>Complex reasoning</td></tr><tr><td><strong>gpt-4</strong></td><td>8K</td><td>Proven, reliable</td><td>Production apps</td></tr><tr><td><strong>gpt-3.5-turbo</strong></td><td>16K</td><td>Fast, cheap</td><td>Quick tasks, chat</td></tr></tbody></table><h3 id=configuration-1>Configuration<a href=#configuration-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelOpenAIGPT5</span><span class=p>,</span><span class=w> </span><span class=s>&#34;api-key&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.7</span><span class=p>),</span><span class=w>           </span><span class=c1>// Creativity</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithMaxTokens</span><span class=p>(</span><span class=mi>4096</span><span class=p>),</span><span class=w>             </span><span class=c1>// Max output</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopP</span><span class=p>(</span><span class=mf>0.9</span><span class=p>),</span><span class=w>                   </span><span class=c1>// Nucleus sampling</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithPresencePenalty</span><span class=p>(</span><span class=mf>0.1</span><span class=p>),</span><span class=w>        </span><span class=c1>// Discourage repetition</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithFrequencyPenalty</span><span class=p>(</span><span class=mf>0.1</span><span class=p>),</span><span class=w>       </span><span class=c1>// Penalize frequent words</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithStopSequences</span><span class=p>([]</span><span class=kt>string</span><span class=p>{</span><span class=s>&#34;\n\n&#34;</span><span class=p>}),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=function-calling>Function Calling<a href=#function-calling class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Define functions</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>functions</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=p>[]</span><span class=nx>core</span><span class=p>.</span><span class=nx>Function</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Name</span><span class=p>:</span><span class=w>        </span><span class=s>&#34;get_weather&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Description</span><span class=p>:</span><span class=w> </span><span class=s>&#34;Get current weather for a location&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>Parameters</span><span class=p>:</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=s>&#34;type&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;object&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=s>&#34;properties&#34;</span><span class=p>:</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=s>&#34;location&#34;</span><span class=p>:</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=s>&#34;type&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;string&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=s>&#34;description&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;City name&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=s>&#34;required&#34;</span><span class=p>:</span><span class=w> </span><span class=p>[]</span><span class=kt>string</span><span class=p>{</span><span class=s>&#34;location&#34;</span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>},</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetFunctions</span><span class=p>(</span><span class=nx>functions</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=azure-openai>Azure OpenAI<a href=#azure-openai class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=s>&#34;gpt-5&#34;</span><span class=p>,</span><span class=w> </span><span class=s>&#34;api-key&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithBaseURL</span><span class=p>(</span><span class=s>&#34;https://your-resource.openai.azure.com&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithAPIVersion</span><span class=p>(</span><span class=s>&#34;2024-02-15-preview&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithAPIType</span><span class=p>(</span><span class=s>&#34;azure&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=streaming-1>Streaming<a href=#streaming-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreaming</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreamHandler</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>chunk</span><span class=w> </span><span class=kt>string</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>chunk</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>prompt</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=rate-limits--pricing-1>Rate Limits & Pricing<a href=#rate-limits--pricing-1 class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>TPM (Tier 1)</th><th>Cost (Input/Output)</th></tr></thead><tbody><tr><td>gpt-5</td><td>500K</td><td>$0.005 / $0.015 per 1K tokens (estimated)</td></tr><tr><td>gpt-5-mini</td><td>1M</td><td>$0.0015 / $0.004 per 1K tokens (estimated)</td></tr><tr><td>gpt-5-nano</td><td>2M</td><td>$0.0005 / $0.001 per 1K tokens (estimated)</td></tr><tr><td>gpt-4o</td><td>500K</td><td>$0.0025 / $0.01 per 1K tokens</td></tr><tr><td>gpt-4o-mini</td><td>2M</td><td>$0.00015 / $0.0006 per 1K tokens</td></tr><tr><td>gpt-4-turbo</td><td>300K</td><td>$0.01 / $0.03 per 1K tokens</td></tr><tr><td>gpt-4</td><td>40K</td><td>$0.03 / $0.06 per 1K tokens</td></tr><tr><td>gpt-3.5-turbo</td><td>200K</td><td>$0.0005 / $0.0015 per 1K tokens</td></tr></tbody></table><h3 id=best-practices-1>Best Practices<a href=#best-practices-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use GPT-5 for most complex reasoning</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelOpenAIGPT5</span><span class=p>,</span><span class=w> </span><span class=nx>key</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use gpt-5-nano for high-volume tasks</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelOpenAIGPT5Nano</span><span class=p>,</span><span class=w> </span><span class=nx>key</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use gpt-4o for production balance</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOpenAI</span><span class=p>(</span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelOpenAIGPT4o</span><span class=p>,</span><span class=w> </span><span class=nx>key</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Implement retry logic</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetMaxRetries</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetRetryDelay</span><span class=p>(</span><span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>)</span></span></span></code></pre></div></figure></div><p><strong>Get API Key:</strong> <a href=https://platform.openai.com/api-keys>OpenAI Platform</a></p><hr><h2 id=anthropic-claude>Anthropic Claude<a href=#anthropic-claude class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> Long context, detailed reasoning, safety</p><h3 id=setup-2>Setup<a href=#setup-2 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kn>import</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/core&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/llms&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Basic setup</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewAnthropicLLM</span><span class=p>(</span><span class=s>&#34;your-api-key&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelAnthropicSonnet</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>core</span><span class=p>.</span><span class=nf>SetDefaultLLM</span><span class=p>(</span><span class=nx>llm</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=available-models-2>Available Models<a href=#available-models-2 class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>Context Window</th><th>Features</th><th>Best For</th></tr></thead><tbody><tr><td><strong>claude-3.5-sonnet</strong></td><td>200K</td><td>Latest, balanced, multimodal</td><td>General purpose, production</td></tr><tr><td><strong>claude-3-opus</strong></td><td>200K</td><td>Most capable, best reasoning</td><td>Complex analysis, research</td></tr><tr><td><strong>claude-3-haiku</strong></td><td>200K</td><td>Fast, efficient</td><td>Quick tasks, high volume</td></tr></tbody></table><h3 id=configuration-2>Configuration<a href=#configuration-2 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewAnthropicLLM</span><span class=p>(</span><span class=s>&#34;api-key&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelAnthropicSonnet</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.7</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithMaxTokens</span><span class=p>(</span><span class=mi>4096</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopP</span><span class=p>(</span><span class=mf>0.9</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopK</span><span class=p>(</span><span class=mi>40</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=multimodal-support-1>Multimodal Support<a href=#multimodal-support-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Analyze images with Claude</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>imageData</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>ReadFile</span><span class=p>(</span><span class=s>&#34;document.jpg&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>predictor</span><span class=p>.</span><span class=nf>Process</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=kd>map</span><span class=p>[</span><span class=kt>string</span><span class=p>]</span><span class=kd>interface</span><span class=p>{}{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;image&#34;</span><span class=p>:</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nf>NewImageContent</span><span class=p>(</span><span class=nx>imageData</span><span class=p>,</span><span class=w> </span><span class=s>&#34;image/jpeg&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;question&#34;</span><span class=p>:</span><span class=w> </span><span class=s>&#34;Extract all text from this document&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span></span></span></code></pre></div></figure></div><h3 id=streaming-2>Streaming<a href=#streaming-2 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreaming</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreamHandler</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>chunk</span><span class=w> </span><span class=kt>string</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>chunk</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>prompt</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=rate-limits--pricing-2>Rate Limits & Pricing<a href=#rate-limits--pricing-2 class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Model</th><th>TPM</th><th>Cost (Input/Output)</th></tr></thead><tbody><tr><td>claude-3.5-sonnet</td><td>400K</td><td>$0.003 / $0.015 per 1K tokens</td></tr><tr><td>claude-3-opus</td><td>400K</td><td>$0.015 / $0.075 per 1K tokens</td></tr><tr><td>claude-3-haiku</td><td>400K</td><td>$0.00025 / $0.00125 per 1K tokens</td></tr></tbody></table><h3 id=best-practices-2>Best Practices<a href=#best-practices-2 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use 3.5 Sonnet for production</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewAnthropicLLM</span><span class=p>(</span><span class=nx>key</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelAnthropicSonnet</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use Haiku for fast, cheap tasks</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewAnthropicLLM</span><span class=p>(</span><span class=nx>key</span><span class=p>,</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ModelAnthropicHaiku</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Leverage 200K context for documents</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Can analyze entire books!</span></span></span></code></pre></div></figure></div><p><strong>Get API Key:</strong> <a href=https://console.anthropic.com/>Anthropic Console</a></p><hr><h2 id=ollama-local>Ollama (Local)<a href=#ollama-local class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> Privacy, offline use, no API costs, Llama 3.2 & Qwen 2.5</p><h3 id=setup-3>Setup<a href=#setup-3 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kn>import</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s>&#34;github.com/XiaoConstantine/dspy-go/pkg/llms&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Basic setup (assumes Ollama running on localhost:11434)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;llama3:8b&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Custom server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;qwen2.5:7b&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithBaseURL</span><span class=p>(</span><span class=s>&#34;http://192.168.1.100:11434&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=available-models-3>Available Models<a href=#available-models-3 class=anchor aria-hidden=true>#</a></h3><p>Latest models supported by dspy-go:</p><table><thead><tr><th>Model</th><th>Size</th><th>Context</th><th>Best For</th></tr></thead><tbody><tr><td><strong>llama3.2:3b</strong></td><td>3B</td><td>8K</td><td>Efficient, fast, latest Llama</td></tr><tr><td><strong>llama3.1:8b</strong></td><td>8B</td><td>128K</td><td>Latest Llama 3.1, long context</td></tr><tr><td><strong>llama3.1:70b</strong></td><td>70B</td><td>128K</td><td>Most capable Llama</td></tr><tr><td><strong>qwen2.5:7b</strong></td><td>7B</td><td>32K</td><td>Latest Qwen, excellent reasoning</td></tr><tr><td><strong>qwen2.5:14b</strong></td><td>14B</td><td>32K</td><td>Best Qwen, superior performance</td></tr><tr><td><strong>codellama:13b</strong></td><td>13B</td><td>16K</td><td>Code generation</td></tr><tr><td><strong>codellama:34b</strong></td><td>34B</td><td>16K</td><td>Advanced code tasks</td></tr><tr><td><strong>mistral:7b</strong></td><td>7B</td><td>32K</td><td>Fast, efficient</td></tr><tr><td><strong>gemma:2b</strong></td><td>2B</td><td>8K</td><td>Ultra-efficient</td></tr><tr><td><strong>gemma:7b</strong></td><td>7B</td><td>8K</td><td>Balanced efficiency</td></tr></tbody></table><h3 id=installation>Installation<a href=#installation class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Install Ollama</span>
</span></span><span class=line><span class=cl>curl https://ollama.ai/install.sh <span class=p>|</span> sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Pull latest models</span>
</span></span><span class=line><span class=cl>ollama pull llama3.2:3b
</span></span><span class=line><span class=cl>ollama pull qwen2.5:7b
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run Ollama server</span>
</span></span><span class=line><span class=cl>ollama serve</span></span></code></pre></div></figure></div><h3 id=configuration-3>Configuration<a href=#configuration-3 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;llama3.1:8b&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.8</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithNumCtx</span><span class=p>(</span><span class=mi>8192</span><span class=p>),</span><span class=w>          </span><span class=c1>// Context window</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithNumPredict</span><span class=p>(</span><span class=mi>2048</span><span class=p>),</span><span class=w>      </span><span class=c1>// Max tokens</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithNumGPU</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span><span class=w>             </span><span class=c1>// GPU layers</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithRepeatPenalty</span><span class=p>(</span><span class=mf>1.1</span><span class=p>),</span><span class=w>    </span><span class=c1>// Repetition penalty</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=streaming-3>Streaming<a href=#streaming-3 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreaming</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreamHandler</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>chunk</span><span class=w> </span><span class=kt>string</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>chunk</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>prompt</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=embedding-models>Embedding Models<a href=#embedding-models class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Use Ollama for embeddings</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;nomic-embed-text&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>embeddings</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>CreateEmbedding</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=s>&#34;text to embed&#34;</span><span class=p>)</span></span></span></code></pre></div></figure></div><p>Available embedding models:</p><ul><li><strong>nomic-embed-text</strong> - 768 dimensions, best quality</li><li><strong>mxbai-embed-large</strong> - 1024 dimensions, large</li><li><strong>all-minilm</strong> - 384 dimensions, fast</li></ul><h3 id=performance-tips>Performance Tips<a href=#performance-tips class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Use quantized models for speed</span>
</span></span><span class=line><span class=cl>ollama pull llama3.2:3b-q4_K_M
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Enable GPU acceleration</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OLLAMA_NUM_GPU</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Increase context for long documents</span>
</span></span><span class=line><span class=cl>ollama run llama3.1:8b --ctx-size <span class=m>16384</span></span></span></code></pre></div></figure></div><h3 id=best-practices-3>Best Practices<a href=#best-practices-3 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use Llama 3.2 for latest capabilities</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;llama3.2:3b&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use Qwen 2.5 for best reasoning</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;qwen2.5:7b&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use CodeLlama for code tasks</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewOllamaLLM</span><span class=p>(</span><span class=s>&#34;codellama:13b&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Batch requests for efficiency</span></span></span></code></pre></div></figure></div><p><strong>Get Started:</strong> <a href=https://ollama.com/>ollama.com</a></p><hr><h2 id=llamacpp-local-gguf>LlamaCpp (Local GGUF)<a href=#llamacpp-local-gguf class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> Running quantized models locally, maximum control, GGUF format</p><h3 id=setup-4>Setup<a href=#setup-4 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Basic setup (assumes llama.cpp server on localhost:8080)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewLlamacppLLM</span><span class=p>(</span><span class=s>&#34;http://localhost:8080&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>log</span><span class=p>.</span><span class=nf>Fatal</span><span class=p>(</span><span class=nx>err</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>core</span><span class=p>.</span><span class=nf>SetDefaultLLM</span><span class=p>(</span><span class=nx>llm</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Custom configuration</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewLlamacppLLM</span><span class=p>(</span><span class=s>&#34;http://localhost:8080&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.7</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithMaxTokens</span><span class=p>(</span><span class=mi>2048</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=installation-1>Installation<a href=#installation-1 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Clone llama.cpp</span>
</span></span><span class=line><span class=cl>git clone https://github.com/ggerganov/llama.cpp
</span></span><span class=line><span class=cl><span class=nb>cd</span> llama.cpp
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build with GPU support (optional)</span>
</span></span><span class=line><span class=cl>make <span class=nv>LLAMA_CUBLAS</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download a GGUF model from Hugging Face</span>
</span></span><span class=line><span class=cl><span class=c1># Example: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF</span>
</span></span><span class=line><span class=cl>wget https://huggingface.co/.../llama-2-7b-chat.Q4_K_M.gguf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Start server</span>
</span></span><span class=line><span class=cl>./server -m llama-2-7b-chat.Q4_K_M.gguf --port <span class=m>8080</span></span></span></code></pre></div></figure></div><h3 id=available-models-4>Available Models<a href=#available-models-4 class=anchor aria-hidden=true>#</a></h3><p>Any GGUF quantized model from Hugging Face:</p><table><thead><tr><th>Quantization</th><th>Size</th><th>Quality</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>Q2_K</strong></td><td>Smallest</td><td>Lower</td><td>Testing, memory-constrained</td></tr><tr><td><strong>Q4_K_M</strong></td><td>Medium</td><td>Good</td><td>Balanced performance</td></tr><tr><td><strong>Q5_K_M</strong></td><td>Larger</td><td>Better</td><td>Recommended for most</td></tr><tr><td><strong>Q8_0</strong></td><td>Largest</td><td>Best</td><td>Maximum quality</td></tr><tr><td><strong>F16</strong></td><td>Full</td><td>Native</td><td>Best quality, large memory</td></tr></tbody></table><h3 id=configuration-4>Configuration<a href=#configuration-4 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>NewLlamacppLLM</span><span class=p>(</span><span class=s>&#34;http://localhost:8080&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTemperature</span><span class=p>(</span><span class=mf>0.8</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopK</span><span class=p>(</span><span class=mi>40</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithTopP</span><span class=p>(</span><span class=mf>0.9</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithRepeatPenalty</span><span class=p>(</span><span class=mf>1.1</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=popular-gguf-models>Popular GGUF Models<a href=#popular-gguf-models class=anchor aria-hidden=true>#</a></h3><ul><li><strong>Llama 3.1 8B</strong> - Latest Meta Llama</li><li><strong>Qwen 2.5 7B</strong> - Excellent reasoning</li><li><strong>Mistral 7B</strong> - Fast, efficient</li><li><strong>CodeLlama 13B</strong> - Code generation</li><li><strong>Yi 34B</strong> - Strong general purpose</li></ul><p>Find more: <a href="https://huggingface.co/models?search=gguf">Hugging Face GGUF Models</a></p><h3 id=streaming-4>Streaming<a href=#streaming-4 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreaming</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetStreamHandler</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>chunk</span><span class=w> </span><span class=kt>string</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Print</span><span class=p>(</span><span class=nx>chunk</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>prompt</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=best-practices-4>Best Practices<a href=#best-practices-4 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use Q4_K_M for balance</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Good quality, reasonable size</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use Q5_K_M for better quality</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Slightly larger, better output</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Monitor GPU memory usage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Adjust context size if needed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use --ctx-size for long contexts</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>.</span><span class=o>/</span><span class=nx>server</span><span class=w> </span><span class=o>-</span><span class=nx>m</span><span class=w> </span><span class=nx>model</span><span class=p>.</span><span class=nx>gguf</span><span class=w> </span><span class=o>--</span><span class=nx>ctx</span><span class=o>-</span><span class=nx>size</span><span class=w> </span><span class=mi>8192</span></span></span></code></pre></div></figure></div><hr><h2 id=litellm-unified-api>LiteLLM (Unified API)<a href=#litellm-unified-api class=anchor aria-hidden=true>#</a></h2><p><strong>Best for:</strong> Supporting 100+ models through one API, multi-provider flexibility</p><h3 id=setup-5>Setup<a href=#setup-5 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Basic setup (assumes LiteLLM proxy running)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>config</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ProviderConfig</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Name</span><span class=p>:</span><span class=w>    </span><span class=s>&#34;litellm&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>BaseURL</span><span class=p>:</span><span class=w> </span><span class=s>&#34;http://localhost:4000&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;gpt-4&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>core</span><span class=p>.</span><span class=nf>SetDefaultLLM</span><span class=p>(</span><span class=nx>llm</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// With API key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;claude-3-sonnet&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>llms</span><span class=p>.</span><span class=nf>WithAPIKey</span><span class=p>(</span><span class=s>&#34;your-litellm-key&#34;</span><span class=p>),</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=supported-providers-1>Supported Providers<a href=#supported-providers-1 class=anchor aria-hidden=true>#</a></h3><p>LiteLLM provides unified access to <strong>100+ models</strong>:</p><table><thead><tr><th>Category</th><th>Providers</th></tr></thead><tbody><tr><td><strong>Major APIs</strong></td><td>OpenAI, Anthropic, Google, Cohere</td></tr><tr><td><strong>Cloud</strong></td><td>AWS Bedrock, Azure OpenAI, Vertex AI</td></tr><tr><td><strong>Open Source</strong></td><td>Hugging Face, Replicate, Together AI</td></tr><tr><td><strong>Local</strong></td><td>Ollama, LlamaCpp, LocalAI</td></tr></tbody></table><h3 id=installation-2>Installation<a href=#installation-2 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Install LiteLLM</span>
</span></span><span class=line><span class=cl>pip install litellm<span class=o>[</span>proxy<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create config file</span>
</span></span><span class=line><span class=cl>cat &gt; litellm_config.yaml <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>model_list:
</span></span></span><span class=line><span class=cl><span class=s>  - model_name: gpt-4
</span></span></span><span class=line><span class=cl><span class=s>    litellm_params:
</span></span></span><span class=line><span class=cl><span class=s>      model: openai/gpt-4
</span></span></span><span class=line><span class=cl><span class=s>      api_key: os.environ/OPENAI_API_KEY
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  - model_name: claude-3-sonnet
</span></span></span><span class=line><span class=cl><span class=s>    litellm_params:
</span></span></span><span class=line><span class=cl><span class=s>      model: anthropic/claude-3-sonnet-20240229
</span></span></span><span class=line><span class=cl><span class=s>      api_key: os.environ/ANTHROPIC_API_KEY
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  - model_name: llama-3-70b
</span></span></span><span class=line><span class=cl><span class=s>    litellm_params:
</span></span></span><span class=line><span class=cl><span class=s>      model: together_ai/meta-llama/Llama-3-70b-chat-hf
</span></span></span><span class=line><span class=cl><span class=s>      api_key: os.environ/TOGETHER_API_KEY
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Start proxy server</span>
</span></span><span class=line><span class=cl>litellm --config litellm_config.yaml --port <span class=m>4000</span></span></span></code></pre></div></figure></div><h3 id=configuration-5>Configuration<a href=#configuration-5 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Use any provider through LiteLLM</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>config</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>core</span><span class=p>.</span><span class=nx>ProviderConfig</span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>Name</span><span class=p>:</span><span class=w>    </span><span class=s>&#34;litellm&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>BaseURL</span><span class=p>:</span><span class=w> </span><span class=s>&#34;http://localhost:4000&#34;</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// OpenAI GPT-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llmGPT4</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;gpt-4&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Anthropic Claude</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llmClaude</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;claude-3-sonnet&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Together AI Llama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llmLlama</span><span class=p>,</span><span class=w> </span><span class=nx>_</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;llama-3-70b&#34;</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=model-routing>Model Routing<a href=#model-routing class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># litellm_config.yaml - Advanced routing</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>router_settings</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>routing_strategy</span><span class=p>:</span><span class=w> </span><span class=l>least-busy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model_list</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>model_name</span><span class=p>:</span><span class=w> </span><span class=l>gpt-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>litellm_params</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>model</span><span class=p>:</span><span class=w> </span><span class=l>openai/gpt-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>api_key</span><span class=p>:</span><span class=w> </span><span class=l>os.environ/OPENAI_API_KEY</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>model_name</span><span class=p>:</span><span class=w> </span><span class=l>gpt-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>litellm_params</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>model</span><span class=p>:</span><span class=w> </span><span class=l>azure/gpt-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>api_key</span><span class=p>:</span><span class=w> </span><span class=l>os.environ/AZURE_API_KEY</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>api_base</span><span class=p>:</span><span class=w> </span><span class=l>os.environ/AZURE_ENDPOINT</span></span></span></code></pre></div></figure></div><h3 id=load-balancing>Load Balancing<a href=#load-balancing class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// LiteLLM automatically load balances</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Just configure multiple instances in litellm_config.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llms</span><span class=p>.</span><span class=nf>LiteLLMProviderFactory</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=nx>config</span><span class=p>,</span><span class=w> </span><span class=s>&#34;gpt-4&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Requests automatically distributed across providers</span></span></span></code></pre></div></figure></div><h3 id=cost-tracking>Cost Tracking<a href=#cost-tracking class=anchor aria-hidden=true>#</a></h3><p>LiteLLM provides built-in cost tracking:</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># View costs</span>
</span></span><span class=line><span class=cl>curl http://localhost:4000/spend/logs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set budget limits in config</span>
</span></span><span class=line><span class=cl>general_settings:
</span></span><span class=line><span class=cl>  master_key: sk-1234
</span></span><span class=line><span class=cl>  budget_duration: 30d
</span></span><span class=line><span class=cl>  max_budget: <span class=m>100</span></span></span></code></pre></div></figure></div><h3 id=best-practices-5>Best Practices<a href=#best-practices-5 class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// ✅ Use for multi-provider applications</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Switch providers without code changes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Implement fallback logic</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// LiteLLM can auto-fallback to backup models</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Monitor costs centrally</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Single dashboard for all providers</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// ✅ Use for A/B testing</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Easy to compare different models</span></span></span></code></pre></div></figure></div><p><strong>Get Started:</strong> <a href=https://docs.litellm.ai/>LiteLLM Docs</a></p><hr><h2 id=provider-comparison>Provider Comparison<a href=#provider-comparison class=anchor aria-hidden=true>#</a></h2><h3 id=performance-benchmarks>Performance Benchmarks<a href=#performance-benchmarks class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Provider</th><th>Latency (P50)</th><th>Throughput</th><th>Cost Efficiency</th></tr></thead><tbody><tr><td>Gemini 2.5 Flash</td><td>200ms</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td>GPT-5 Nano</td><td>300ms</td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td>Claude Haiku</td><td>250ms</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td>Ollama (local)</td><td>50ms</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr></tbody></table><h3 id=feature-matrix>Feature Matrix<a href=#feature-matrix class=anchor aria-hidden=true>#</a></h3><table><thead><tr><th>Feature</th><th>Gemini</th><th>OpenAI</th><th>Claude</th><th>Ollama</th><th>LlamaCpp</th><th>LiteLLM</th></tr></thead><tbody><tr><td><strong>Streaming</strong></td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td><strong>Multimodal</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td><strong>Function Calling</strong></td><td>✅</td><td>✅</td><td>✅</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td><strong>Embeddings</strong></td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td><strong>JSON Mode</strong></td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td><strong>Long Context</strong></td><td>2M tokens</td><td>256K tokens</td><td>200K tokens</td><td>128K</td><td>Varies</td><td>Varies</td></tr></tbody></table><h3 id=context-window-comparison>Context Window Comparison<a href=#context-window-comparison class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><pre tabindex=0><code>Gemini 2.5 Pro     ██████████████████████████████████████████████████ 2M tokens
GPT-5              ████████████████ 256K tokens
Claude 3.5         ██████████ 200K tokens
Llama 3.1 70B      ████████ 128K tokens
GPT-4o             ████████ 128K tokens
Mistral 7B         ████ 32K tokens
GPT-4              █ 8K tokens</code></pre></figure></div><hr><h2 id=environment-variables>Environment Variables<a href=#environment-variables class=anchor aria-hidden=true>#</a></h2><p>Quick reference for all providers:</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Google Gemini</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>GEMINI_API_KEY</span><span class=o>=</span><span class=s2>&#34;your-api-key&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># OpenAI</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OPENAI_API_KEY</span><span class=o>=</span><span class=s2>&#34;your-api-key&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OPENAI_BASE_URL</span><span class=o>=</span><span class=s2>&#34;https://api.openai.com/v1&#34;</span>  <span class=c1># optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Anthropic Claude</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>ANTHROPIC_API_KEY</span><span class=o>=</span><span class=s2>&#34;your-api-key&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ollama (local)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OLLAMA_BASE_URL</span><span class=o>=</span><span class=s2>&#34;http://localhost:11434&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LiteLLM</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LITELLM_BASE_URL</span><span class=o>=</span><span class=s2>&#34;http://localhost:4000&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LITELLM_API_KEY</span><span class=o>=</span><span class=s2>&#34;optional-key&#34;</span></span></span></code></pre></div></figure></div><hr><h2 id=troubleshooting>Troubleshooting<a href=#troubleshooting class=anchor aria-hidden=true>#</a></h2><h3 id=rate-limit-errors>Rate Limit Errors<a href=#rate-limit-errors class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Implement exponential backoff</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetMaxRetries</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetRetryDelay</span><span class=p>(</span><span class=mi>2</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>llm</span><span class=p>.</span><span class=nf>SetRetryBackoff</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=context-length-errors>Context Length Errors<a href=#context-length-errors class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Check model&#39;s context window</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>maxContext</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>GetContextWindow</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Truncate if needed</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>if</span><span class=w> </span><span class=nb>len</span><span class=p>(</span><span class=nx>prompt</span><span class=p>)</span><span class=w> </span><span class=p>&gt;</span><span class=w> </span><span class=nx>maxContext</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>prompt</span><span class=w> </span><span class=p>=</span><span class=w> </span><span class=nx>prompt</span><span class=p>[:</span><span class=nx>maxContext</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span></span></span></code></pre></div></figure></div><h3 id=api-key-issues>API Key Issues<a href=#api-key-issues class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=c1>// Verify API key is set</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>apiKey</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>Getenv</span><span class=p>(</span><span class=s>&#34;OPENAI_API_KEY&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>if</span><span class=w> </span><span class=nx>apiKey</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=s>&#34;&#34;</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>log</span><span class=p>.</span><span class=nf>Fatal</span><span class=p>(</span><span class=s>&#34;API key not found&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>// Test with simple request</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>result</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>llm</span><span class=p>.</span><span class=nf>Generate</span><span class=p>(</span><span class=nx>ctx</span><span class=p>,</span><span class=w> </span><span class=s>&#34;Hello, world!&#34;</span><span class=p>)</span></span></span></code></pre></div></figure></div><h3 id=local-model-issues>Local Model Issues<a href=#local-model-issues class=anchor aria-hidden=true>#</a></h3><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Ollama - Check if running</span>
</span></span><span class=line><span class=cl>curl http://localhost:11434/api/tags
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LlamaCpp - Check server</span>
</span></span><span class=line><span class=cl>curl http://localhost:8080/health</span></span></code></pre></div></figure></div><hr><h2 id=next-steps>Next Steps<a href=#next-steps class=anchor aria-hidden=true>#</a></h2><ul><li><strong><a href=configuration/>Configuration Reference →</a></strong> - Detailed configuration options</li><li><strong><a href=../../guides/getting-started/>Getting Started →</a></strong> - Quick start guide</li><li><strong><a href=../../guides/multimodal/>Multimodal Guide →</a></strong> - Work with images and vision</li></ul><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="page-nav d-flex flex-column flex-sm-row"><div class="card w-100"><div class="card-body d-flex"><div class="d-flex flex-column justify-content-center"><svg class="icon icon-tabler icon-tabler-arrow-left" width="20" height="20" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 12h14"/><path d="M5 12l6 6"/><path d="M5 12l6-6"/></svg></div><div class="d-flex flex-column"><div class=text-body-secondary>Prev</div><a href=/dspy-go/docs/reference/cli-reference/ class="stretched-link text-reset text-decoration-none">CLI Reference</a></div></div></div><div class=m-2></div><div class="card text-end w-100"><div class="card-body d-flex justify-content-end"><div class="d-flex flex-column"><div class=text-body-secondary>Next</div><a href=/dspy-go/docs/resources/ class="stretched-link text-reset text-decoration-none">Resources</a></div><div class="d-flex flex-column justify-content-center"><svg class="icon icon-tabler icon-tabler-arrow-right" width="20" height="20" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 12h14"/><path d="M13 18l6-6"/><path d="M13 6l6 6"/></svg></div></div></div></div></main></div></div></div><footer class="footer text-muted"><div class=container-lg><div class=row><div class="col-lg-8 text-center text-lg-start"><ul class=list-inline><li class=list-inline-item><a class=text-muted href=/dspy-go/privacy/>Privacy Policy</a></li></ul></div><div class="col-lg-8 text-center text-lg-end"><ul class=list-inline><li class=list-inline-item></li></ul></div></div></div></footer><script async src=/dspy-go/js/app.19b73e0182301dd61c9211c2f6890104740e2eb9eb12fe4ba8d4f31435b22ed6.js integrity="sha256-Gbc+AYIwHdYckhHC9okBBHQOLrnrEv5LqNTzFDWyLtY="></script><script async src=/dspy-go/js/flexsearch.5dd6433c29c3e043627f046054aed58ff3790f58fdb8423f45125bbafdcad335.js integrity="sha256-XdZDPCnD4ENifwRgVK7Vj/N5D1j9uEI/RRJbuv3K0zU="></script><script async src=/dspy-go/js/search-modal.96e662d8f691fe25c859a3437074b485f2d7bed0bca612725028c6cf4322e2f2.js integrity="sha256-luZi2PaR/iXIWaNDcHS0hfLXvtC8phJyUCjGz0Mi4vI="></script></body></html>